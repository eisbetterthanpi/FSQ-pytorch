{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/FSQ-pytorch/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cb5c7b-8b48-4e9e-eaf7-cf14c41b1a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "9bff3f67-360c-460b-d6a6-c28eb1a935a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[4], d_model, 2, 1, 0), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Dropout(p=drop),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n",
        "# 1278976\n",
        "# 1278979\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Bos81kQf1dwh"
      },
      "outputs": [],
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "SFVbGqMDqcDR"
      },
      "outputs": [],
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AfjFbveH64Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256, drop=0.5): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.), nn.Linear(in_dim, 2, bias=False)\n",
        "            # nn.Dropout(p=drop), nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=drop), nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=drop), nn.Linear(d_model, 2)\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device), reduction='none')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.tcost(x), dim=-1)@self.tc\n",
        "\n",
        "    def loss(self, x, y, reduction='mean'):\n",
        "        out = self.tcost(x)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        if reduction=='mean': return self.loss_fn(out, y).mean()\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024).to(device)\n",
        "# x=torch.randn((256,1024), device=device)\n",
        "# # import time\n",
        "# # start = time.time()\n",
        "# out=tcost(x).squeeze()\n",
        "# # out=tcost.tcost(x).squeeze()\n",
        "# print(out)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "1e3000fa-e656-4993-a424-bdac9e02d543",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-bd344d321dd5>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.2)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVeF35lpwqlJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "ff5b0968-dedc-4f1c-c5aa-63dbcc8944a2",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-bc8ce1da7eb2>:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.0154, grad_fn=<SelectBackward0>)\n",
            "0 tensor(0.3601, grad_fn=<SelectBackward0>)\n",
            "1 tensor(0.0154, grad_fn=<SelectBackward0>)\n",
            "1 tensor(0.3677, grad_fn=<SelectBackward0>)\n",
            "2 tensor(0.0154, grad_fn=<SelectBackward0>)\n",
            "2 tensor(0.3493, grad_fn=<SelectBackward0>)\n",
            "2 tensor(0.0154, grad_fn=<SelectBackward0>)\n",
            "2 tensor(0.3493, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title test batch argm\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd): # best case z for train\n",
        "    # self.tcost.eval() # disable tcost dropout\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    batch = 64 # 16\n",
        "    lsy, la, rwd = lsy.repeat(batch,1,1), la.repeat(batch,1,1), rwd.repeat(batch,1) # [batch*batch_size, bptt, d_model], [batch*batch_size, d_model, dim_a], [batch*batch_size, bptt]\n",
        "    lz = nn.Parameter(torch.zeros((batch*batch_size, bptt, agent.dim_z), device=device))\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lsy_, lh0 = agent.rnn_it(sy_, la, lz, h0_)\n",
        "        # repr_loss = F.mse_loss(lsy, lsy_)\n",
        "        repr_loss = ((lsy-lsy_)**2).unflatten(0, (batch,batch_size)).flatten(1).mean(-1)\n",
        "        syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch*batch_size,bptt,d_model], [bptt,num_layers,batch*batch_size,d_model] -> [batch*batch_size*bptt, (1+num_layers)*d_model]\n",
        "        # clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "        clossl = agent.tcost.loss(syh0, rwd.flatten(), reduction='none').unflatten(0, (batch,batch_size*bptt)).mean(-1) # [batch*batch_size*bptt] -> [batch]\n",
        "        # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "        # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "        print(i, repr_loss[0])\n",
        "        print(i, clossl[0])\n",
        "        cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(i, \"repr closs, lz\", torch.cat([torch.tensor([repr_loss[0], clossl[0]]), lz[0].cpu()],dim=-1).squeeze().data)\n",
        "    print(i, repr_loss[0])\n",
        "    print(i, clossl[0])\n",
        "    # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    idx = torch.argmin(cost)\n",
        "    return lz.unflatten(0, (batch,batch_size))[idx].squeeze(0).detach()\n",
        "\n",
        "# repr_loss, clossl untrained # 0.0850 # 0.6967\n",
        "# repr_loss, clossl trained # 0.0168, 0.4233\n",
        "\n",
        "# nn.HuberLoss() # near 0 is L2, outside is L1. less sensitive to outliers than L2? use like mse\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader):\n",
        "#     imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "#     break\n",
        "it = iter(train_loader)\n",
        "state, action, reward = next(it)\n",
        "imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "sy_ = agent.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "# state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "bptt=25#25\n",
        "for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "        la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "        lz = argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "    break\n",
        "# break\n",
        "# print(lz.squeeze().data)\n",
        "\n",
        "# import torch\n",
        "# la = torch.arange(15).reshape(3, 5)\n",
        "# la = la.repeat(2,1)\n",
        "# print(la)\n",
        "# print(la.unflatten(0, (2,3)))\n",
        "# # idx = torch.argmin(loss.unflatten(0, (batch_size,batch)), dim=1) # choose best x even with greatest adv z\n",
        "\n",
        "\n",
        "\n",
        "# sgd\n",
        "# 49 tensor([-0.2660, -0.0749, -0.1358, -0.0611, -0.0368, -0.0041,  0.0691,  0.0764]) 31.68710708618164\n",
        "# 9 tensor([ 0.1100,  0.0433,  0.0141,  0.0167, -0.1715, -0.0177,  0.1303,  0.0350]) 31.716447830200195\n",
        "# 4 tensor([ 0.1101,  0.0435,  0.0145,  0.0172, -0.1721, -0.0174,  0.1302,  0.0355]) 31.693767547607422\n",
        "# 49 tensor([ 0.1081,  0.0407,  0.0088,  0.0105, -0.1647, -0.0212,  0.1312,  0.0288]) 31.741121292114258\n",
        "\n",
        "# 49 tensor([-0.0199,  0.0148, -0.0822,  0.0207, -0.0466,  0.0609,  0.0938,  0.0440]) 31.563823699951172 0.04605092480778694 0.31333568692207336\n",
        "# 1 tensor([ 0.0220, -0.2026,  0.0452,  0.1610, -0.0399,  0.1887, -0.0626,  0.0763]) 31.575210571289062 0.044768013060092926 0.31351369619369507\n",
        "\n",
        "# 49 tensor([ 0.0784,  0.1738,  0.1562, -0.2048, -0.1269,  0.1467,  0.0242, -0.0849]) 31.541893005371094 0.04185735061764717 0.31332606077194214\n",
        "# tensor([-4.4024e-02,  1.5736e-01,  1.3286e-01,  9.2922e-02, -1.6661e-02,\n",
        "\n",
        "# 49 tensor([ 0.1917, -0.2293,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]) 31.557968139648438 0.04187680780887604 0.3134858310222626\n",
        "# tensor([-0.0824,  0.0021,  0.0125,  0.3892, -0.1041])\n",
        "# 49 tensor([-0.2967, -0.3361]) 31.584087371826172 0.04877716675400734 0.3134020268917084\n",
        "# tensor([-0.0822,  0.0095, -0.0060,  0.6069, -0.2085])\n",
        "# 49 tensor([0.0765, 0.0352]) 31.576648712158203 0.046897441148757935 0.3134216070175171\n",
        "# tensor([-0.0218,  0.1031,  0.1796,  0.2206, -0.0100])\n",
        "\n",
        "# 49 tensor(0.2663) 35.62339782714844 0.05059394985437393 0.3537042737007141\n",
        "# tensor([-0.0054,  0.1192,  0.1170,  0.2161, -0.0741])\n",
        "# 49 tensor(0.2765) 31.58695411682129 0.0482589527964592 0.31345659494400024\n",
        "# tensor([-0.0076,  0.0901,  0.1391,  0.1246, -0.0049])\n",
        "# 49 tensor(0.4091) 31.576698303222656 0.04228857159614563 0.31365254521369934\n",
        "# tensor([-0.0398,  0.0932,  0.0598,  0.2072, -0.0510])\n",
        "\n",
        "# 49 tensor(0.2947) 31.615554809570312 0.05741892755031586 0.31328460574150085\n",
        "# tensor([ 0.0335,  0.0820,  0.0564,  0.2376, -0.0323])\n",
        "\n",
        "\n",
        "# 31.553237915039062 0.039482660591602325 0.3135582506656647\n",
        "# tensor([ 0.0844, -0.0316,  0.4948,  0.1411, -0.1128, -0.1359,  0.6359, -0.1350,\n",
        "#          0.2688,  0.2809, -0.3960,  0.3010,  0.4960, -0.1607, -0.1109,  0.0719,\n",
        "#          0.1345, -0.2463,  0.2220, -0.0375,  0.0021, -0.2562,  0.2311, -0.0293,\n",
        "#          0.3958])\n",
        "# tensor([-0.0477,  0.1160,  0.1507,  0.1311, -0.0629])\n",
        "# 31.62661361694336 0.05305254086852074 0.31361350417137146\n",
        "# tensor([-0.6893,  0.4970,  0.6107, -0.2914, -0.1954, -0.3417,  0.6757, -0.2306,\n",
        "#          0.5352, -0.3400, -0.1038,  0.3860,  0.2331,  0.1387,  0.5997, -0.0533,\n",
        "#         -0.4756, -0.4694,  0.2403,  0.1279,  0.0730, -0.2124, -0.0072, -0.2314,\n",
        "#         -0.2863])\n",
        "# tensor([-0.0535,  0.0922,  0.1302,  0.5576, -0.1107])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "ba7ce999-1030-44a8-b4bc-eeac81a76059",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-4211955ac4d0>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=1. # 10 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 50 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 1 # ν cov Covariance\n",
        "        self.closs_coeff=1. # 10\n",
        "        # self.zloss_coeff=0. # 10 1\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model] # batch argm z for search\n",
        "        batch_, T, _ = x.shape\n",
        "        batch = 16 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch*batch_, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        idx = torch.argmin(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T] -> [batch_]\n",
        "        return torch.index_select(z, 0, idx) # [batch_, T,dim_z]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch = 16\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        with torch.no_grad(): x[:,:self.lx.shape[0]] = self.lx.repeat(batch,1,1)[:,:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [batch, T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [batch,T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        idx = torch.argmin(loss.sum(-1)) # loss [batch, T]\n",
        "        return lact[idx], lh0[:,:,idx], x.data[idx], z[idx] # [batch,T], [T, num_layers, batch, d_model], [batch,T, dim_a], [batch,T, dim_z]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz, h0, gamma=0.9): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        lsx, lh0 = self.rnn_it(sx, la, lz, h0)\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        syh0 = torch.cat([lsx, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        # if len(c.shape) == 1: print(\"rnn_pred c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        if len(tcost.shape) == 1: print(\"rnn_pred tcost\", [f'{cc.item():g}' for cc in tcost.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        return c, lh0\n",
        "\n",
        "    def rnn_it(self, sx, la, lz, h0): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        batch_ = batch//sx.shape[0]\n",
        "        sx, h0 = sx.repeat(batch_, 1), h0.repeat(1, batch_, 1)\n",
        "        lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1) # [batch, d_model+dim_a/z]\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                out = self.fc(out)\n",
        "            # sx = sx + out.squeeze(1) # [batch,seq_len,d_model] # h0 = h0 +\n",
        "            sx = out.squeeze(1) # [batch,1,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        return lsx, lh0\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd): # best case z for train\n",
        "        # self.tcost.eval() # disable tcost dropout\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        batch = 64 # 16\n",
        "        lsy, la, rwd = lsy.repeat(batch,1,1), la.repeat(batch,1,1), rwd.repeat(batch,1) # [batch*batch_size, bptt, d_model], [batch*batch_size, d_model, dim_a], [batch*batch_size, bptt]\n",
        "        lz = nn.Parameter(torch.zeros((batch*batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lsy_, lh0 = self.rnn_it(sy_, la, lz, h0_)\n",
        "            # repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            repr_loss = ((lsy-lsy_)**2).unflatten(0, (batch,batch_size)).flatten(1).mean(-1)\n",
        "            syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch*batch_size,bptt,d_model], [bptt,num_layers,batch*batch_size,d_model] -> [batch*batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "            clossl = self.tcost.loss(syh0, rwd.flatten(), reduction='none').unflatten(0, (batch,batch_size*bptt)).mean(-1) # [batch*batch_size*bptt] -> [batch]\n",
        "            # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(i, \"repr closs, z\", torch.cat([torch.tensor([repr_loss[0], clossl[0]]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        idx = torch.argmin(cost)\n",
        "        return lz.unflatten(0, (batch,batch_size))[idx].squeeze(0).detach()\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            sy_ = self.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    lz = self.argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    # with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0.5)).mul_((torch.rand_like(lz)>0.1).bool()) # dropout without scaling\n",
        "                    with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0)).mul_((torch.rand_like(lz)>0.5).bool()) # dropout without scaling\n",
        "                    lsy_, lh0 = self.rnn_it(sy_.squeeze(1), la, lz, h0)\n",
        "                    repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model] # not lsy_, else unstable\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    # print(\"pred\",pred[0])\n",
        "                    # print(\"rwd\",rwd[0])\n",
        "                    # mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # reprloss = ((lsy-lsy_)**2).mean(-1) # [batch_size, bptt]\n",
        "                    # print(\"reprloss\",reprloss[0])\n",
        "                    # mask = (reprloss>0.05)[0]\n",
        "                    # # imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[0][mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                # torch.norm(lsy-torch.cat([sy_,lsy[:-1]], dim=1), dim=-1) # -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "                # prob = F.softmax(output, dim=-1)\n",
        "                # entropy = -torch.sum(prob * torch.log(prob + 1e-5), dim=-1)\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                norm = torch.norm(lsy[0][0], dim=-1).item()\n",
        "                z_norm = torch.norm(lz[0][-1], dim=-1)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                # print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "                print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item(), \"z_norm\": z_norm.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title from charJEPA.ipynb, RNN2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers=1, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, h0=None,c0=None): # [batch_size, seq_len, in_dim]\n",
        "        out, h0 = self.rnn(x, h0) # [batch_size, seq_len, d_model], [num_layers, batch_size, d_model]\n",
        "        # out, _ = self.lstm(x, (h0,c0))\n",
        "        # out = out[:, -1, :] # [batch_size, d_model]\n",
        "        out = self.fc(out) # [batch_size, seq_len, out_dim]\n",
        "        return out, h0\n",
        "\n",
        "# model = RNN(input_size, hidden_size).to(device)\n",
        "# print(model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ILwCZmDU5wUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# rnn = nn.GRU(4, 5, 2) # [in_dim, out_dim, num_layers]\n",
        "rnn = nn.GRU(4, 5, 2, batch_first=True) # [in_dim, out_dim, num_layers]\n",
        "input = torch.randn(1, 3, 4) # [batch_size, seq_len, in_dim]\n",
        "h0 = torch.randn(2, 1, 5) # [num_layers, batch_size, out_dim]\n",
        "output, hn = rnn(input, h0) # [batch_size, seq_len, out_dim], [num_layers, batch_size, out_dim]\n",
        "print(output)\n",
        "print(hn)\n",
        "# print(hn[-1,:,:])\n",
        "print(output[:,-1,:])\n",
        "print(hn[-1,:,:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8OQNnfhO4V",
        "outputId": "ccd8fab7-0d72-41ed-ca77-7611c8d60014"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8099,  0.7630,  0.4902,  0.2586,  0.1378],\n",
            "         [ 0.1062,  0.7019,  0.5144,  0.2655,  0.3116],\n",
            "         [-0.1007,  0.4323,  0.4208,  0.2961,  0.2173]]],\n",
            "       grad_fn=<TransposeBackward1>)\n",
            "tensor([[[ 0.0901, -0.2363,  0.0673, -0.0746,  0.2156]],\n",
            "\n",
            "        [[-0.1007,  0.4323,  0.4208,  0.2961,  0.2173]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "tensor([[-0.1007,  0.4323,  0.4208,  0.2961,  0.2173]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor([[-0.1007,  0.4323,  0.4208,  0.2961,  0.2173]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "49RERFWFMgA_",
        "outputId": "ad17c67e-a305-448c-804b-eaaebe4a5710"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rnn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f80c5a345980>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===========\\ngradient:{}\\n----------\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rnn' is not defined"
          ]
        }
      ],
      "source": [
        "# print(agent.jepa.enc.parameters().values()[0].requires_grad)\n",
        "# for name, param in agent.tcost.named_parameters():\n",
        "# # # for name, param in agent.named_parameters():\n",
        "# #     # print(name, param.requires_grad)\n",
        "#     print(name, param)\n",
        "\n",
        "# for name, param in agent.tcost.named_parameters(): print(param.data)\n",
        "# print(agent.tcost.1.weight.data)\n",
        "# print(agent.tcost.named_parameters()['tcost.1.weight'])\n",
        "# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))\n",
        "\n",
        "for p,n in zip(rnn.parameters(),rnn._all_weights[0]):\n",
        "    if n[:6] == 'weight':\n",
        "        print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.grad))\n",
        "\n",
        "writer = torch.utils.tensorboard.SummaryWriter(\"runs/\")\n",
        "for name, param in model.named_parameters():\n",
        "        writer.add_histogram(name + '/grad', param.grad, global_step=epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "UEH1P802JkHU",
        "outputId": "31b3efd5-1338-4f55-9345-76a8d9371905"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'state' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2eab810ffe5c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sx = agent.jepa.enc(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4_b7ZSW6IF1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e720cffc-11ea-40f5-a22b-e09d35dd8891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ\n",
            "From (redirected): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ&confirm=t&uuid=68b9f4e5-453d-483c-9f1e-8a07ac8035a3\n",
            "To: /content/buffergo.pkl\n",
            "100% 1.80G/1.80G [00:31<00:00, 56.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1MV9Qj_53Vu6wpe7nOFn47M5vDj7F7-gv -O agentoptim.pkl # S3 agentoptimargm2\n",
        "# !gdown 1--1Vl3337zugQng-j1qbptFY8EvhZA-T -O agentoptim.pkl # T4 agentoptimargm3 online\n",
        "# !gdown 1XHFBVPSH4T4FpUOBKN8X20xDQLNmL7go -O agentoptim.pkl # M1 agentoptimargm4\n",
        "# !gdown 1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH -O agentoptim.pkl # B2 agentoptimargm4\n",
        "# !gdown 1H31OMz5YBPfmgb7yxVePGddljS9cwVOF -O agentoptim.pkl # B2 agent_nores1\n",
        "# !gdown 1tZpMKsMz7Bh3-X7WE49dsySs-2XOk6Wc -O agentoptim.pkl # T4 agentgru3tcost3\n",
        "\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "!gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "60c511ed-8f25-41be-a5b2-a0420f1d532d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agentgru1tcost3.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load(folder+'agentcovgru1tcost3drop.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentgru3tcost3.pkl')\n",
        "torch.save(checkpoint, folder+'agentcovgru1fctcost1drop512.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# # torch.save(all_sd, 'all_sd.pkl')\n",
        "# torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buffer1=buffer[:512]\n",
        "buffer2=buffer[512:]\n",
        "buffer=buffer1"
      ],
      "metadata": {
        "id": "3f5hTNB0f7_W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 100):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    # def pop_unif(self, buffer_, n=3):\n",
        "    #     buffer_.pop(random.randrange(len(buffer_)))\n",
        "    #     return buffer_\n",
        "\n",
        "# while len(train_data.data)>10000:\n",
        "#     buffer.pop(random.randrange(len(buffer)))\n",
        "#     train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "de0f9c2f-c4e3-4d24-a820-1d5f13a46588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.,  0., -1., -1., -1.,  0.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1., -1.,  0.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1.,  0., -1.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1.,  0., -1., -1., -1.])\n",
            "tensor([-9.7484e-01, -7.0030e-04, -9.7754e-01, -9.9554e-01, -9.8310e-01,\n",
            "        -4.4471e-04, -6.2429e-04, -2.0100e-01, -9.7308e-01, -1.6898e-03])\n",
            "tensor([-2.1445e-03, -8.8898e-01, -9.0162e-01, -6.0961e-03, -9.7959e-01,\n",
            "        -6.7119e-01, -9.9201e-01, -6.2080e-04, -4.9569e-03, -5.0217e-01])\n",
            "tensor([-9.5651e-01, -1.2607e-03, -9.9393e-01, -5.0935e-01, -5.4060e-03,\n",
            "        -8.5862e-01, -1.3469e-03, -9.6510e-01, -9.8579e-01, -5.1930e-04])\n",
            "tensor([-3.9293e-04, -5.2593e-01, -9.9150e-01, -6.0961e-03, -9.8767e-01,\n",
            "        -4.3619e-02, -1.3140e-04, -9.9069e-01, -9.8764e-01, -9.9804e-01])\n",
            "tensor(24.0644, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.1100)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
            "reward, pred tensor([-1., -1., -1., -1.]) tensor([-0.2010, -0.0061, -0.0061, -0.0436])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=min(40, batch_size)\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "# mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "mask = (abs(labels- pred)>0.5)\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# optim = torch.optim.SGD(agent.parameters(), 1e-1, momentum=0.9, dampening=0, weight_decay=0)\n",
        "# print(optim.param_groups[0][\"lr\"])\n",
        "# print(optim)\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGYjyJZ5aG5z",
        "outputId": "9ae48025-8020-481a-99b7-fba4a3e925f8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-4211955ac4d0>:229: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repr, std, cov, clossl, z, norm 0.011543848551809788 0.427734375 0.014847248792648315 0.10995961725711823 0.07220873236656189 5.234375\n",
            "repr, std, cov, clossl, z, norm 0.010365946218371391 0.419921875 0.025938045233488083 0.09221503883600235 0.049846019595861435 3.65625\n",
            "repr, std, cov, clossl, z, norm 0.011274262331426144 0.424560546875 0.019432995468378067 0.08802013099193573 0.16931752860546112 5.41015625\n",
            "repr, std, cov, clossl, z, norm 0.012683793902397156 0.41943359375 0.02345362678170204 0.10019949078559875 0.1546027958393097 5.32421875\n",
            "repr, std, cov, clossl, z, norm 0.01209255401045084 0.424072265625 0.019356729462742805 0.09810978919267654 0.09187562018632889 4.9296875\n",
            "repr, std, cov, clossl, z, norm 0.009800915606319904 0.427734375 0.01595722883939743 0.07846468687057495 0.20382194221019745 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.010780644603073597 0.43017578125 0.014083798043429852 0.09174977988004684 0.04291040450334549 5.55859375\n",
            "repr, std, cov, clossl, z, norm 0.010112268850207329 0.429931640625 0.015099069103598595 0.08398843556642532 0.03874636068940163 5.3203125\n",
            "1\n",
            "repr, std, cov, clossl, z, norm 0.011920720338821411 0.426513671875 0.01740986853837967 0.10144540667533875 0.20755009353160858 5.640625\n",
            "repr, std, cov, clossl, z, norm 0.00958198681473732 0.4296875 0.01389111578464508 0.0811530128121376 0.1550946682691574 5.2734375\n",
            "repr, std, cov, clossl, z, norm 0.009256979450583458 0.427978515625 0.016005156561732292 0.08270430564880371 0.0013691261410713196 5.3203125\n",
            "repr, std, cov, clossl, z, norm 0.010632243938744068 0.41845703125 0.028333503752946854 0.1011277437210083 0.08659917861223221 4.98828125\n",
            "repr, std, cov, clossl, z, norm 0.01102921087294817 0.423828125 0.02174726501107216 0.0973430648446083 0.10646307468414307 5.8828125\n",
            "repr, std, cov, clossl, z, norm 0.01190927717834711 0.423583984375 0.021847786381840706 0.10509231686592102 0.05642273277044296 4.6875\n",
            "repr, std, cov, clossl, z, norm 0.010609675198793411 0.426025390625 0.01929408684372902 0.08447694778442383 0.07208926975727081 5.22265625\n",
            "repr, std, cov, clossl, z, norm 0.010908081196248531 0.425537109375 0.018818654119968414 0.09199882298707962 0.15708677470684052 4.06640625\n",
            "2\n",
            "repr, std, cov, clossl, z, norm 0.01048620231449604 0.42724609375 0.017101433128118515 0.09532833099365234 0.0744345486164093 5.609375\n",
            "repr, std, cov, clossl, z, norm 0.010320847854018211 0.429931640625 0.015793099999427795 0.09448476880788803 0.18616963922977448 5.28125\n",
            "repr, std, cov, clossl, z, norm 0.009611480869352818 0.427734375 0.019840136170387268 0.08630625903606415 0.11485825479030609 4.6640625\n",
            "repr, std, cov, clossl, z, norm 0.010132994502782822 0.4287109375 0.01683826558291912 0.09269300848245621 0.20628660917282104 5.609375\n",
            "repr, std, cov, clossl, z, norm 0.009983928874135017 0.42919921875 0.01690008118748665 0.08496128022670746 0.13050734996795654 5.0546875\n",
            "repr, std, cov, clossl, z, norm 0.011496705003082752 0.429443359375 0.016788264736533165 0.11146876960992813 0.1299019157886505 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.011084290221333504 0.428955078125 0.016633883118629456 0.09015465527772903 0.06951051205396652 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.012779278680682182 0.42724609375 0.017386632040143013 0.10941532254219055 0.1525387316942215 5.1328125\n",
            "3\n",
            "repr, std, cov, clossl, z, norm 0.012605676427483559 0.427490234375 0.016537073999643326 0.09802272915840149 0.10195275396108627 5.1015625\n",
            "repr, std, cov, clossl, z, norm 0.011576035059988499 0.430419921875 0.01467803679406643 0.10073553770780563 0.09235971421003342 5.34765625\n",
            "repr, std, cov, clossl, z, norm 0.011810374446213245 0.42578125 0.018978767096996307 0.1044718325138092 0.06842810660600662 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.009228351525962353 0.429443359375 0.014672964811325073 0.0873800739645958 0.035192131996154785 4.17578125\n",
            "repr, std, cov, clossl, z, norm 0.010273014195263386 0.426513671875 0.017178528010845184 0.10529930144548416 0.14525192975997925 5.11328125\n",
            "repr, std, cov, clossl, z, norm 0.01015612855553627 0.423828125 0.020199760794639587 0.08658178150653839 0.08570393919944763 5.16015625\n",
            "repr, std, cov, clossl, z, norm 0.0112859345972538 0.418212890625 0.025326769798994064 0.09761680662631989 0.07761096209287643 5.91796875\n",
            "repr, std, cov, clossl, z, norm 0.010596268810331821 0.42041015625 0.022560443729162216 0.09474505484104156 0.021023139357566833 5.625\n",
            "4\n",
            "repr, std, cov, clossl, z, norm 0.010601653717458248 0.41943359375 0.02354399673640728 0.10295215249061584 0.09702016413211823 5.0390625\n",
            "repr, std, cov, clossl, z, norm 0.011196457780897617 0.4208984375 0.0234394334256649 0.1025499701499939 0.11065743118524551 4.12890625\n",
            "repr, std, cov, clossl, z, norm 0.010372447781264782 0.423583984375 0.01915663667023182 0.09301485121250153 0.19500461220741272 3.171875\n",
            "repr, std, cov, clossl, z, norm 0.012205048464238644 0.4208984375 0.022214703261852264 0.11688131839036942 0.07424946874380112 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.010504956357181072 0.427490234375 0.015412546694278717 0.08981870114803314 0.05168039724230766 4.26171875\n",
            "repr, std, cov, clossl, z, norm 0.00946071557700634 0.43115234375 0.012729399837553501 0.09034016728401184 0.053920481353998184 5.3359375\n",
            "repr, std, cov, clossl, z, norm 0.009061223827302456 0.428955078125 0.015079094097018242 0.08803531527519226 0.04819358140230179 5.140625\n",
            "repr, std, cov, clossl, z, norm 0.010591709055006504 0.4248046875 0.017999446019530296 0.10759016126394272 0.03690344840288162 5.3515625\n",
            "5\n",
            "repr, std, cov, clossl, z, norm 0.009379635564982891 0.42529296875 0.018702607601881027 0.0825151801109314 0.04036388546228409 5.74609375\n",
            "repr, std, cov, clossl, z, norm 0.01066818181425333 0.423828125 0.019347839057445526 0.0971975028514862 0.07157823443412781 5.81640625\n",
            "repr, std, cov, clossl, z, norm 0.010750064626336098 0.424072265625 0.019032791256904602 0.10025618970394135 0.04305533692240715 5.6328125\n",
            "repr, std, cov, clossl, z, norm 0.010867344215512276 0.42822265625 0.015322279185056686 0.1063561737537384 0.1649041324853897 5.76953125\n",
            "repr, std, cov, clossl, z, norm 0.010822203010320663 0.42626953125 0.01738198660314083 0.10477878153324127 0.1301359087228775 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.01018126867711544 0.427734375 0.015009649097919464 0.09956813603639603 0.05865755304694176 4.9453125\n",
            "repr, std, cov, clossl, z, norm 0.008520682342350483 0.429443359375 0.014376644045114517 0.0722719132900238 0.15105682611465454 4.82421875\n",
            "repr, std, cov, clossl, z, norm 0.011332286521792412 0.422607421875 0.019670164212584496 0.11742173880338669 0.013861686922609806 5.27734375\n",
            "6\n",
            "repr, std, cov, clossl, z, norm 0.010192379355430603 0.42431640625 0.0185139998793602 0.09450355172157288 0.19572298228740692 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.010586735792458057 0.422119140625 0.019885431975126266 0.09359294921159744 0.10550399124622345 5.28515625\n",
            "repr, std, cov, clossl, z, norm 0.010404858738183975 0.423828125 0.018514718860387802 0.09752662479877472 0.2391781359910965 5.390625\n",
            "repr, std, cov, clossl, z, norm 0.010029994882643223 0.419921875 0.022896498441696167 0.08610374480485916 0.027813542634248734 5.453125\n",
            "repr, std, cov, clossl, z, norm 0.010368461720645428 0.420654296875 0.021564165130257607 0.09383918344974518 0.14391937851905823 5.11328125\n",
            "repr, std, cov, clossl, z, norm 0.010049747303128242 0.423583984375 0.019309524446725845 0.09122754633426666 0.10177507251501083 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.010118241421878338 0.421142578125 0.02075650542974472 0.08931828290224075 0.22768034040927887 5.12109375\n",
            "repr, std, cov, clossl, z, norm 0.010916497558355331 0.421875 0.019719308242201805 0.105537049472332 0.23685413599014282 5.65625\n",
            "7\n",
            "repr, std, cov, clossl, z, norm 0.010645655915141106 0.423583984375 0.018738729879260063 0.0999094769358635 0.23221132159233093 5.40625\n",
            "repr, std, cov, clossl, z, norm 0.009293265640735626 0.42724609375 0.014984821900725365 0.09035716205835342 0.0 4.953125\n",
            "repr, std, cov, clossl, z, norm 0.01053212396800518 0.424560546875 0.0166261475533247 0.10208175331354141 0.0599154457449913 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.009874892421066761 0.42626953125 0.015331132337450981 0.08722303062677383 0.0987391248345375 4.96875\n",
            "repr, std, cov, clossl, z, norm 0.01095519121736288 0.421875 0.019511746242642403 0.09822399169206619 0.16744282841682434 5.09765625\n",
            "repr, std, cov, clossl, z, norm 0.011138061992824078 0.418701171875 0.023025207221508026 0.10263260453939438 0.17118600010871887 5.30078125\n",
            "repr, std, cov, clossl, z, norm 0.010320200584828854 0.423095703125 0.0193040668964386 0.09894126653671265 0.055500149726867676 5.28515625\n",
            "repr, std, cov, clossl, z, norm 0.00931171141564846 0.426025390625 0.016933750361204147 0.0895722359418869 0.12583614885807037 5.234375\n",
            "8\n",
            "repr, std, cov, clossl, z, norm 0.009890709072351456 0.42578125 0.016205517575144768 0.09695237874984741 0.15193012356758118 5.46875\n",
            "repr, std, cov, clossl, z, norm 0.009219816885888577 0.425048828125 0.016610726714134216 0.0944075733423233 0.05841311812400818 3.998046875\n",
            "repr, std, cov, clossl, z, norm 0.01214724499732256 0.41748046875 0.024286415427923203 0.1114589273929596 0.02043049782514572 5.51171875\n",
            "repr, std, cov, clossl, z, norm 0.01047689188271761 0.418212890625 0.023937568068504333 0.08840833604335785 0.05581142380833626 5.6171875\n",
            "repr, std, cov, clossl, z, norm 0.008924301713705063 0.42236328125 0.019979558885097504 0.07047000527381897 0.03509176895022392 5.56640625\n",
            "repr, std, cov, clossl, z, norm 0.011279954575002193 0.421142578125 0.020512713119387627 0.10387016087770462 0.2598891258239746 4.36328125\n",
            "repr, std, cov, clossl, z, norm 0.010651403106749058 0.42041015625 0.021745767444372177 0.10059235990047455 0.10103414952754974 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.01017292495816946 0.41943359375 0.024546802043914795 0.09423654526472092 0.15791188180446625 4.8671875\n",
            "9\n",
            "repr, std, cov, clossl, z, norm 0.011436257511377335 0.4228515625 0.01834951341152191 0.11591601371765137 0.016993550583720207 3.904296875\n",
            "repr, std, cov, clossl, z, norm 0.009301298297941685 0.425048828125 0.0164444949477911 0.0776609554886818 0.07187653332948685 3.998046875\n",
            "repr, std, cov, clossl, z, norm 0.009541308507323265 0.425537109375 0.016487281769514084 0.07684909552335739 0.07809881865978241 5.55859375\n",
            "repr, std, cov, clossl, z, norm 0.0116005539894104 0.424072265625 0.018315870314836502 0.11212966591119766 0.027950970456004143 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.010312484577298164 0.423583984375 0.01833970844745636 0.09324266016483307 0.09417327493429184 5.5703125\n",
            "repr, std, cov, clossl, z, norm 0.009937219321727753 0.4228515625 0.018884148448705673 0.0968359187245369 0.12450280785560608 4.22265625\n",
            "repr, std, cov, clossl, z, norm 0.010397768579423428 0.421630859375 0.020222017541527748 0.09178739041090012 0.11781825870275497 5.2109375\n",
            "repr, std, cov, clossl, z, norm 0.011402717791497707 0.423583984375 0.019233398139476776 0.10392282158136368 0.027100365608930588 6.078125\n",
            "10\n",
            "repr, std, cov, clossl, z, norm 0.011129685677587986 0.423828125 0.018008597195148468 0.10144208371639252 0.05995515361428261 6.11328125\n",
            "repr, std, cov, clossl, z, norm 0.010137889534235 0.42529296875 0.017299145460128784 0.08835139125585556 0.11257991939783096 5.96484375\n",
            "repr, std, cov, clossl, z, norm 0.01061234436929226 0.423828125 0.018678903579711914 0.08950603008270264 0.14608986675739288 5.19921875\n",
            "repr, std, cov, clossl, z, norm 0.009861205704510212 0.4267578125 0.015653733164072037 0.08466079831123352 0.13473206758499146 5.49609375\n",
            "repr, std, cov, clossl, z, norm 0.009913209825754166 0.426513671875 0.015450348146259785 0.09219511598348618 0.09014607220888138 5.8125\n",
            "repr, std, cov, clossl, z, norm 0.011155805550515652 0.423095703125 0.017802953720092773 0.10181264579296112 0.08157134801149368 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.011034265160560608 0.419921875 0.021865950897336006 0.09204211831092834 0.0760306641459465 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.010682465508580208 0.41748046875 0.024497121572494507 0.08737655729055405 0.10480388253927231 5.37890625\n",
            "11\n",
            "repr, std, cov, clossl, z, norm 0.011567201465368271 0.420654296875 0.01944994553923607 0.10453946888446808 0.06187358498573303 5.5625\n",
            "repr, std, cov, clossl, z, norm 0.011808746494352818 0.42138671875 0.018704885616898537 0.09805263578891754 0.0851522833108902 5.5859375\n",
            "repr, std, cov, clossl, z, norm 0.01158063393086195 0.419921875 0.020391549915075302 0.09378005564212799 0.13107122480869293 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.011351868510246277 0.420654296875 0.020036838948726654 0.08270002156496048 0.06814399361610413 5.34375\n",
            "repr, std, cov, clossl, z, norm 0.012367506511509418 0.420654296875 0.020529232919216156 0.12349552661180496 0.18021772801876068 5.234375\n",
            "repr, std, cov, clossl, z, norm 0.010919896885752678 0.42236328125 0.019078750163316727 0.09646249562501907 0.1454055905342102 5.140625\n",
            "repr, std, cov, clossl, z, norm 0.009546637535095215 0.42333984375 0.017627663910388947 0.08414207398891449 0.05953286588191986 5.33984375\n",
            "repr, std, cov, clossl, z, norm 0.010177967138588428 0.422119140625 0.01831352896988392 0.0904635339975357 0.1824234277009964 5.33203125\n",
            "12\n",
            "repr, std, cov, clossl, z, norm 0.011733033694326878 0.4208984375 0.020445743575692177 0.09957882016897202 0.08183152973651886 3.990234375\n",
            "repr, std, cov, clossl, z, norm 0.010210561566054821 0.423095703125 0.01779237762093544 0.09729455411434174 0.1731463223695755 5.47265625\n",
            "repr, std, cov, clossl, z, norm 0.010659766383469105 0.42138671875 0.019255567342042923 0.0948060005903244 0.1331159472465515 5.2109375\n",
            "repr, std, cov, clossl, z, norm 0.009489745832979679 0.425537109375 0.015390513464808464 0.08265908807516098 0.12130658328533173 5.48828125\n",
            "repr, std, cov, clossl, z, norm 0.011035587638616562 0.41796875 0.023536793887615204 0.1005207896232605 0.13475213944911957 5.515625\n",
            "repr, std, cov, clossl, z, norm 0.009624864906072617 0.423095703125 0.01840057410299778 0.07465379685163498 0.09213501960039139 5.68359375\n",
            "repr, std, cov, clossl, z, norm 0.010159828700125217 0.42138671875 0.018926270306110382 0.08270485699176788 0.14219892024993896 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.010845014825463295 0.424560546875 0.01615513302385807 0.10739963501691818 0.22656747698783875 5.04296875\n",
            "13\n",
            "repr, std, cov, clossl, z, norm 0.010765168815851212 0.42236328125 0.01854798011481762 0.0954095795750618 0.15243761241436005 5.14453125\n",
            "repr, std, cov, clossl, z, norm 0.01096009835600853 0.41845703125 0.02244013175368309 0.08698955178260803 0.14682362973690033 5.6015625\n",
            "repr, std, cov, clossl, z, norm 0.012184551917016506 0.4150390625 0.02883807197213173 0.10487980395555496 0.05272147059440613 5.4609375\n",
            "repr, std, cov, clossl, z, norm 0.010804939083755016 0.419189453125 0.02140263468027115 0.08978984504938126 0.08301568776369095 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.01193808764219284 0.419189453125 0.024902768433094025 0.11078793555498123 0.25760266184806824 3.69921875\n",
            "repr, std, cov, clossl, z, norm 0.012291292659938335 0.420654296875 0.021714316681027412 0.1027306616306305 0.019924486055970192 5.78515625\n",
            "repr, std, cov, clossl, z, norm 0.012692282907664776 0.42431640625 0.01797257363796234 0.10390307009220123 0.05335571616888046 4.96875\n",
            "repr, std, cov, clossl, z, norm 0.011479179374873638 0.42578125 0.01627063751220703 0.10806160420179367 0.16414371132850647 5.17578125\n",
            "14\n",
            "repr, std, cov, clossl, z, norm 0.010553708299994469 0.426513671875 0.015410197898745537 0.08729564398527145 0.07666590064764023 4.84375\n",
            "repr, std, cov, clossl, z, norm 0.011076758615672588 0.42236328125 0.019680358469486237 0.10338564962148666 0.029230862855911255 5.01171875\n",
            "repr, std, cov, clossl, z, norm 0.010716244578361511 0.426025390625 0.016473403200507164 0.10111482441425323 0.1328405737876892 4.8515625\n",
            "repr, std, cov, clossl, z, norm 0.01005650870501995 0.426513671875 0.015942614525556564 0.08932512998580933 0.09349022805690765 4.9296875\n",
            "repr, std, cov, clossl, z, norm 0.010623915120959282 0.42822265625 0.014038197696208954 0.09583820402622223 0.07220824062824249 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.010919562540948391 0.427734375 0.013599954545497894 0.1081090122461319 0.2120208740234375 5.10546875\n",
            "repr, std, cov, clossl, z, norm 0.011415521614253521 0.425537109375 0.01582176610827446 0.11595813184976578 0.09350831061601639 4.65234375\n",
            "repr, std, cov, clossl, z, norm 0.010100049898028374 0.427490234375 0.01464348565787077 0.09150926023721695 0.12988683581352234 5.01171875\n",
            "15\n",
            "repr, std, cov, clossl, z, norm 0.01077224500477314 0.422119140625 0.018802901729941368 0.09167882800102234 0.024156354367733 6.28125\n",
            "repr, std, cov, clossl, z, norm 0.011629965156316757 0.419189453125 0.02190190553665161 0.1026439443230629 0.1745752990245819 5.95703125\n",
            "repr, std, cov, clossl, z, norm 0.010538156144320965 0.41748046875 0.023441169410943985 0.09880877286195755 0.05173378810286522 5.33203125\n",
            "repr, std, cov, clossl, z, norm 0.011260862462222576 0.4208984375 0.019885383546352386 0.09566247463226318 0.14923465251922607 5.13671875\n",
            "repr, std, cov, clossl, z, norm 0.01183520071208477 0.416259765625 0.02477273903787136 0.11190757155418396 0.01891498640179634 5.55859375\n",
            "repr, std, cov, clossl, z, norm 0.011413054540753365 0.418701171875 0.021374644711613655 0.10238014906644821 0.03333127126097679 5.21484375\n",
            "repr, std, cov, clossl, z, norm 0.010272547602653503 0.4267578125 0.014280721545219421 0.1084178164601326 0.08632384985685349 5.1796875\n",
            "repr, std, cov, clossl, z, norm 0.009568564593791962 0.4248046875 0.016816692426800728 0.09403662383556366 0.11644171178340912 4.78125\n",
            "16\n",
            "repr, std, cov, clossl, z, norm 0.009615031071007252 0.421875 0.01962346024811268 0.09460362046957016 0.13071532547473907 4.94921875\n",
            "repr, std, cov, clossl, z, norm 0.010092124342918396 0.4189453125 0.02136879414319992 0.09048501402139664 0.05457289144396782 5.09375\n",
            "repr, std, cov, clossl, z, norm 0.010442139580845833 0.41943359375 0.020948605611920357 0.10357557237148285 0.006109583657234907 5.328125\n",
            "repr, std, cov, clossl, z, norm 0.00967327319085598 0.420166015625 0.02189096435904503 0.08762230724096298 0.0543002150952816 5.3203125\n",
            "repr, std, cov, clossl, z, norm 0.010844947770237923 0.422119140625 0.018253717571496964 0.10989691317081451 0.1482817679643631 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.010491903871297836 0.418701171875 0.02175312489271164 0.10266358405351639 0.032460760325193405 5.16015625\n",
            "repr, std, cov, clossl, z, norm 0.009865951724350452 0.42431640625 0.016695886850357056 0.09954708069562912 0.06854794919490814 5.53125\n",
            "repr, std, cov, clossl, z, norm 0.009561712853610516 0.423583984375 0.01650036685168743 0.08572263270616531 0.05432441458106041 4.6796875\n",
            "17\n",
            "repr, std, cov, clossl, z, norm 0.010349969379603863 0.42138671875 0.018919816240668297 0.09737645834684372 0.08264978975057602 5.3671875\n",
            "repr, std, cov, clossl, z, norm 0.009754033759236336 0.42138671875 0.018320642411708832 0.08408381044864655 0.10091067105531693 5.1015625\n",
            "repr, std, cov, clossl, z, norm 0.009958974085748196 0.423583984375 0.016419904306530952 0.08873677998781204 0.0804256722331047 5.52734375\n",
            "repr, std, cov, clossl, z, norm 0.010956740006804466 0.4208984375 0.019525527954101562 0.10358888655900955 0.0629744902253151 4.96875\n",
            "repr, std, cov, clossl, z, norm 0.01161903515458107 0.417724609375 0.023201724514365196 0.1022229939699173 0.22370338439941406 5.64453125\n",
            "repr, std, cov, clossl, z, norm 0.010695402510464191 0.41748046875 0.023264318704605103 0.08935727179050446 0.050272293388843536 5.5390625\n",
            "repr, std, cov, clossl, z, norm 0.01143126655369997 0.4189453125 0.020934656262397766 0.10276469588279724 0.15197424590587616 4.51953125\n",
            "repr, std, cov, clossl, z, norm 0.010307785123586655 0.417724609375 0.022578980773687363 0.09047935903072357 0.04498545825481415 4.87890625\n",
            "18\n",
            "repr, std, cov, clossl, z, norm 0.01016527134925127 0.42138671875 0.019101014360785484 0.084929920732975 0.11370145529508591 5.2734375\n",
            "repr, std, cov, clossl, z, norm 0.011484869755804539 0.418701171875 0.021759727969765663 0.09546111524105072 0.07754674553871155 5.13671875\n",
            "repr, std, cov, clossl, z, norm 0.010645559057593346 0.421875 0.018571577966213226 0.08714306354522705 0.1143580973148346 5.1875\n",
            "repr, std, cov, clossl, z, norm 0.01126184407621622 0.421142578125 0.019337749108672142 0.10558324307203293 0.15371575951576233 5.19140625\n",
            "repr, std, cov, clossl, z, norm 0.009747558273375034 0.4296875 0.012744700536131859 0.08357369899749756 0.11675941944122314 4.9375\n",
            "repr, std, cov, clossl, z, norm 0.009611200541257858 0.4267578125 0.01563389226794243 0.089336097240448 0.10720399022102356 5.4765625\n",
            "repr, std, cov, clossl, z, norm 0.010151519440114498 0.424072265625 0.017097285017371178 0.08600464463233948 0.06951744854450226 5.48828125\n",
            "repr, std, cov, clossl, z, norm 0.010413067415356636 0.419677734375 0.022717714309692383 0.09269151836633682 0.0689445212483406 4.8046875\n",
            "19\n",
            "repr, std, cov, clossl, z, norm 0.010537059046328068 0.420166015625 0.020188534632325172 0.0994090661406517 0.0574878454208374 6.0078125\n",
            "repr, std, cov, clossl, z, norm 0.010895490646362305 0.419677734375 0.02054051123559475 0.09270097315311432 0.029387760907411575 4.15625\n",
            "repr, std, cov, clossl, z, norm 0.010351751931011677 0.420166015625 0.020341526716947556 0.0879649966955185 0.17988748848438263 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.010660636238753796 0.418701171875 0.023795463144779205 0.08650779724121094 0.15384486317634583 5.140625\n",
            "repr, std, cov, clossl, z, norm 0.010599685832858086 0.425537109375 0.01680433377623558 0.08593478798866272 0.1252591907978058 5.25390625\n",
            "repr, std, cov, clossl, z, norm 0.012946329079568386 0.4208984375 0.02229716069996357 0.12471749633550644 0.08374840021133423 5.69140625\n",
            "repr, std, cov, clossl, z, norm 0.009544436819851398 0.42578125 0.015674037858843803 0.07597151398658752 0.26434075832366943 5.8984375\n",
            "repr, std, cov, clossl, z, norm 0.010808848775923252 0.420654296875 0.0210304856300354 0.10315463691949844 0.06450258195400238 5.51171875\n",
            "20\n",
            "repr, std, cov, clossl, z, norm 0.009667960926890373 0.421630859375 0.022503692656755447 0.08048276603221893 0.13130410015583038 5.23046875\n",
            "repr, std, cov, clossl, z, norm 0.009800763800740242 0.42626953125 0.014811178669333458 0.08186399191617966 0.07984365522861481 5.09765625\n",
            "repr, std, cov, clossl, z, norm 0.010328974574804306 0.425537109375 0.017795216292142868 0.09571998566389084 0.09822961688041687 5.28125\n",
            "repr, std, cov, clossl, z, norm 0.010682952590286732 0.42431640625 0.017543867230415344 0.10632212460041046 0.082117959856987 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.009929794818162918 0.42041015625 0.020361097529530525 0.0974794551730156 0.15701909363269806 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.010677745565772057 0.41943359375 0.02131129801273346 0.10107283294200897 0.029214659705758095 5.15625\n",
            "repr, std, cov, clossl, z, norm 0.009804670698940754 0.421630859375 0.01828322745859623 0.0781366303563118 0.08624458312988281 5.6796875\n",
            "repr, std, cov, clossl, z, norm 0.010312842205166817 0.41943359375 0.021146945655345917 0.09662722051143646 0.0626070499420166 5.421875\n",
            "21\n",
            "repr, std, cov, clossl, z, norm 0.009937422350049019 0.423828125 0.017437729984521866 0.09117530286312103 0.21467691659927368 5.09765625\n",
            "repr, std, cov, clossl, z, norm 0.009846456348896027 0.425048828125 0.015702228993177414 0.09118270874023438 0.11790142208337784 5.05078125\n",
            "repr, std, cov, clossl, z, norm 0.008366244845092297 0.423828125 0.017133371904492378 0.06758030503988266 0.06820162385702133 5.6796875\n",
            "repr, std, cov, clossl, z, norm 0.01057153008878231 0.4189453125 0.02137627825140953 0.10082002729177475 0.0782754123210907 5.3359375\n",
            "repr, std, cov, clossl, z, norm 0.010415883734822273 0.419189453125 0.02086169645190239 0.09729322046041489 0.17756503820419312 4.97265625\n",
            "repr, std, cov, clossl, z, norm 0.010175437666475773 0.418701171875 0.021441882476210594 0.10536818206310272 0.11967171728610992 4.69140625\n",
            "repr, std, cov, clossl, z, norm 0.01003789622336626 0.419921875 0.020427975803613663 0.0901888757944107 0.06720241904258728 4.7890625\n",
            "repr, std, cov, clossl, z, norm 0.010239183902740479 0.41650390625 0.02420244738459587 0.09396436810493469 0.043395139276981354 4.71875\n",
            "22\n",
            "repr, std, cov, clossl, z, norm 0.011607859283685684 0.416259765625 0.024192681536078453 0.11273572593927383 0.09462370723485947 5.19921875\n",
            "repr, std, cov, clossl, z, norm 0.008730585686862469 0.423583984375 0.01646609418094158 0.0769256129860878 0.025412188842892647 5.14453125\n",
            "repr, std, cov, clossl, z, norm 0.010230512358248234 0.42333984375 0.017426667734980583 0.10080888867378235 0.07709333300590515 5.53125\n",
            "repr, std, cov, clossl, z, norm 0.009304012171924114 0.427001953125 0.015359872952103615 0.0769093707203865 0.06214238330721855 5.50390625\n",
            "repr, std, cov, clossl, z, norm 0.009366638958454132 0.4287109375 0.013422428630292416 0.09016595780849457 0.186680406332016 4.80859375\n",
            "repr, std, cov, clossl, z, norm 0.010112835094332695 0.42578125 0.014125348068773746 0.08809318393468857 0.06243874877691269 5.3359375\n",
            "repr, std, cov, clossl, z, norm 0.011133594438433647 0.419677734375 0.020702818408608437 0.10683639347553253 0.10266492515802383 5.59765625\n",
            "repr, std, cov, clossl, z, norm 0.010380501858890057 0.416259765625 0.025365464389324188 0.09217170625925064 0.021555989980697632 4.84765625\n",
            "23\n",
            "repr, std, cov, clossl, z, norm 0.010581922717392445 0.42041015625 0.018780577927827835 0.08276336640119553 0.09324076771736145 4.73046875\n",
            "repr, std, cov, clossl, z, norm 0.010245592333376408 0.422607421875 0.018289614468812943 0.0804857462644577 0.21204519271850586 5.359375\n",
            "repr, std, cov, clossl, z, norm 0.01039035338908434 0.4208984375 0.01992693915963173 0.08852115273475647 0.08600549399852753 5.12890625\n",
            "repr, std, cov, clossl, z, norm 0.010732486844062805 0.417724609375 0.023268913850188255 0.09446917474269867 0.0994587317109108 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.009968356229364872 0.42041015625 0.01942833699285984 0.08062185347080231 0.0145909758284688 6.1015625\n",
            "repr, std, cov, clossl, z, norm 0.01160083431750536 0.418212890625 0.02184874750673771 0.09929899126291275 0.19725899398326874 5.953125\n",
            "repr, std, cov, clossl, z, norm 0.009595096111297607 0.417724609375 0.022378992289304733 0.07800882309675217 0.16275833547115326 5.703125\n",
            "repr, std, cov, clossl, z, norm 0.010753684677183628 0.419189453125 0.02053065598011017 0.09721306711435318 0.0003293034387752414 5.29296875\n",
            "24\n",
            "repr, std, cov, clossl, z, norm 0.009586251340806484 0.42138671875 0.018636014312505722 0.0797746330499649 0.15611180663108826 5.26953125\n",
            "repr, std, cov, clossl, z, norm 0.010328080505132675 0.420654296875 0.019631341099739075 0.1052641049027443 0.1489170640707016 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.008947662077844143 0.421875 0.01857209950685501 0.07402843236923218 0.13531683385372162 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.009917212650179863 0.422119140625 0.01808808371424675 0.09094737470149994 0.04628291726112366 5.59765625\n",
            "repr, std, cov, clossl, z, norm 0.010545585304498672 0.42333984375 0.01719594933092594 0.09295577555894852 0.04335588961839676 5.1328125\n",
            "repr, std, cov, clossl, z, norm 0.010085513815283775 0.4208984375 0.018874583765864372 0.0919363796710968 0.10271681100130081 5.046875\n",
            "repr, std, cov, clossl, z, norm 0.009195043705403805 0.425537109375 0.015827259048819542 0.07635761052370071 0.03630143776535988 5.0859375\n",
            "repr, std, cov, clossl, z, norm 0.010901367291808128 0.420654296875 0.020321611315011978 0.1064101979136467 0.06575296074151993 5.15625\n",
            "25\n",
            "repr, std, cov, clossl, z, norm 0.01121593452990055 0.416748046875 0.02277427539229393 0.11085445433855057 0.1889282464981079 5.17578125\n",
            "repr, std, cov, clossl, z, norm 0.008472158573567867 0.42431640625 0.01591801457107067 0.06419363617897034 0.09411082416772842 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.010672170668840408 0.418701171875 0.021626919507980347 0.09507568925619125 0.07754313945770264 5.31640625\n",
            "repr, std, cov, clossl, z, norm 0.01079515926539898 0.4150390625 0.025733403861522675 0.09409619867801666 0.03843628242611885 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.010575256310403347 0.418701171875 0.020581070333719254 0.0944291353225708 0.12340807169675827 5.63671875\n",
            "repr, std, cov, clossl, z, norm 0.010410292074084282 0.41650390625 0.024858325719833374 0.08834768831729889 0.1031583920121193 4.984375\n",
            "repr, std, cov, clossl, z, norm 0.010528061538934708 0.419677734375 0.021017463877797127 0.09084701538085938 0.043374840170145035 5.28515625\n",
            "repr, std, cov, clossl, z, norm 0.009722399525344372 0.422607421875 0.016591310501098633 0.08525767177343369 0.10861898213624954 5.96484375\n",
            "26\n",
            "repr, std, cov, clossl, z, norm 0.011196598410606384 0.421875 0.01810533180832863 0.09827211499214172 0.16755925118923187 5.9609375\n",
            "repr, std, cov, clossl, z, norm 0.009863896295428276 0.42529296875 0.01532419677823782 0.07383397221565247 0.07686255127191544 5.15625\n",
            "repr, std, cov, clossl, z, norm 0.011266461573541164 0.420654296875 0.01979837752878666 0.10249602794647217 0.03842024877667427 5.34765625\n",
            "repr, std, cov, clossl, z, norm 0.009649371728301048 0.4189453125 0.02119266800582409 0.0712628960609436 0.1761297881603241 5.234375\n",
            "repr, std, cov, clossl, z, norm 0.010279829613864422 0.418701171875 0.0208868607878685 0.09020152688026428 0.0983722060918808 4.6015625\n",
            "repr, std, cov, clossl, z, norm 0.0091107077896595 0.424072265625 0.016514431685209274 0.07445774972438812 0.11616005748510361 6.01171875\n",
            "repr, std, cov, clossl, z, norm 0.009712118655443192 0.419921875 0.019844477996230125 0.0807684138417244 0.10512377321720123 5.3359375\n",
            "repr, std, cov, clossl, z, norm 0.011238704435527325 0.420166015625 0.020129729062318802 0.11344758421182632 0.020097076892852783 5.15234375\n",
            "27\n",
            "repr, std, cov, clossl, z, norm 0.009826431050896645 0.421875 0.017685171216726303 0.0860765278339386 0.08262702077627182 5.31640625\n",
            "repr, std, cov, clossl, z, norm 0.009259839542210102 0.424072265625 0.015679355710744858 0.07000841945409775 0.12572498619556427 4.859375\n",
            "repr, std, cov, clossl, z, norm 0.010269822552800179 0.418701171875 0.02085060626268387 0.0901738703250885 0.07027620077133179 4.828125\n",
            "repr, std, cov, clossl, z, norm 0.010647054761648178 0.418701171875 0.020571909844875336 0.09392816573381424 0.12300388514995575 4.90234375\n",
            "repr, std, cov, clossl, z, norm 0.010151411406695843 0.421875 0.018615856766700745 0.0800807923078537 0.04971838742494583 5.00390625\n",
            "repr, std, cov, clossl, z, norm 0.010418067686259747 0.419921875 0.018539821729063988 0.09057542681694031 0.1786758154630661 5.59765625\n",
            "repr, std, cov, clossl, z, norm 0.009716549888253212 0.420654296875 0.019545041024684906 0.07719629257917404 0.10223372280597687 5.35546875\n",
            "repr, std, cov, clossl, z, norm 0.01134438719600439 0.41650390625 0.021964428946375847 0.10814595967531204 0.19043904542922974 5.33984375\n",
            "28\n",
            "repr, std, cov, clossl, z, norm 0.010035681538283825 0.421630859375 0.01699049025774002 0.08114521205425262 0.11292456835508347 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.011154234409332275 0.418212890625 0.0206811111420393 0.09444843232631683 0.06372728198766708 5.83984375\n",
            "repr, std, cov, clossl, z, norm 0.010593063198029995 0.4189453125 0.020275801420211792 0.0947711169719696 0.06306072324514389 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.010202877223491669 0.42041015625 0.0186463575810194 0.08495093882083893 0.0576610304415226 4.98046875\n",
            "repr, std, cov, clossl, z, norm 0.00972270779311657 0.419677734375 0.02005506306886673 0.09042167663574219 0.11961223930120468 5.69921875\n",
            "repr, std, cov, clossl, z, norm 0.009821077808737755 0.416015625 0.02411811240017414 0.082811638712883 0.14606301486492157 5.03515625\n",
            "repr, std, cov, clossl, z, norm 0.009702671319246292 0.420166015625 0.019937466830015182 0.08777648955583572 0.05799442529678345 5.1015625\n",
            "repr, std, cov, clossl, z, norm 0.009749239310622215 0.41796875 0.02184901013970375 0.09151797741651535 0.08020201325416565 6.09765625\n",
            "29\n",
            "repr, std, cov, clossl, z, norm 0.010240837931632996 0.41943359375 0.019414355978369713 0.10052822530269623 0.046499401330947876 5.54296875\n",
            "repr, std, cov, clossl, z, norm 0.009851002134382725 0.42138671875 0.017586946487426758 0.08791528642177582 0.05040702223777771 4.49609375\n",
            "repr, std, cov, clossl, z, norm 0.00820703711360693 0.424560546875 0.01603585109114647 0.06182871386408806 0.09776780009269714 5.09765625\n",
            "repr, std, cov, clossl, z, norm 0.01173771359026432 0.418701171875 0.020803745836019516 0.10884078592061996 0.12396600097417831 4.90234375\n",
            "repr, std, cov, clossl, z, norm 0.009385491721332073 0.422119140625 0.016600685194134712 0.07492631673812866 0.08112916350364685 5.6015625\n",
            "repr, std, cov, clossl, z, norm 0.009655666537582874 0.4208984375 0.018908213824033737 0.07969804853200912 0.2048497498035431 5.125\n",
            "repr, std, cov, clossl, z, norm 0.009898181073367596 0.41943359375 0.019576402381062508 0.08004046976566315 0.0498824380338192 5.44921875\n",
            "repr, std, cov, clossl, z, norm 0.009532734751701355 0.4208984375 0.018127016723155975 0.07772538810968399 0.1320115476846695 4.96484375\n",
            "30\n",
            "repr, std, cov, clossl, z, norm 0.010799476876854897 0.415771484375 0.023071080446243286 0.08179216086864471 0.057406842708587646 5.16796875\n",
            "repr, std, cov, clossl, z, norm 0.010157093405723572 0.41748046875 0.021706949919462204 0.07435279339551926 0.10455617308616638 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.010616693645715714 0.421875 0.016947975382208824 0.09105197340250015 0.024152763187885284 5.15234375\n",
            "repr, std, cov, clossl, z, norm 0.010167676955461502 0.417724609375 0.020946823060512543 0.08178763091564178 0.2300625443458557 5.171875\n",
            "repr, std, cov, clossl, z, norm 0.009503151290118694 0.41748046875 0.022379418835043907 0.0758591741323471 0.16855452954769135 5.3515625\n",
            "repr, std, cov, clossl, z, norm 0.009590799920260906 0.418212890625 0.020679768174886703 0.07943398505449295 0.03590812161564827 5.44140625\n",
            "repr, std, cov, clossl, z, norm 0.009609103202819824 0.422607421875 0.017901910468935966 0.09045328199863434 0.05383841693401337 5.328125\n",
            "repr, std, cov, clossl, z, norm 0.0101343197748065 0.42041015625 0.019588584080338478 0.09086505323648453 0.19714948534965515 5.1171875\n",
            "31\n",
            "repr, std, cov, clossl, z, norm 0.010878531262278557 0.416259765625 0.02243204228579998 0.0926280990242958 0.08569099009037018 5.140625\n",
            "repr, std, cov, clossl, z, norm 0.010113494470715523 0.416015625 0.023166321218013763 0.08376306295394897 0.04193790256977081 5.62890625\n",
            "repr, std, cov, clossl, z, norm 0.009260052815079689 0.4169921875 0.02252526953816414 0.07578711956739426 0.1631355732679367 5.19921875\n",
            "repr, std, cov, clossl, z, norm 0.008638066239655018 0.420166015625 0.018975159153342247 0.06315769255161285 0.08646300435066223 5.26171875\n",
            "repr, std, cov, clossl, z, norm 0.010356447659432888 0.42041015625 0.01981203258037567 0.09616433829069138 0.17604802548885345 5.18359375\n",
            "repr, std, cov, clossl, z, norm 0.009918991476297379 0.42236328125 0.0180631335824728 0.08765122294425964 0.019640451297163963 5.19140625\n",
            "repr, std, cov, clossl, z, norm 0.00918563548475504 0.423095703125 0.017365528270602226 0.08489330112934113 0.14069001376628876 5.94140625\n",
            "repr, std, cov, clossl, z, norm 0.009623927064239979 0.41943359375 0.020153135061264038 0.0959303230047226 0.05535513535141945 5.36328125\n",
            "32\n",
            "repr, std, cov, clossl, z, norm 0.009501516819000244 0.41552734375 0.025519467890262604 0.07799377292394638 0.07279951125383377 4.9296875\n",
            "repr, std, cov, clossl, z, norm 0.01058441586792469 0.416015625 0.02238374575972557 0.09969200193881989 0.11434188485145569 4.98046875\n",
            "repr, std, cov, clossl, z, norm 0.009250783361494541 0.42041015625 0.019936032593250275 0.08130042999982834 0.11461804807186127 5.44921875\n",
            "repr, std, cov, clossl, z, norm 0.010406370274722576 0.4208984375 0.01957486942410469 0.08803970366716385 0.036613378673791885 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.00947400089353323 0.42333984375 0.01605372689664364 0.08200468122959137 0.0977620854973793 4.3046875\n",
            "repr, std, cov, clossl, z, norm 0.008635047823190689 0.424560546875 0.015777263790369034 0.06953282654285431 0.027427464723587036 5.19921875\n",
            "repr, std, cov, clossl, z, norm 0.010186993516981602 0.42041015625 0.022847700864076614 0.09078463166952133 0.16599474847316742 5.22265625\n",
            "repr, std, cov, clossl, z, norm 0.009283086284995079 0.4267578125 0.013447625562548637 0.0778542011976242 0.04917259141802788 4.83203125\n",
            "33\n",
            "repr, std, cov, clossl, z, norm 0.00968992430716753 0.42431640625 0.01578657701611519 0.07769504189491272 0.013759887777268887 5.265625\n",
            "repr, std, cov, clossl, z, norm 0.010897338390350342 0.419921875 0.02114705741405487 0.08421125262975693 0.3186244070529938 4.984375\n",
            "repr, std, cov, clossl, z, norm 0.010333286598324776 0.421142578125 0.018483933061361313 0.0883907899260521 0.05583488568663597 5.09375\n",
            "repr, std, cov, clossl, z, norm 0.010104640386998653 0.418701171875 0.019461538642644882 0.07965259999036789 0.1771671622991562 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.010480348952114582 0.418212890625 0.021312430500984192 0.08621490001678467 0.017277592793107033 4.03515625\n",
            "repr, std, cov, clossl, z, norm 0.010177168995141983 0.416259765625 0.022185180336236954 0.0784919410943985 0.08938173949718475 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.010728300549089909 0.41552734375 0.02323407493531704 0.08620668947696686 0.32613977789878845 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.010617276653647423 0.416015625 0.02227768488228321 0.08941688388586044 0.051282670348882675 5.1796875\n",
            "34\n",
            "repr, std, cov, clossl, z, norm 0.010818114504218102 0.41650390625 0.02327582612633705 0.08175027370452881 0.05554397776722908 5.42578125\n",
            "repr, std, cov, clossl, z, norm 0.010795180685818195 0.4169921875 0.021445831283926964 0.09151006489992142 0.14505289494991302 5.94921875\n",
            "repr, std, cov, clossl, z, norm 0.010019809938967228 0.419189453125 0.020038437098264694 0.07829213887453079 0.1714869886636734 5.28125\n",
            "repr, std, cov, clossl, z, norm 0.010479175485670567 0.418701171875 0.02018747106194496 0.09482526779174805 0.2287428081035614 4.69921875\n",
            "repr, std, cov, clossl, z, norm 0.009732622653245926 0.419921875 0.018830932676792145 0.08364825695753098 0.17440156638622284 5.29296875\n",
            "repr, std, cov, clossl, z, norm 0.011121945455670357 0.419189453125 0.01914551481604576 0.08971813321113586 0.08898594230413437 5.125\n",
            "repr, std, cov, clossl, z, norm 0.010261757299304008 0.420654296875 0.017303287982940674 0.09153091162443161 0.07047272473573685 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.010916516184806824 0.417724609375 0.02018406055867672 0.07742886990308762 0.0 5.34765625\n",
            "35\n",
            "repr, std, cov, clossl, z, norm 0.01038688886910677 0.419189453125 0.018605779856443405 0.07898001372814178 0.09744105488061905 5.328125\n",
            "repr, std, cov, clossl, z, norm 0.01163477636873722 0.415771484375 0.023758767172694206 0.09336119145154953 0.1707722544670105 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.010444719344377518 0.41845703125 0.020154349505901337 0.08971143513917923 0.11687759310007095 4.99609375\n",
            "repr, std, cov, clossl, z, norm 0.010306473821401596 0.4228515625 0.015931347385048866 0.0752984955906868 0.018969545140862465 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.01198702771216631 0.41845703125 0.019563857465982437 0.08832547068595886 0.14702638983726501 5.40234375\n",
            "repr, std, cov, clossl, z, norm 0.011836926452815533 0.414306640625 0.02456936426460743 0.09209807217121124 0.0012953878613188863 5.3828125\n",
            "repr, std, cov, clossl, z, norm 0.012910814024508 0.413818359375 0.026457488536834717 0.11005449295043945 0.1445201337337494 4.91015625\n",
            "repr, std, cov, clossl, z, norm 0.010804436169564724 0.4169921875 0.020829321816563606 0.08956586569547653 0.05429130420088768 5.65234375\n",
            "36\n",
            "repr, std, cov, clossl, z, norm 0.010801379568874836 0.418212890625 0.019824862480163574 0.0929754227399826 0.10588507354259491 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.010914468206465244 0.41943359375 0.018475644290447235 0.09928994625806808 0.09585826098918915 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.01008645910769701 0.42431640625 0.0163557268679142 0.08231963217258453 0.038100291043519974 4.79296875\n",
            "repr, std, cov, clossl, z, norm 0.010151134803891182 0.423828125 0.01573350466787815 0.08173654228448868 0.011855164542794228 5.6484375\n",
            "repr, std, cov, clossl, z, norm 0.010257639922201633 0.42041015625 0.022534800693392754 0.0962899923324585 0.23537378013134003 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.010776993818581104 0.42138671875 0.017257418483495712 0.09816629439592361 0.021385006606578827 5.1796875\n",
            "repr, std, cov, clossl, z, norm 0.011262067593634129 0.4150390625 0.02640063688158989 0.10602059960365295 0.049694761633872986 5.6328125\n",
            "repr, std, cov, clossl, z, norm 0.009883193299174309 0.418701171875 0.022550854831933975 0.08904267847537994 0.14367692172527313 4.640625\n",
            "37\n",
            "repr, std, cov, clossl, z, norm 0.009953104890882969 0.420654296875 0.019172701984643936 0.08491458743810654 0.06763649731874466 4.78125\n",
            "repr, std, cov, clossl, z, norm 0.009933710098266602 0.420166015625 0.018653709441423416 0.08263351023197174 0.06310054659843445 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.010381666012108326 0.4169921875 0.022070664912462234 0.1058136597275734 0.049396663904190063 3.857421875\n",
            "repr, std, cov, clossl, z, norm 0.009347436018288136 0.423095703125 0.01517730113118887 0.08816953748464584 0.0989895910024643 4.51171875\n",
            "repr, std, cov, clossl, z, norm 0.010430882684886456 0.420654296875 0.017610281705856323 0.09812295436859131 0.10382895171642303 2.412109375\n",
            "repr, std, cov, clossl, z, norm 0.009592876769602299 0.42041015625 0.01824607513844967 0.08465732634067535 0.03810488060116768 4.859375\n",
            "repr, std, cov, clossl, z, norm 0.010523689910769463 0.41845703125 0.018825341016054153 0.12037049978971481 0.07354971021413803 5.328125\n",
            "repr, std, cov, clossl, z, norm 0.010083488188683987 0.4169921875 0.021700043231248856 0.08975443989038467 0.15736201405525208 4.12890625\n",
            "38\n",
            "repr, std, cov, clossl, z, norm 0.011022007092833519 0.41552734375 0.023289285600185394 0.09824191778898239 0.14770446717739105 4.92578125\n",
            "repr, std, cov, clossl, z, norm 0.010908370837569237 0.415771484375 0.022329870611429214 0.0931747630238533 0.06338492035865784 2.67578125\n",
            "repr, std, cov, clossl, z, norm 0.01034488994628191 0.417724609375 0.020209763199090958 0.08933085948228836 0.05532602593302727 5.453125\n",
            "repr, std, cov, clossl, z, norm 0.010230720043182373 0.42236328125 0.016222279518842697 0.09523216634988785 0.05101718753576279 4.76953125\n",
            "repr, std, cov, clossl, z, norm 0.011392435990273952 0.41796875 0.020002184435725212 0.12430127710103989 0.05270858481526375 5.3515625\n",
            "repr, std, cov, clossl, z, norm 0.010869336314499378 0.41064453125 0.028074972331523895 0.0944819301366806 0.03201620653271675 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.012166781350970268 0.40771484375 0.03173740953207016 0.11754807829856873 0.12570060789585114 4.93359375\n",
            "repr, std, cov, clossl, z, norm 0.010943830944597721 0.41162109375 0.028441142290830612 0.10657865554094315 0.024835096672177315 5.33984375\n",
            "39\n",
            "repr, std, cov, clossl, z, norm 0.011031550355255604 0.420166015625 0.019095879048109055 0.11565104871988297 0.08002522587776184 5.078125\n",
            "repr, std, cov, clossl, z, norm 0.011157611384987831 0.420654296875 0.017670802772045135 0.12216262519359589 0.051015883684158325 5.0546875\n",
            "repr, std, cov, clossl, z, norm 0.01037769764661789 0.423828125 0.015576476231217384 0.09588970988988876 0.12622807919979095 1.806640625\n",
            "repr, std, cov, clossl, z, norm 0.010811340995132923 0.423583984375 0.016002338379621506 0.10279235243797302 0.11755145341157913 5.02734375\n",
            "repr, std, cov, clossl, z, norm 0.010264809243381023 0.423583984375 0.014298180118203163 0.08810783922672272 0.04710984602570534 5.05859375\n",
            "repr, std, cov, clossl, z, norm 0.011187792755663395 0.419189453125 0.019570667296648026 0.10266431421041489 0.13827750086784363 4.09765625\n",
            "repr, std, cov, clossl, z, norm 0.011706516146659851 0.416259765625 0.02230324037373066 0.1316814422607422 0.1452663391828537 4.234375\n",
            "repr, std, cov, clossl, z, norm 0.01219859067350626 0.41259765625 0.027107741683721542 0.11342998594045639 0.2148849070072174 5.16015625\n",
            "40\n",
            "repr, std, cov, clossl, z, norm 0.013344483450055122 0.414306640625 0.023610100150108337 0.1369045525789261 0.06293211877346039 4.72265625\n",
            "repr, std, cov, clossl, z, norm 0.012915081344544888 0.420654296875 0.018511883914470673 0.13562051951885223 0.07427525520324707 5.06640625\n",
            "repr, std, cov, clossl, z, norm 0.010551661252975464 0.42529296875 0.014121444895863533 0.13897840678691864 0.07540576159954071 4.9375\n",
            "repr, std, cov, clossl, z, norm 0.011398711241781712 0.420654296875 0.017465680837631226 0.10631401836872101 0.02632557787001133 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.013524902984499931 0.412841796875 0.02698523923754692 0.13114093244075775 0.1527133285999298 5.12890625\n",
            "repr, std, cov, clossl, z, norm 0.012672911398112774 0.414794921875 0.022702045738697052 0.12167969346046448 0.1641562581062317 5.26953125\n",
            "repr, std, cov, clossl, z, norm 0.013355594128370285 0.415771484375 0.021641407161951065 0.18896226584911346 0.17444448173046112 5.0078125\n",
            "repr, std, cov, clossl, z, norm 0.01257072202861309 0.41845703125 0.018635164946317673 0.16887882351875305 0.11616657674312592 4.546875\n",
            "41\n",
            "repr, std, cov, clossl, z, norm 0.012564165517687798 0.416259765625 0.021866079419851303 0.1463003158569336 0.030719168484210968 4.7265625\n",
            "repr, std, cov, clossl, z, norm 0.01146756112575531 0.420166015625 0.017931457608938217 0.14133937656879425 0.08759710192680359 3.47265625\n",
            "repr, std, cov, clossl, z, norm 0.013006308116018772 0.413818359375 0.023923033848404884 0.15824539959430695 0.14824949204921722 3.955078125\n",
            "repr, std, cov, clossl, z, norm 0.013001847080886364 0.412353515625 0.02771666646003723 0.19425514340400696 0.06878331303596497 5.5390625\n",
            "repr, std, cov, clossl, z, norm 0.012565265409648418 0.4169921875 0.02106010913848877 0.17569945752620697 0.07958591729402542 4.39453125\n",
            "repr, std, cov, clossl, z, norm 0.013163898140192032 0.41796875 0.019596565514802933 0.1688166707754135 0.21946048736572266 2.94140625\n",
            "repr, std, cov, clossl, z, norm 0.012235214002430439 0.422607421875 0.01653479039669037 0.20744101703166962 0.06980376690626144 4.375\n",
            "repr, std, cov, clossl, z, norm 0.01219798345118761 0.423095703125 0.015074491500854492 0.3302629888057709 0.033500153571367264 1.64453125\n",
            "42\n",
            "repr, std, cov, clossl, z, norm 0.017535531893372536 0.40576171875 0.0415700264275074 0.3414367735385895 0.06513111293315887 3.216796875\n",
            "repr, std, cov, clossl, z, norm 0.017737185582518578 0.41259765625 0.028078049421310425 0.36946430802345276 0.09505812078714371 3.783203125\n",
            "repr, std, cov, clossl, z, norm 0.013904916122555733 0.423583984375 0.020148713141679764 0.44584450125694275 0.08660663664340973 3.318359375\n",
            "repr, std, cov, clossl, z, norm 0.013873515650629997 0.42138671875 0.026821017265319824 0.28335821628570557 0.043413229286670685 1.9736328125\n",
            "repr, std, cov, clossl, z, norm 0.013533162884414196 0.4267578125 0.0164751298725605 0.3045024573802948 0.09851283580064774 4.7578125\n",
            "repr, std, cov, clossl, z, norm 0.014373866841197014 0.42626953125 0.01655946671962738 0.3355000615119934 0.031147539615631104 3.62890625\n",
            "repr, std, cov, clossl, z, norm 0.015220372006297112 0.425537109375 0.017399737611413002 0.3626135587692261 0.17926350235939026 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.015947770327329636 0.42333984375 0.01849767565727234 0.30542582273483276 0.09406400471925735 4.77734375\n",
            "43\n",
            "repr, std, cov, clossl, z, norm 0.015162719413638115 0.427001953125 0.01444251835346222 0.3675214648246765 0.004075977485626936 3.8828125\n",
            "repr, std, cov, clossl, z, norm 0.017952121794223785 0.423828125 0.01882709190249443 0.3658734858036041 0.16230277717113495 3.673828125\n",
            "repr, std, cov, clossl, z, norm 0.017698222771286964 0.424072265625 0.01714743673801422 0.3869714140892029 0.07492530345916748 4.01171875\n",
            "repr, std, cov, clossl, z, norm 0.017125887796282768 0.422607421875 0.018750829622149467 0.5412142276763916 0.06423544883728027 1.5078125\n",
            "repr, std, cov, clossl, z, norm 0.017927786335349083 0.4130859375 0.03174734115600586 0.40051352977752686 0.038535863161087036 0.34130859375\n",
            "repr, std, cov, clossl, z, norm 0.01654384844005108 0.421142578125 0.019373275339603424 0.435870885848999 0.17327412962913513 2.732421875\n",
            "repr, std, cov, clossl, z, norm 0.01439396571367979 0.430419921875 0.01655782386660576 0.616178035736084 0.05355120822787285 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.013498176820576191 0.4296875 0.013493889011442661 0.41417253017425537 0.07755708694458008 3.955078125\n",
            "44\n",
            "repr, std, cov, clossl, z, norm 0.015429625287652016 0.423583984375 0.018079467117786407 0.39803546667099 0.01225377433001995 1.0712890625\n",
            "repr, std, cov, clossl, z, norm 0.014877742156386375 0.425048828125 0.0179289523512125 0.4143733084201813 0.22824101150035858 3.38671875\n",
            "repr, std, cov, clossl, z, norm 0.014212507754564285 0.425048828125 0.018283121287822723 0.3971306383609772 0.07065530866384506 4.28515625\n",
            "repr, std, cov, clossl, z, norm 0.016208523884415627 0.4248046875 0.019937478005886078 0.6042959690093994 0.20486827194690704 3.77734375\n",
            "repr, std, cov, clossl, z, norm 0.020177535712718964 0.420654296875 0.02599305845797062 0.5933712720870972 0.09087494015693665 3.255859375\n",
            "repr, std, cov, clossl, z, norm 0.019109252840280533 0.4228515625 0.023384416475892067 0.5061563849449158 0.23875941336154938 3.2578125\n",
            "repr, std, cov, clossl, z, norm 0.014085086062550545 0.431884765625 0.013769306242465973 0.3838702440261841 0.036141056567430496 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.01458478532731533 0.4326171875 0.013645227067172527 0.8552933931350708 0.14536696672439575 2.814453125\n",
            "45\n",
            "repr, std, cov, clossl, z, norm 0.014122572727501392 0.425048828125 0.01925053820014 0.4370630085468292 0.12625719606876373 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.016477450728416443 0.41552734375 0.042057476937770844 0.5370320081710815 0.04471346735954285 1.9619140625\n",
            "repr, std, cov, clossl, z, norm 0.014160344377160072 0.425048828125 0.018793269991874695 0.4913148880004883 0.050751131027936935 1.490234375\n",
            "repr, std, cov, clossl, z, norm 0.014632814563810825 0.432373046875 0.014871996827423573 0.4669288396835327 0.1689709722995758 2.91796875\n",
            "repr, std, cov, clossl, z, norm 0.016360698267817497 0.43310546875 0.014486403204500675 0.5016095638275146 0.02954760752618313 3.30859375\n",
            "repr, std, cov, clossl, z, norm 0.01566922850906849 0.434326171875 0.01260640099644661 0.44453907012939453 0.0017526861047372222 3.439453125\n",
            "repr, std, cov, clossl, z, norm 0.016730045899748802 0.430419921875 0.013302217237651348 0.5250434279441833 0.22564373910427094 5.02734375\n",
            "repr, std, cov, clossl, z, norm 0.017627540975809097 0.4228515625 0.021050406619906425 0.41057088971138 0.09424281865358353 1.55859375\n",
            "46\n",
            "repr, std, cov, clossl, z, norm 0.01845705509185791 0.41943359375 0.027361439540982246 0.378351092338562 0.1210082545876503 3.005859375\n",
            "repr, std, cov, clossl, z, norm 0.022665947675704956 0.421630859375 0.026311058551073074 0.3559116721153259 0.011098377406597137 3.7890625\n",
            "repr, std, cov, clossl, z, norm 0.024357296526432037 0.435546875 0.01093742623925209 0.33326926827430725 0.17868906259536743 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.02311123162508011 0.439208984375 0.009871761314570904 0.306630402803421 0.06064896658062935 2.1171875\n",
            "repr, std, cov, clossl, z, norm 0.021440014243125916 0.43505859375 0.01271013729274273 0.3511826992034912 0.1815512776374817 3.958984375\n",
            "repr, std, cov, clossl, z, norm 0.019204100593924522 0.4326171875 0.014770383015275002 0.3775984048843384 0.03350060060620308 4.79296875\n",
            "repr, std, cov, clossl, z, norm 0.018912767991423607 0.42529296875 0.020674677565693855 0.40110450983047485 0.17236967384815216 1.6943359375\n",
            "repr, std, cov, clossl, z, norm 0.0180093701928854 0.42333984375 0.022663112729787827 0.4229332506656647 0.04274274408817291 2.54296875\n",
            "47\n",
            "repr, std, cov, clossl, z, norm 0.016085712239146233 0.4287109375 0.014634301885962486 0.2718517780303955 0.10999423265457153 3.07421875\n",
            "repr, std, cov, clossl, z, norm 0.01687581092119217 0.432861328125 0.012562820687890053 0.3304779827594757 0.07556722313165665 3.736328125\n",
            "repr, std, cov, clossl, z, norm 0.016440067440271378 0.435302734375 0.012476274743676186 0.504776120185852 0.06373545527458191 4.48828125\n",
            "repr, std, cov, clossl, z, norm 0.01630447991192341 0.43310546875 0.013979388400912285 0.345853328704834 0.05938617140054703 3.9296875\n",
            "repr, std, cov, clossl, z, norm 0.01796603947877884 0.424072265625 0.01947387307882309 0.269422709941864 0.12067850679159164 1.8291015625\n",
            "repr, std, cov, clossl, z, norm 0.019909219816327095 0.41845703125 0.027145370841026306 0.31186479330062866 0.1738262176513672 3.400390625\n",
            "repr, std, cov, clossl, z, norm 0.01886964961886406 0.42236328125 0.02063334360718727 0.2760891318321228 0.053589627146720886 3.62109375\n",
            "repr, std, cov, clossl, z, norm 0.02014900930225849 0.424560546875 0.01747417449951172 0.26665836572647095 0.09831658005714417 2.748046875\n",
            "48\n",
            "repr, std, cov, clossl, z, norm 0.020040949806571007 0.427734375 0.015347420237958431 0.19586537778377533 0.14923322200775146 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.021043820306658745 0.429443359375 0.015633858740329742 0.3270285725593567 0.08661601692438126 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.019816705957055092 0.42822265625 0.01630624383687973 0.2679033875465393 0.1514454483985901 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.019717421382665634 0.42333984375 0.021298080682754517 0.24454334378242493 0.07963700592517853 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.019999736919999123 0.425048828125 0.018219158053398132 0.280415415763855 0.23696766793727875 4.1015625\n",
            "repr, std, cov, clossl, z, norm 0.018937446177005768 0.428955078125 0.013419133611023426 0.22836317121982574 0.07096579670906067 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.01884874701499939 0.4306640625 0.013000942766666412 0.3249894380569458 0.1379804015159607 3.353515625\n",
            "repr, std, cov, clossl, z, norm 0.01994570717215538 0.428955078125 0.013939818367362022 0.2250773310661316 0.2359696328639984 3.8671875\n",
            "49\n",
            "repr, std, cov, clossl, z, norm 0.018682552501559258 0.423583984375 0.021121151745319366 0.2813747227191925 0.020741917192935944 6.37109375\n",
            "repr, std, cov, clossl, z, norm 0.016749314963817596 0.425537109375 0.016428308561444283 0.18000786006450653 0.011128660291433334 3.056640625\n",
            "repr, std, cov, clossl, z, norm 0.019722945988178253 0.416259765625 0.027006570249795914 0.26679930090904236 0.04528341069817543 4.671875\n",
            "repr, std, cov, clossl, z, norm 0.017573248594999313 0.41943359375 0.021742958575487137 0.20518836379051208 0.050464075058698654 5.0859375\n",
            "repr, std, cov, clossl, z, norm 0.01784580945968628 0.41943359375 0.022820791229605675 0.27389755845069885 0.12391968816518784 2.3515625\n",
            "repr, std, cov, clossl, z, norm 0.015572087839245796 0.424072265625 0.0172848142683506 0.19130006432533264 0.14728692173957825 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.01586751826107502 0.423095703125 0.019531765952706337 0.30387067794799805 0.06943026185035706 3.28125\n",
            "repr, std, cov, clossl, z, norm 0.016463860869407654 0.419189453125 0.023846199735999107 0.2138945758342743 0.21332703530788422 3.408203125\n",
            "50\n",
            "repr, std, cov, clossl, z, norm 0.01723482646048069 0.4189453125 0.021778710186481476 0.22983166575431824 0.02151697315275669 3.818359375\n",
            "repr, std, cov, clossl, z, norm 0.01565762422978878 0.420654296875 0.019018035382032394 0.17239414155483246 0.13853657245635986 4.953125\n",
            "repr, std, cov, clossl, z, norm 0.016513777896761894 0.421875 0.018711460754275322 0.18915675580501556 0.07344072312116623 4.69921875\n",
            "repr, std, cov, clossl, z, norm 0.014837455004453659 0.42578125 0.015084986574947834 0.2606077492237091 0.09137575328350067 3.7109375\n",
            "repr, std, cov, clossl, z, norm 0.015355579555034637 0.427734375 0.014239510521292686 0.2007618248462677 0.14017654955387115 4.4921875\n",
            "repr, std, cov, clossl, z, norm 0.015538234263658524 0.42236328125 0.018735207617282867 0.18201354146003723 0.11746238172054291 3.8828125\n",
            "repr, std, cov, clossl, z, norm 0.014682967215776443 0.42333984375 0.01966690458357334 0.17930786311626434 0.04173825681209564 3.59375\n",
            "repr, std, cov, clossl, z, norm 0.015374908223748207 0.42333984375 0.018516626209020615 0.24155780673027039 0.16087661683559418 4.26171875\n",
            "51\n",
            "repr, std, cov, clossl, z, norm 0.014171839691698551 0.424560546875 0.016406234353780746 0.1390579789876938 0.14038722217082977 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.015105278231203556 0.4248046875 0.016115952283143997 0.15430407226085663 0.1532537341117859 3.51953125\n",
            "repr, std, cov, clossl, z, norm 0.01515024434775114 0.42431640625 0.016623420640826225 0.14025834202766418 0.16218172013759613 5.296875\n",
            "repr, std, cov, clossl, z, norm 0.016941888257861137 0.423583984375 0.018186276778578758 0.19147849082946777 0.010386635549366474 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.01635659672319889 0.423095703125 0.017739446833729744 0.15110467374324799 0.1584673821926117 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.016889294609427452 0.421142578125 0.01878199353814125 0.18674995005130768 0.074119433760643 5.17578125\n",
            "repr, std, cov, clossl, z, norm 0.01705743744969368 0.41845703125 0.021874479949474335 0.16975845396518707 0.11077429354190826 5.046875\n",
            "repr, std, cov, clossl, z, norm 0.017267370596528053 0.415771484375 0.024378597736358643 0.1784593164920807 0.05794619768857956 4.671875\n",
            "52\n",
            "repr, std, cov, clossl, z, norm 0.017767267301678658 0.415283203125 0.023713892325758934 0.153227761387825 0.09802153706550598 3.7421875\n",
            "repr, std, cov, clossl, z, norm 0.01781090907752514 0.418701171875 0.02168523147702217 0.19001853466033936 0.25340723991394043 5.91796875\n",
            "repr, std, cov, clossl, z, norm 0.016899479553103447 0.421142578125 0.017356768250465393 0.19351013004779816 0.049420345574617386 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.01576317474246025 0.420166015625 0.017660170793533325 0.18378087878227234 0.135450080037117 4.31640625\n",
            "repr, std, cov, clossl, z, norm 0.016539201140403748 0.4189453125 0.020814744755625725 0.15700307488441467 0.05867437645792961 3.470703125\n",
            "repr, std, cov, clossl, z, norm 0.01697140745818615 0.41845703125 0.02018454670906067 0.20741818845272064 0.1780412644147873 5.53125\n",
            "repr, std, cov, clossl, z, norm 0.016129476949572563 0.41357421875 0.0259169340133667 0.18004092574119568 0.06684589385986328 4.984375\n",
            "repr, std, cov, clossl, z, norm 0.015362310223281384 0.41796875 0.020162273198366165 0.16924627125263214 0.052463237196207047 4.12109375\n",
            "53\n",
            "repr, std, cov, clossl, z, norm 0.0139989722520113 0.42041015625 0.018934819847345352 0.16480372846126556 0.08731767535209656 3.9921875\n",
            "repr, std, cov, clossl, z, norm 0.015179065056145191 0.41943359375 0.018941419199109077 0.16117072105407715 0.09853900223970413 2.587890625\n",
            "repr, std, cov, clossl, z, norm 0.016042666509747505 0.41650390625 0.020658813416957855 0.16498397290706635 0.07220111042261124 2.609375\n",
            "repr, std, cov, clossl, z, norm 0.014273307286202908 0.419189453125 0.018178705126047134 0.13377144932746887 0.08885926008224487 4.82421875\n",
            "repr, std, cov, clossl, z, norm 0.01686556451022625 0.41943359375 0.019103890284895897 0.20156532526016235 0.11687466502189636 5.2890625\n",
            "repr, std, cov, clossl, z, norm 0.016166960820555687 0.422607421875 0.016064701601862907 0.21598243713378906 0.08212914317846298 3.95703125\n",
            "repr, std, cov, clossl, z, norm 0.015005223453044891 0.42041015625 0.01846686378121376 0.153915673494339 0.1631443351507187 4.55078125\n",
            "repr, std, cov, clossl, z, norm 0.015499587170779705 0.4208984375 0.017266321927309036 0.17482131719589233 0.031043458729982376 3.310546875\n",
            "54\n",
            "repr, std, cov, clossl, z, norm 0.01582466810941696 0.416259765625 0.02110682986676693 0.15378306806087494 0.1241815835237503 3.23046875\n",
            "repr, std, cov, clossl, z, norm 0.015455295331776142 0.418212890625 0.018568754196166992 0.1439245343208313 0.1299973577260971 3.40234375\n",
            "repr, std, cov, clossl, z, norm 0.014792687259614468 0.41845703125 0.019016006961464882 0.1464037299156189 0.12426929175853729 2.634765625\n",
            "repr, std, cov, clossl, z, norm 0.014082963578402996 0.41796875 0.018864907324314117 0.1294846534729004 0.06088174879550934 3.314453125\n",
            "repr, std, cov, clossl, z, norm 0.014866666868329048 0.41845703125 0.018118754029273987 0.18609429895877838 0.1342695653438568 3.359375\n",
            "repr, std, cov, clossl, z, norm 0.014987225644290447 0.416015625 0.020574990659952164 0.14496438205242157 0.13950596749782562 3.166015625\n",
            "repr, std, cov, clossl, z, norm 0.015096251852810383 0.414794921875 0.02133892849087715 0.12977293133735657 0.08938877284526825 3.3046875\n",
            "repr, std, cov, clossl, z, norm 0.01571308635175228 0.41552734375 0.021591652184724808 0.19081902503967285 0.06808555126190186 5.05859375\n",
            "55\n",
            "repr, std, cov, clossl, z, norm 0.014787883497774601 0.41259765625 0.02423097938299179 0.1423170417547226 0.1870676577091217 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.013831472024321556 0.414306640625 0.021519139409065247 0.11100275069475174 0.133670836687088 3.69921875\n",
            "repr, std, cov, clossl, z, norm 0.015138218179345131 0.4130859375 0.023400427773594856 0.14905528724193573 0.11136337369680405 3.69921875\n",
            "repr, std, cov, clossl, z, norm 0.013172004371881485 0.420166015625 0.017152076587080956 0.1373201310634613 0.06309189647436142 5.12890625\n",
            "repr, std, cov, clossl, z, norm 0.014498131349682808 0.4189453125 0.018409648910164833 0.1588674634695053 0.06012871488928795 3.88671875\n",
            "repr, std, cov, clossl, z, norm 0.016104228794574738 0.41650390625 0.019721241667866707 0.15758702158927917 0.11446361243724823 4.59765625\n",
            "repr, std, cov, clossl, z, norm 0.014735538512468338 0.41259765625 0.024700341746211052 0.13460825383663177 0.007980864495038986 3.947265625\n",
            "repr, std, cov, clossl, z, norm 0.01697479747235775 0.41162109375 0.025098856538534164 0.17323240637779236 0.13095150887966156 4.7734375\n",
            "56\n",
            "repr, std, cov, clossl, z, norm 0.01605127565562725 0.4140625 0.022005241364240646 0.1510719209909439 0.07024909555912018 5.26171875\n",
            "repr, std, cov, clossl, z, norm 0.013587536290287971 0.417724609375 0.01836516335606575 0.11284594982862473 0.1924377977848053 5.59765625\n",
            "repr, std, cov, clossl, z, norm 0.01384023204445839 0.421875 0.015244805254042149 0.14386463165283203 0.03978826850652695 4.515625\n",
            "repr, std, cov, clossl, z, norm 0.012049313634634018 0.42138671875 0.017372366040945053 0.11504208296537399 0.04077669978141785 4.5546875\n",
            "repr, std, cov, clossl, z, norm 0.012736432254314423 0.4208984375 0.017067331820726395 0.10917910933494568 0.1511463224887848 5.515625\n",
            "repr, std, cov, clossl, z, norm 0.012632311321794987 0.42138671875 0.016171880066394806 0.11851111799478531 0.11209709942340851 4.87109375\n",
            "repr, std, cov, clossl, z, norm 0.013161842711269855 0.421142578125 0.016636736690998077 0.13242307305335999 0.031755369156599045 4.125\n",
            "repr, std, cov, clossl, z, norm 0.015014194883406162 0.4140625 0.023298148065805435 0.1337425410747528 0.021199693903326988 5.47265625\n",
            "57\n",
            "repr, std, cov, clossl, z, norm 0.014985529705882072 0.4111328125 0.025033656507730484 0.13316212594509125 0.05369872227311134 4.98828125\n",
            "repr, std, cov, clossl, z, norm 0.015619619749486446 0.413330078125 0.021416770294308662 0.12902653217315674 0.04024602845311165 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.013130648992955685 0.417236328125 0.018890267238020897 0.09660544991493225 0.09209184348583221 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.014011424034833908 0.416259765625 0.020693186670541763 0.12347808480262756 0.054254982620477676 3.814453125\n",
            "repr, std, cov, clossl, z, norm 0.014455397613346577 0.420166015625 0.0178978331387043 0.11540040373802185 0.28064069151878357 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.013656212948262691 0.419677734375 0.01773669198155403 0.10896670073270798 0.04478403553366661 3.75\n",
            "repr, std, cov, clossl, z, norm 0.012967290356755257 0.422607421875 0.014567532576620579 0.11392876505851746 0.137289360165596 5.41015625\n",
            "repr, std, cov, clossl, z, norm 0.013667107559740543 0.41796875 0.018902398645877838 0.14743047952651978 0.060015030205249786 4.76953125\n",
            "58\n",
            "repr, std, cov, clossl, z, norm 0.013135668821632862 0.41748046875 0.018649518489837646 0.1066313311457634 0.07278504967689514 5.83984375\n",
            "repr, std, cov, clossl, z, norm 0.013193856924772263 0.413330078125 0.02302820421755314 0.10335647314786911 0.08811479806900024 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.012829819694161415 0.413330078125 0.022388527169823647 0.09365735948085785 0.0822688639163971 5.57421875\n",
            "repr, std, cov, clossl, z, norm 0.013416624628007412 0.411376953125 0.025754719972610474 0.09967426210641861 0.2285194844007492 5.875\n",
            "repr, std, cov, clossl, z, norm 0.01248073298484087 0.414306640625 0.021498190239071846 0.08921031653881073 0.0689672902226448 5.078125\n",
            "repr, std, cov, clossl, z, norm 0.014250336214900017 0.4111328125 0.02393384464085102 0.11118531972169876 0.09675890952348709 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.014497813768684864 0.4130859375 0.021591920405626297 0.13012483716011047 0.011041962541639805 3.240234375\n",
            "repr, std, cov, clossl, z, norm 0.013141055591404438 0.4169921875 0.018611310049891472 0.14953111112117767 0.038936056196689606 4.87109375\n",
            "59\n",
            "repr, std, cov, clossl, z, norm 0.013562168926000595 0.4169921875 0.018818436190485954 0.102939173579216 0.04757087677717209 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.012464885599911213 0.416259765625 0.019520223140716553 0.10310211032629013 0.2144070714712143 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.01341529842466116 0.41552734375 0.01988501287996769 0.10050293803215027 0.11832472681999207 3.94921875\n",
            "repr, std, cov, clossl, z, norm 0.01329458225518465 0.415771484375 0.01963680237531662 0.10628511011600494 0.10368992388248444 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.01434208732098341 0.41455078125 0.020321166142821312 0.11831216514110565 0.05739743635058403 6.05859375\n",
            "repr, std, cov, clossl, z, norm 0.013988115824759007 0.41259765625 0.023431500419974327 0.10827365517616272 0.0411817729473114 4.6171875\n",
            "repr, std, cov, clossl, z, norm 0.013710317201912403 0.416748046875 0.019854824990034103 0.1077040582895279 0.023709414526820183 5.109375\n",
            "repr, std, cov, clossl, z, norm 0.013373877853155136 0.419677734375 0.01587572880089283 0.10434281826019287 0.06306829303503036 5.14453125\n",
            "60\n",
            "repr, std, cov, clossl, z, norm 0.014118523336946964 0.41845703125 0.016509830951690674 0.12004087120294571 0.3115380108356476 5.52734375\n",
            "repr, std, cov, clossl, z, norm 0.012087673880159855 0.416015625 0.019398869946599007 0.10065850615501404 0.05690097436308861 4.69140625\n",
            "repr, std, cov, clossl, z, norm 0.012123427353799343 0.4169921875 0.018576249480247498 0.0983300507068634 0.0950576588511467 2.7890625\n",
            "repr, std, cov, clossl, z, norm 0.013187943026423454 0.41162109375 0.023338420316576958 0.11444953829050064 0.0842527523636818 3.349609375\n",
            "repr, std, cov, clossl, z, norm 0.01297390554100275 0.411376953125 0.02337823435664177 0.10074875503778458 0.07388196885585785 3.75\n",
            "repr, std, cov, clossl, z, norm 0.013895461335778236 0.409423828125 0.024757714942097664 0.10802631080150604 0.1894679218530655 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.01198861189186573 0.414306640625 0.021713808178901672 0.09601247310638428 0.07699040323495865 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.01177285797894001 0.4169921875 0.018511435016989708 0.09456028789281845 0.10246354341506958 4.546875\n",
            "61\n",
            "repr, std, cov, clossl, z, norm 0.012593399733304977 0.41455078125 0.020032741129398346 0.11425773054361343 0.1447550356388092 5.03515625\n",
            "repr, std, cov, clossl, z, norm 0.012437690980732441 0.411376953125 0.024449490010738373 0.10052725672721863 0.0965842604637146 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.012694858945906162 0.413330078125 0.02096022106707096 0.08928186446428299 0.16374075412750244 4.59375\n",
            "repr, std, cov, clossl, z, norm 0.013037461787462234 0.41650390625 0.01856844872236252 0.10167571902275085 0.2280961573123932 4.92578125\n",
            "repr, std, cov, clossl, z, norm 0.012480995617806911 0.414306640625 0.021221116185188293 0.09005922079086304 0.05472203716635704 5.33203125\n",
            "repr, std, cov, clossl, z, norm 0.012587987817823887 0.41650390625 0.018819719552993774 0.08435146510601044 0.02848868817090988 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.011352701112627983 0.4130859375 0.022586937993764877 0.09152200818061829 0.0872943252325058 3.291015625\n",
            "repr, std, cov, clossl, z, norm 0.012621115893125534 0.415771484375 0.01951759308576584 0.11074105650186539 0.16000783443450928 3.642578125\n",
            "62\n",
            "repr, std, cov, clossl, z, norm 0.011584700085222721 0.41943359375 0.016225069761276245 0.09588732570409775 0.06917005777359009 4.703125\n",
            "repr, std, cov, clossl, z, norm 0.01187180820852518 0.419189453125 0.01637352630496025 0.09624157100915909 0.07709629088640213 5.56640625\n",
            "repr, std, cov, clossl, z, norm 0.01056600920855999 0.421142578125 0.014288611710071564 0.08312038332223892 0.2750840187072754 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.012237638235092163 0.41650390625 0.018711229786276817 0.10123124718666077 0.07780518382787704 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.01217713113874197 0.408447265625 0.026916123926639557 0.10594001412391663 0.18875835835933685 3.41015625\n",
            "repr, std, cov, clossl, z, norm 0.01139820832759142 0.409423828125 0.024747325107455254 0.08742904663085938 0.0909833237528801 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.011518348939716816 0.4140625 0.021799106150865555 0.09846050292253494 0.16123133897781372 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.012014641426503658 0.412353515625 0.023158036172389984 0.10486453026533127 0.0842517539858818 4.9140625\n",
            "63\n",
            "repr, std, cov, clossl, z, norm 0.011161255650222301 0.412841796875 0.020530449226498604 0.08536624908447266 0.1714412420988083 5.3984375\n",
            "repr, std, cov, clossl, z, norm 0.012142126448452473 0.4130859375 0.022357812151312828 0.10415820777416229 0.1568351835012436 5.70703125\n",
            "repr, std, cov, clossl, z, norm 0.01136584859341383 0.41259765625 0.023028360679745674 0.0854983702301979 0.07107166200876236 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.011249125003814697 0.416015625 0.019128024578094482 0.0862310603260994 0.07803145051002502 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.011169051751494408 0.414794921875 0.02022881619632244 0.08303507417440414 0.06426157802343369 5.2109375\n",
            "repr, std, cov, clossl, z, norm 0.010785078629851341 0.419189453125 0.01725085824728012 0.07965877652168274 0.09814389795064926 4.28515625\n",
            "repr, std, cov, clossl, z, norm 0.010921720415353775 0.4189453125 0.017601849511265755 0.0808655172586441 0.07483968883752823 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.012499307282269001 0.413818359375 0.020890895277261734 0.10396899282932281 0.1139468401670456 4.3125\n",
            "64\n",
            "repr, std, cov, clossl, z, norm 0.010922577232122421 0.417724609375 0.01752942055463791 0.0790676698088646 0.10042238235473633 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.012232684530317783 0.411865234375 0.023755423724651337 0.10219952464103699 0.09719540923833847 4.4296875\n",
            "repr, std, cov, clossl, z, norm 0.01112316269427538 0.412841796875 0.021299179643392563 0.08643035590648651 0.1699242740869522 5.3671875\n",
            "repr, std, cov, clossl, z, norm 0.01081258524209261 0.41748046875 0.016921814531087875 0.08231116086244583 0.13060860335826874 4.6953125\n",
            "repr, std, cov, clossl, z, norm 0.011252252385020256 0.41748046875 0.01763308048248291 0.08970069885253906 0.1467055082321167 4.83984375\n",
            "repr, std, cov, clossl, z, norm 0.010824565775692463 0.41650390625 0.018042370676994324 0.08667618781328201 0.07294170558452606 4.75\n",
            "repr, std, cov, clossl, z, norm 0.011549885384738445 0.411865234375 0.02293461561203003 0.08642853796482086 0.04829549416899681 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.011795337311923504 0.410400390625 0.023380719125270844 0.092599056661129 0.1476869434118271 4.13671875\n",
            "65\n",
            "repr, std, cov, clossl, z, norm 0.012532706372439861 0.40869140625 0.025741353631019592 0.10258161276578903 0.15661096572875977 4.2890625\n",
            "repr, std, cov, clossl, z, norm 0.011303216218948364 0.407470703125 0.02841058373451233 0.07398705929517746 0.1284795105457306 4.640625\n",
            "repr, std, cov, clossl, z, norm 0.01160450465977192 0.414306640625 0.020228520035743713 0.08617149293422699 0.042525384575128555 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.011293494142591953 0.41259765625 0.022846166044473648 0.0906352698802948 0.04861074313521385 5.07421875\n",
            "repr, std, cov, clossl, z, norm 0.011277812533080578 0.41455078125 0.021428994834423065 0.08151791989803314 0.13735276460647583 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.011308168061077595 0.414794921875 0.019402164965867996 0.09675586223602295 0.08858011662960052 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.010822596028447151 0.41796875 0.018461031839251518 0.07202732563018799 0.07113170623779297 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.010562760755419731 0.416015625 0.019073564559221268 0.07344474643468857 0.12812571227550507 5.25\n",
            "66\n",
            "repr, std, cov, clossl, z, norm 0.011698109097778797 0.413818359375 0.020073281601071358 0.0862557590007782 0.14477713406085968 6.1640625\n",
            "repr, std, cov, clossl, z, norm 0.010055765509605408 0.41748046875 0.017504997551441193 0.06779791414737701 0.047649119049310684 4.546875\n",
            "repr, std, cov, clossl, z, norm 0.010900280438363552 0.413330078125 0.021226990967988968 0.08054374158382416 0.0424155667424202 5.21875\n",
            "repr, std, cov, clossl, z, norm 0.011340413242578506 0.413330078125 0.020815517753362656 0.0730072483420372 0.0711909607052803 4.6015625\n",
            "repr, std, cov, clossl, z, norm 0.011082218959927559 0.4140625 0.020523086190223694 0.08405906707048416 0.12107103317975998 4.12890625\n",
            "repr, std, cov, clossl, z, norm 0.010661978274583817 0.4150390625 0.018842967227101326 0.08700934797525406 0.11776812374591827 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.011889942921698093 0.410400390625 0.023546313866972923 0.08409486711025238 0.09395500272512436 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.011714203283190727 0.41259765625 0.020417001098394394 0.08296497911214828 0.16687101125717163 4.75390625\n",
            "67\n",
            "repr, std, cov, clossl, z, norm 0.01035567931830883 0.4140625 0.019968442618846893 0.07069132477045059 0.1764822006225586 4.39453125\n",
            "repr, std, cov, clossl, z, norm 0.012744245119392872 0.408447265625 0.026018327102065086 0.09056022763252258 0.22578521072864532 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.011545454151928425 0.414794921875 0.018720943480730057 0.07634024322032928 0.3825269639492035 4.68359375\n",
            "repr, std, cov, clossl, z, norm 0.012545914389193058 0.41357421875 0.019852591678500175 0.084825798869133 0.06279782205820084 3.537109375\n",
            "repr, std, cov, clossl, z, norm 0.01304096169769764 0.408203125 0.024903206154704094 0.08449521660804749 0.16813567280769348 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.012959148734807968 0.4091796875 0.024009844288229942 0.08194553107023239 0.09900405257940292 4.8984375\n",
            "repr, std, cov, clossl, z, norm 0.012892376631498337 0.40576171875 0.027607731521129608 0.09396955370903015 0.1502411961555481 3.796875\n",
            "repr, std, cov, clossl, z, norm 0.010958494618535042 0.4140625 0.01982332207262516 0.07878904044628143 0.07964972406625748 4.125\n",
            "68\n",
            "repr, std, cov, clossl, z, norm 0.010284986346960068 0.4140625 0.018968120217323303 0.07161571830511093 0.11554614454507828 4.75\n",
            "repr, std, cov, clossl, z, norm 0.012296945787966251 0.41162109375 0.021979765966534615 0.08867798000574112 0.08788828551769257 5.01953125\n",
            "repr, std, cov, clossl, z, norm 0.010806391015648842 0.415771484375 0.01723537966609001 0.0725371316075325 0.11605275422334671 5.49609375\n",
            "repr, std, cov, clossl, z, norm 0.012034532614052296 0.41259765625 0.020305350422859192 0.08679983019828796 0.08247028291225433 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.011208717711269855 0.416015625 0.017665263265371323 0.08413434773683548 0.16786551475524902 5.5078125\n",
            "repr, std, cov, clossl, z, norm 0.011735896579921246 0.4130859375 0.019779380410909653 0.07906267791986465 0.21714827418327332 4.5859375\n",
            "repr, std, cov, clossl, z, norm 0.01080008689314127 0.4130859375 0.01985492929816246 0.06913303583860397 0.15266862511634827 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.012350981123745441 0.409423828125 0.023875843733549118 0.09638404846191406 0.04534797742962837 4.80078125\n",
            "69\n",
            "repr, std, cov, clossl, z, norm 0.011157927103340626 0.411376953125 0.021481793373823166 0.06845885515213013 0.07743921875953674 4.2890625\n",
            "repr, std, cov, clossl, z, norm 0.011201166547834873 0.408935546875 0.02352498471736908 0.08416757732629776 0.10382814705371857 4.67578125\n",
            "repr, std, cov, clossl, z, norm 0.01113195065408945 0.412353515625 0.020768288522958755 0.07344291359186172 0.1002521887421608 4.43359375\n",
            "repr, std, cov, clossl, z, norm 0.010936692357063293 0.410888671875 0.02145223692059517 0.07946866005659103 0.21156996488571167 4.05078125\n",
            "repr, std, cov, clossl, z, norm 0.011747858487069607 0.412109375 0.02098943293094635 0.09247791022062302 0.03378954157233238 4.8984375\n",
            "repr, std, cov, clossl, z, norm 0.010866665281355381 0.41162109375 0.021272385492920876 0.07360909879207611 0.17784236371517181 3.783203125\n",
            "repr, std, cov, clossl, z, norm 0.01102451141923666 0.408935546875 0.023817267268896103 0.072834812104702 0.22495146095752716 4.74609375\n",
            "repr, std, cov, clossl, z, norm 0.011313608847558498 0.411376953125 0.021268436685204506 0.08226685971021652 0.15281549096107483 4.84765625\n",
            "70\n",
            "repr, std, cov, clossl, z, norm 0.01165393553674221 0.4072265625 0.025637343525886536 0.08630756288766861 0.060108959674835205 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.010116803459823132 0.413818359375 0.01925419270992279 0.07156896591186523 0.2273530662059784 4.140625\n",
            "repr, std, cov, clossl, z, norm 0.011023993603885174 0.40966796875 0.02408239245414734 0.07595352083444595 0.07535440474748611 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.011389657855033875 0.4091796875 0.024933867156505585 0.07806641608476639 0.019743628799915314 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.011053206399083138 0.413818359375 0.019066859036684036 0.10064506530761719 0.05966298282146454 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.010817336849868298 0.4140625 0.018620945513248444 0.08007338643074036 0.17210349440574646 4.1796875\n",
            "repr, std, cov, clossl, z, norm 0.009623643942177296 0.413818359375 0.019655829295516014 0.06201058253645897 0.11132088303565979 4.734375\n",
            "repr, std, cov, clossl, z, norm 0.011418387293815613 0.41015625 0.022859664633870125 0.10022984445095062 0.14651329815387726 4.9453125\n",
            "71\n",
            "repr, std, cov, clossl, z, norm 0.01182874757796526 0.412109375 0.021121595054864883 0.07858120650053024 0.11408455669879913 4.6640625\n",
            "repr, std, cov, clossl, z, norm 0.011514399200677872 0.40869140625 0.024964455515146255 0.08071759343147278 0.06103409081697464 4.83984375\n",
            "repr, std, cov, clossl, z, norm 0.01148161105811596 0.413330078125 0.01929326541721821 0.0771586075425148 0.06652674078941345 4.8515625\n",
            "repr, std, cov, clossl, z, norm 0.011960131116211414 0.4111328125 0.02133619785308838 0.0906972885131836 0.12437368184328079 4.65625\n",
            "repr, std, cov, clossl, z, norm 0.011204656213521957 0.40966796875 0.024191323667764664 0.07799572497606277 0.0755762904882431 4.63671875\n",
            "repr, std, cov, clossl, z, norm 0.011452140286564827 0.40966796875 0.023460477590560913 0.08403977006673813 0.12147688865661621 5.1875\n",
            "repr, std, cov, clossl, z, norm 0.011616243049502373 0.40673828125 0.030515577644109726 0.08798627555370331 0.12440817058086395 4.62890625\n",
            "repr, std, cov, clossl, z, norm 0.009502764791250229 0.4189453125 0.015603242442011833 0.06426931917667389 0.004659619648009539 4.2578125\n",
            "72\n",
            "repr, std, cov, clossl, z, norm 0.011024551466107368 0.4130859375 0.020271306857466698 0.0881546214222908 0.11612240225076675 5.04296875\n",
            "repr, std, cov, clossl, z, norm 0.009426252916455269 0.41796875 0.017333952710032463 0.07063031941652298 0.11560261249542236 3.466796875\n",
            "repr, std, cov, clossl, z, norm 0.01041384693235159 0.4169921875 0.017837699502706528 0.07939497381448746 0.1403948813676834 4.4765625\n",
            "repr, std, cov, clossl, z, norm 0.00938999094069004 0.4189453125 0.01601218618452549 0.07190636545419693 0.03759156912565231 4.9453125\n",
            "repr, std, cov, clossl, z, norm 0.010445740073919296 0.41650390625 0.01854844018816948 0.07163941860198975 0.0020011886954307556 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.011320037767291069 0.413818359375 0.01979058049619198 0.0960659459233284 0.06426779925823212 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.011175897903740406 0.40673828125 0.028967229649424553 0.08132419735193253 0.029314812272787094 5.109375\n",
            "repr, std, cov, clossl, z, norm 0.011354519985616207 0.40966796875 0.023965219035744667 0.08701137453317642 0.06024644896388054 5.0\n",
            "73\n",
            "repr, std, cov, clossl, z, norm 0.01065064687281847 0.40869140625 0.025676283985376358 0.0665818303823471 0.06642086058855057 4.45703125\n",
            "repr, std, cov, clossl, z, norm 0.011176707223057747 0.41162109375 0.021803786978125572 0.07988376915454865 0.13223938643932343 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.011431402526795864 0.41162109375 0.02261742204427719 0.08390022069215775 0.13995186984539032 4.69921875\n",
            "repr, std, cov, clossl, z, norm 0.010406289249658585 0.41259765625 0.020423494279384613 0.08248849958181381 0.20363849401474 4.5390625\n",
            "repr, std, cov, clossl, z, norm 0.010705320164561272 0.413818359375 0.019797151908278465 0.07796859741210938 0.09301714599132538 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.010114780627191067 0.416015625 0.018097754567861557 0.0686001256108284 0.10942157357931137 4.15234375\n",
            "repr, std, cov, clossl, z, norm 0.010322777554392815 0.415771484375 0.018040303140878677 0.08724261075258255 0.05039836838841438 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.0102831507101655 0.416015625 0.018121618777513504 0.07778575271368027 0.08988786488771439 3.02734375\n",
            "74\n",
            "repr, std, cov, clossl, z, norm 0.010209325700998306 0.4130859375 0.020410902798175812 0.07804209738969803 0.050572920590639114 5.08984375\n",
            "repr, std, cov, clossl, z, norm 0.010966288857161999 0.409912109375 0.02352948859333992 0.0740523636341095 0.12753620743751526 4.4765625\n",
            "repr, std, cov, clossl, z, norm 0.010956098325550556 0.410888671875 0.02168624848127365 0.06596218049526215 0.1868441253900528 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.01212928257882595 0.41015625 0.022370517253875732 0.07624243199825287 0.12953302264213562 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.011788242496550083 0.408203125 0.02406097948551178 0.07776975631713867 0.07295733690261841 4.22265625\n",
            "repr, std, cov, clossl, z, norm 0.011231089942157269 0.40771484375 0.024897156283259392 0.059372566640377045 0.2140708565711975 5.265625\n",
            "repr, std, cov, clossl, z, norm 0.011921815574169159 0.410400390625 0.021959230303764343 0.07756709307432175 0.01206246204674244 4.859375\n",
            "repr, std, cov, clossl, z, norm 0.010954461991786957 0.4150390625 0.018522340804338455 0.08000838756561279 0.04278351739048958 5.34375\n",
            "75\n",
            "repr, std, cov, clossl, z, norm 0.010990200564265251 0.411865234375 0.021280765533447266 0.0755920335650444 0.2609013617038727 5.69921875\n",
            "repr, std, cov, clossl, z, norm 0.011406973004341125 0.41064453125 0.02172921970486641 0.0972733274102211 0.1306711882352829 5.0078125\n",
            "repr, std, cov, clossl, z, norm 0.010830235667526722 0.411376953125 0.02253299206495285 0.07054892927408218 0.10122586786746979 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.01108585111796856 0.412841796875 0.020177138969302177 0.06362736970186234 0.18049313127994537 3.619140625\n",
            "repr, std, cov, clossl, z, norm 0.011659083887934685 0.408203125 0.02528112381696701 0.07817794382572174 0.11070966720581055 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.011809930205345154 0.41015625 0.022004995495080948 0.0721016600728035 0.19177433848381042 5.04296875\n",
            "repr, std, cov, clossl, z, norm 0.012498902156949043 0.408447265625 0.02546270191669464 0.08352427929639816 0.04182024300098419 4.91015625\n",
            "repr, std, cov, clossl, z, norm 0.013001705519855022 0.404296875 0.02904522605240345 0.10723655670881271 0.05166994035243988 4.2890625\n",
            "76\n",
            "repr, std, cov, clossl, z, norm 0.010978488251566887 0.412109375 0.020388418808579445 0.08529000729322433 0.2267041653394699 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.011262461543083191 0.414794921875 0.01820649951696396 0.08421701192855835 0.2076946347951889 3.890625\n",
            "repr, std, cov, clossl, z, norm 0.010727398097515106 0.41455078125 0.019046951085329056 0.0742882788181305 0.003941849805414677 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.011372844688594341 0.410400390625 0.022698989138007164 0.09666860103607178 0.12105175852775574 4.859375\n",
            "repr, std, cov, clossl, z, norm 0.011481831781566143 0.413330078125 0.019292188808321953 0.08951185643672943 0.04143385589122772 4.91796875\n",
            "repr, std, cov, clossl, z, norm 0.01088965404778719 0.41650390625 0.016616573557257652 0.07890240103006363 0.08340666443109512 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.011235161684453487 0.413330078125 0.019885724410414696 0.08612123131752014 0.0026233866810798645 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.009613052010536194 0.416748046875 0.017535757273435593 0.06974177807569504 0.14648933708667755 5.25\n",
            "77\n",
            "repr, std, cov, clossl, z, norm 0.01127950381487608 0.4130859375 0.018962610512971878 0.09153858572244644 0.10753791779279709 5.11328125\n",
            "repr, std, cov, clossl, z, norm 0.010654846206307411 0.407958984375 0.024202266708016396 0.07220946252346039 0.058164503425359726 4.32421875\n",
            "repr, std, cov, clossl, z, norm 0.01125503983348608 0.40771484375 0.026433264836668968 0.0731331929564476 0.08105035871267319 4.9765625\n",
            "repr, std, cov, clossl, z, norm 0.01152802538126707 0.41064453125 0.022519996389746666 0.08817369490861893 0.07340095192193985 4.453125\n",
            "repr, std, cov, clossl, z, norm 0.010984969325363636 0.412109375 0.019783006981015205 0.06916387379169464 0.016107456758618355 4.59765625\n",
            "repr, std, cov, clossl, z, norm 0.010567666962742805 0.412841796875 0.019818974658846855 0.06606857478618622 0.11206533014774323 3.724609375\n",
            "repr, std, cov, clossl, z, norm 0.011227195151150227 0.4111328125 0.0207299143075943 0.07422992587089539 0.04807537421584129 4.734375\n",
            "repr, std, cov, clossl, z, norm 0.010077724233269691 0.4140625 0.018455013632774353 0.06053076311945915 0.026147395372390747 4.7109375\n",
            "78\n",
            "repr, std, cov, clossl, z, norm 0.01056237705051899 0.4150390625 0.01800064742565155 0.07591181993484497 0.06827708333730698 4.6875\n",
            "repr, std, cov, clossl, z, norm 0.011697368696331978 0.408935546875 0.02369777113199234 0.09183740615844727 0.05315687507390976 5.95703125\n",
            "repr, std, cov, clossl, z, norm 0.00972641073167324 0.41015625 0.021853145211935043 0.06363438814878464 0.028267057612538338 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.011457645334303379 0.40625 0.02573823556303978 0.0765971690416336 0.11529257148504257 3.99609375\n",
            "repr, std, cov, clossl, z, norm 0.011036362498998642 0.4072265625 0.024927780032157898 0.07682456821203232 0.12163542211055756 5.25\n",
            "repr, std, cov, clossl, z, norm 0.010232169181108475 0.41259765625 0.019254375249147415 0.06770187616348267 0.055439822375774384 4.578125\n",
            "repr, std, cov, clossl, z, norm 0.010439391247928143 0.41064453125 0.021117180585861206 0.06788621097803116 0.06627249717712402 5.26171875\n",
            "repr, std, cov, clossl, z, norm 0.010952798649668694 0.40673828125 0.024388950318098068 0.07964886724948883 0.10426176339387894 4.80078125\n",
            "79\n",
            "repr, std, cov, clossl, z, norm 0.010320842266082764 0.41064453125 0.02031559869647026 0.06215390935540199 0.013108335435390472 5.07421875\n",
            "repr, std, cov, clossl, z, norm 0.0107443667948246 0.411865234375 0.019628413021564484 0.07288224995136261 0.10210081934928894 4.578125\n",
            "repr, std, cov, clossl, z, norm 0.010235919617116451 0.411865234375 0.020602403208613396 0.07261412590742111 0.11942818015813828 4.375\n",
            "repr, std, cov, clossl, z, norm 0.011896936222910881 0.4072265625 0.023978523910045624 0.089047372341156 0.212981715798378 4.53515625\n",
            "repr, std, cov, clossl, z, norm 0.010418321937322617 0.410888671875 0.02032828889787197 0.06561294198036194 0.07619256526231766 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.010625837370753288 0.410400390625 0.02107357233762741 0.07097475975751877 0.07555405050516129 4.1875\n",
            "repr, std, cov, clossl, z, norm 0.011030993424355984 0.40771484375 0.02332156151533127 0.0834781602025032 0.04818233847618103 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.010826029814779758 0.407470703125 0.023497041314840317 0.07170142978429794 0.05816352739930153 4.38671875\n",
            "80\n",
            "repr, std, cov, clossl, z, norm 0.011406316421926022 0.40576171875 0.025447774678468704 0.07372031360864639 0.08876894414424896 4.87109375\n",
            "repr, std, cov, clossl, z, norm 0.010874927043914795 0.406005859375 0.024966474622488022 0.07691850513219833 0.14308665692806244 3.9609375\n",
            "repr, std, cov, clossl, z, norm 0.011378037743270397 0.405029296875 0.026676781475543976 0.09224327653646469 0.09636770933866501 4.49609375\n",
            "repr, std, cov, clossl, z, norm 0.010485896840691566 0.40673828125 0.02446676604449749 0.06539362668991089 0.15204231441020966 5.16796875\n",
            "repr, std, cov, clossl, z, norm 0.009541462175548077 0.408935546875 0.023098722100257874 0.062245626002550125 0.21160483360290527 2.703125\n",
            "repr, std, cov, clossl, z, norm 0.010650329291820526 0.411376953125 0.020724784582853317 0.0790138989686966 0.06765571236610413 4.03515625\n",
            "repr, std, cov, clossl, z, norm 0.011899072676897049 0.40966796875 0.02146877720952034 0.09292243421077728 0.0886724591255188 5.09765625\n",
            "repr, std, cov, clossl, z, norm 0.010235794819891453 0.41162109375 0.020525237545371056 0.06435419619083405 0.020131131634116173 5.234375\n",
            "81\n",
            "repr, std, cov, clossl, z, norm 0.01043677981942892 0.408935546875 0.02312740683555603 0.06539779901504517 0.0051997508853673935 4.15234375\n",
            "repr, std, cov, clossl, z, norm 0.009795567952096462 0.416748046875 0.016142887994647026 0.054849930107593536 0.11588174849748611 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.011624017730355263 0.41357421875 0.018322784453630447 0.07520432770252228 0.12741361558437347 4.33203125\n",
            "repr, std, cov, clossl, z, norm 0.01219554990530014 0.411376953125 0.020341157913208008 0.0979192927479744 0.08990922570228577 4.66796875\n",
            "repr, std, cov, clossl, z, norm 0.011205722577869892 0.4130859375 0.018858889117836952 0.05614633858203888 0.13097412884235382 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.011634905822575092 0.40966796875 0.021587323397397995 0.08505760878324509 0.023848777636885643 4.16015625\n",
            "repr, std, cov, clossl, z, norm 0.010522416792809963 0.412109375 0.019458886235952377 0.07726098597049713 0.18323840200901031 4.10546875\n",
            "repr, std, cov, clossl, z, norm 0.011344064958393574 0.40380859375 0.02775740623474121 0.07518361508846283 0.1020783856511116 4.875\n",
            "82\n",
            "repr, std, cov, clossl, z, norm 0.010922488756477833 0.40283203125 0.029997214674949646 0.07083319872617722 0.14864878356456757 5.1796875\n",
            "repr, std, cov, clossl, z, norm 0.011174377985298634 0.40576171875 0.025164606049656868 0.08098118007183075 0.09918931871652603 4.50390625\n",
            "repr, std, cov, clossl, z, norm 0.010963116772472858 0.411376953125 0.020675022155046463 0.07981622964143753 0.026718001812696457 4.515625\n",
            "repr, std, cov, clossl, z, norm 0.011322950944304466 0.40625 0.025433380156755447 0.07482318580150604 0.05310063064098358 4.46484375\n",
            "repr, std, cov, clossl, z, norm 0.011491302400827408 0.40869140625 0.022552847862243652 0.07463714480400085 0.0870940312743187 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.009898340329527855 0.4169921875 0.016149144619703293 0.06572278589010239 0.29991742968559265 4.11328125\n",
            "repr, std, cov, clossl, z, norm 0.010577316395938396 0.415283203125 0.016448386013507843 0.06893659383058548 0.08392740041017532 4.546875\n",
            "repr, std, cov, clossl, z, norm 0.010330066084861755 0.41552734375 0.016211455687880516 0.06171907112002373 0.1184479296207428 4.49609375\n",
            "83\n",
            "repr, std, cov, clossl, z, norm 0.010615795850753784 0.410400390625 0.021077163517475128 0.058404404670000076 0.1086498498916626 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.011860662139952183 0.40625 0.025141548365354538 0.07074364274740219 0.07416073977947235 4.37890625\n",
            "repr, std, cov, clossl, z, norm 0.011963107623159885 0.4033203125 0.02894432842731476 0.08803122490644455 0.10480131208896637 4.01171875\n",
            "repr, std, cov, clossl, z, norm 0.010950412601232529 0.408203125 0.023175444453954697 0.06737763434648514 0.06465619057416916 4.55078125\n",
            "repr, std, cov, clossl, z, norm 0.011778024025261402 0.40625 0.02534126117825508 0.06704490631818771 0.04068262130022049 4.81640625\n",
            "repr, std, cov, clossl, z, norm 0.01141861267387867 0.40966796875 0.021379347890615463 0.06355085968971252 0.20376881957054138 4.47265625\n",
            "repr, std, cov, clossl, z, norm 0.011839723214507103 0.40966796875 0.02074126899242401 0.07896266877651215 0.13745684921741486 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.011230225674808025 0.41162109375 0.019671622663736343 0.07060154527425766 0.09837008267641068 4.109375\n",
            "84\n",
            "repr, std, cov, clossl, z, norm 0.011606226675212383 0.40625 0.026991575956344604 0.07903127372264862 0.15418362617492676 4.5078125\n",
            "repr, std, cov, clossl, z, norm 0.010916276834905148 0.406982421875 0.025416674092411995 0.06215233355760574 0.1701822429895401 4.62109375\n",
            "repr, std, cov, clossl, z, norm 0.009611275978386402 0.416259765625 0.016041196882724762 0.060287684202194214 0.09697240591049194 3.814453125\n",
            "repr, std, cov, clossl, z, norm 0.01158362440764904 0.410400390625 0.022586636245250702 0.09188800305128098 0.08496318757534027 4.9921875\n",
            "repr, std, cov, clossl, z, norm 0.010077621787786484 0.41455078125 0.01775866374373436 0.06555086374282837 0.1259569525718689 2.966796875\n",
            "repr, std, cov, clossl, z, norm 0.010718796402215958 0.409423828125 0.021325629204511642 0.07001755386590958 0.05337749421596527 3.140625\n",
            "repr, std, cov, clossl, z, norm 0.011311664246022701 0.40185546875 0.031218595802783966 0.07610656321048737 0.1612536758184433 4.84375\n",
            "repr, std, cov, clossl, z, norm 0.011594424024224281 0.406005859375 0.024989932775497437 0.08642381429672241 0.07265225052833557 4.2109375\n",
            "85\n",
            "repr, std, cov, clossl, z, norm 0.010078365914523602 0.407958984375 0.022496525198221207 0.06168214604258537 0.18055272102355957 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.011463157832622528 0.406494140625 0.024600684642791748 0.07432322949171066 0.041544005274772644 4.96875\n",
            "repr, std, cov, clossl, z, norm 0.010951384902000427 0.408447265625 0.023529035970568657 0.07008045166730881 0.03387424722313881 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.010727892629802227 0.4140625 0.017975319176912308 0.0770811066031456 0.13248588144779205 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.011242358013987541 0.413330078125 0.01814950257539749 0.08888276666402817 0.047778889536857605 5.0546875\n",
            "repr, std, cov, clossl, z, norm 0.009908441454172134 0.414794921875 0.017385030165314674 0.058550260961055756 0.15515942871570587 4.5859375\n",
            "repr, std, cov, clossl, z, norm 0.0113707659766078 0.408935546875 0.023198336362838745 0.09201976656913757 0.0757841095328331 4.46875\n",
            "repr, std, cov, clossl, z, norm 0.010511700995266438 0.4072265625 0.025477107614278793 0.07290184497833252 0.10344456881284714 3.19921875\n",
            "86\n",
            "repr, std, cov, clossl, z, norm 0.011783367954194546 0.403564453125 0.027801238000392914 0.07846736907958984 0.12148810923099518 5.05859375\n",
            "repr, std, cov, clossl, z, norm 0.01071774773299694 0.410400390625 0.02068442851305008 0.06828325241804123 0.07546202093362808 3.880859375\n",
            "repr, std, cov, clossl, z, norm 0.011216683313250542 0.407958984375 0.023774059489369392 0.06427156925201416 0.13488326966762543 3.6015625\n",
            "repr, std, cov, clossl, z, norm 0.011183802969753742 0.408935546875 0.022007690742611885 0.07448741048574448 0.0719015896320343 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.010766655206680298 0.41162109375 0.019027486443519592 0.08277931064367294 0.03364994004368782 5.484375\n",
            "repr, std, cov, clossl, z, norm 0.01143531035631895 0.4072265625 0.02346082404255867 0.0797790139913559 0.0028191632591187954 3.8203125\n",
            "repr, std, cov, clossl, z, norm 0.011575596407055855 0.40869140625 0.02191404439508915 0.0853661447763443 0.11387131363153458 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.01190105453133583 0.40869140625 0.02146317809820175 0.08547231554985046 0.0423172228038311 4.58984375\n",
            "87\n",
            "repr, std, cov, clossl, z, norm 0.012093794532120228 0.406494140625 0.02344401367008686 0.08557383716106415 0.04038430005311966 5.921875\n",
            "repr, std, cov, clossl, z, norm 0.010797402821481228 0.40966796875 0.02151631936430931 0.07519674301147461 0.13418987393379211 4.3515625\n",
            "repr, std, cov, clossl, z, norm 0.010657116770744324 0.41162109375 0.01932927966117859 0.059483855962753296 0.05816767364740372 4.71875\n",
            "repr, std, cov, clossl, z, norm 0.011212772689759731 0.412841796875 0.018215643242001534 0.07618632912635803 0.13789768517017365 4.28515625\n",
            "repr, std, cov, clossl, z, norm 0.0111711286008358 0.408203125 0.021615473553538322 0.07119118422269821 0.05974254012107849 4.87109375\n",
            "repr, std, cov, clossl, z, norm 0.010623112320899963 0.409912109375 0.02049720659852028 0.06743695586919785 0.08599082380533218 4.55078125\n",
            "repr, std, cov, clossl, z, norm 0.010860776528716087 0.408447265625 0.02299545332789421 0.07093841582536697 0.1802724003791809 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.011611776426434517 0.40966796875 0.019968919456005096 0.09679750353097916 0.07641522586345673 4.36328125\n",
            "88\n",
            "repr, std, cov, clossl, z, norm 0.010405895300209522 0.40869140625 0.021547824144363403 0.05679033324122429 0.2024061679840088 4.92578125\n",
            "repr, std, cov, clossl, z, norm 0.011812986806035042 0.408203125 0.02182980254292488 0.0760575383901596 0.06949368864297867 5.390625\n",
            "repr, std, cov, clossl, z, norm 0.01312935072928667 0.4033203125 0.027276378124952316 0.0886465460062027 0.05336454138159752 4.1875\n",
            "repr, std, cov, clossl, z, norm 0.012599360197782516 0.40478515625 0.024947509169578552 0.08072628825902939 0.12087162584066391 3.63671875\n",
            "repr, std, cov, clossl, z, norm 0.011524967849254608 0.403564453125 0.027110755443572998 0.05790731683373451 0.19643235206604004 4.8046875\n",
            "repr, std, cov, clossl, z, norm 0.012574516236782074 0.40380859375 0.026340346783399582 0.09473855793476105 0.13026942312717438 4.015625\n",
            "repr, std, cov, clossl, z, norm 0.01263159979134798 0.39990234375 0.030709082260727882 0.08470337092876434 0.17049390077590942 5.0859375\n",
            "repr, std, cov, clossl, z, norm 0.011015859432518482 0.408935546875 0.021359659731388092 0.07231032103300095 0.14876654744148254 3.841796875\n",
            "89\n",
            "repr, std, cov, clossl, z, norm 0.011501729488372803 0.407958984375 0.02288060262799263 0.07680301368236542 0.11266735196113586 4.484375\n",
            "repr, std, cov, clossl, z, norm 0.01070190779864788 0.411865234375 0.019642632454633713 0.06805600225925446 0.18405795097351074 4.26953125\n",
            "repr, std, cov, clossl, z, norm 0.012245341204106808 0.411865234375 0.019424308091402054 0.08518661558628082 0.10562552511692047 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.011426351964473724 0.41357421875 0.01815655082464218 0.09810374677181244 0.15569458901882172 4.640625\n",
            "repr, std, cov, clossl, z, norm 0.010460913181304932 0.4150390625 0.016548531129956245 0.054856300354003906 0.20071490108966827 4.1796875\n",
            "repr, std, cov, clossl, z, norm 0.010633834637701511 0.411865234375 0.019466595724225044 0.06267057359218597 0.03349737823009491 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.010797319002449512 0.4111328125 0.01904725655913353 0.07718421518802643 0.10330691933631897 4.99609375\n",
            "repr, std, cov, clossl, z, norm 0.011487964540719986 0.409423828125 0.020644038915634155 0.12129204720258713 0.04360540956258774 4.28125\n",
            "90\n",
            "repr, std, cov, clossl, z, norm 0.010803324170410633 0.40673828125 0.022829607129096985 0.06927204132080078 0.12376076728105545 4.46875\n",
            "repr, std, cov, clossl, z, norm 0.010724923573434353 0.404052734375 0.026355737820267677 0.06759697943925858 0.11219591647386551 3.6953125\n",
            "repr, std, cov, clossl, z, norm 0.012170622125267982 0.400634765625 0.0313422791659832 0.0930047556757927 0.13661617040634155 3.9921875\n",
            "repr, std, cov, clossl, z, norm 0.009800116531550884 0.40478515625 0.025676000863313675 0.06236233562231064 0.1946253925561905 3.8125\n",
            "repr, std, cov, clossl, z, norm 0.011619100347161293 0.40576171875 0.02419370226562023 0.09677667170763016 0.1375858634710312 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.010694937780499458 0.405029296875 0.026714351028203964 0.08299905806779861 0.07121559232473373 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.011361245065927505 0.404541015625 0.02653568983078003 0.10340816527605057 0.12995007634162903 3.728515625\n",
            "repr, std, cov, clossl, z, norm 0.00999094545841217 0.409912109375 0.022308051586151123 0.0753607526421547 0.0884694829583168 3.734375\n",
            "91\n",
            "repr, std, cov, clossl, z, norm 0.010547058656811714 0.410400390625 0.01968539133667946 0.0888044685125351 0.1459134817123413 3.9140625\n",
            "repr, std, cov, clossl, z, norm 0.010793843306601048 0.407470703125 0.022850356996059418 0.09160833805799484 0.06125319376587868 3.689453125\n",
            "repr, std, cov, clossl, z, norm 0.010110704228281975 0.41162109375 0.018924862146377563 0.070843406021595 0.16179044544696808 4.625\n",
            "repr, std, cov, clossl, z, norm 0.011326953768730164 0.408203125 0.02145041897892952 0.0961928740143776 0.14207887649536133 4.5\n",
            "repr, std, cov, clossl, z, norm 0.010437381453812122 0.408203125 0.02278214320540428 0.08844307065010071 0.12143313884735107 5.20703125\n",
            "repr, std, cov, clossl, z, norm 0.009684291668236256 0.408447265625 0.02250901609659195 0.07253243029117584 0.06933332979679108 4.0703125\n",
            "repr, std, cov, clossl, z, norm 0.009869626723229885 0.408935546875 0.02118435502052307 0.07567030191421509 0.0766911581158638 4.171875\n",
            "repr, std, cov, clossl, z, norm 0.010922597721219063 0.407958984375 0.021765198558568954 0.09946013987064362 0.110395647585392 3.6640625\n",
            "92\n",
            "repr, std, cov, clossl, z, norm 0.01023754570633173 0.410888671875 0.019532006233930588 0.08408662676811218 0.07963839918375015 4.5546875\n",
            "repr, std, cov, clossl, z, norm 0.00959013495594263 0.4111328125 0.019743740558624268 0.0734795331954956 0.013491570949554443 3.2890625\n",
            "repr, std, cov, clossl, z, norm 0.010527499951422215 0.407470703125 0.022738587111234665 0.08875182271003723 0.1454755663871765 4.68359375\n",
            "repr, std, cov, clossl, z, norm 0.010008943267166615 0.406982421875 0.023539939895272255 0.08493811637163162 0.08731312304735184 4.22265625\n",
            "repr, std, cov, clossl, z, norm 0.010843341238796711 0.408203125 0.02266150526702404 0.09203090518712997 0.08093563467264175 4.3984375\n",
            "repr, std, cov, clossl, z, norm 0.00974781159311533 0.40576171875 0.024729685857892036 0.05941525474190712 0.08211162686347961 4.02734375\n",
            "repr, std, cov, clossl, z, norm 0.011508799158036709 0.4013671875 0.029755525290966034 0.09139319509267807 0.08577777445316315 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.010847904719412327 0.4072265625 0.02329852059483528 0.07427118718624115 0.06092512607574463 3.970703125\n",
            "93\n",
            "repr, std, cov, clossl, z, norm 0.010238805785775185 0.408935546875 0.020863210782408714 0.06436296552419662 0.1588466316461563 3.421875\n",
            "repr, std, cov, clossl, z, norm 0.011113259941339493 0.40771484375 0.02342560701072216 0.08101475238800049 0.22380487620830536 4.06640625\n",
            "repr, std, cov, clossl, z, norm 0.010295704007148743 0.40771484375 0.02224126085639 0.0752691701054573 0.09030979871749878 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.010758395306766033 0.406982421875 0.022709963843226433 0.08969730138778687 0.07996664196252823 4.35546875\n",
            "repr, std, cov, clossl, z, norm 0.011186269111931324 0.40869140625 0.021559854969382286 0.08610980957746506 0.05546283349394798 4.56640625\n",
            "repr, std, cov, clossl, z, norm 0.010487050749361515 0.4072265625 0.02297935262322426 0.07495135813951492 0.06341547518968582 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.010774144902825356 0.409423828125 0.020404117181897163 0.0742596685886383 0.019680863246321678 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.01072201132774353 0.409423828125 0.020786460489034653 0.07285979390144348 0.1797727346420288 4.5546875\n",
            "94\n",
            "repr, std, cov, clossl, z, norm 0.011275817640125751 0.40771484375 0.022236187011003494 0.07623876631259918 0.12569130957126617 5.45703125\n",
            "repr, std, cov, clossl, z, norm 0.01189123373478651 0.4072265625 0.022762533277273178 0.08790387958288193 0.034541815519332886 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.010488327592611313 0.40869140625 0.021336212754249573 0.06866458058357239 0.1332840472459793 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.010968825779855251 0.408935546875 0.020910155028104782 0.07518355548381805 0.0729692354798317 4.30078125\n",
            "repr, std, cov, clossl, z, norm 0.01095737423747778 0.40771484375 0.023022659122943878 0.08413276821374893 0.08539456129074097 4.14453125\n",
            "repr, std, cov, clossl, z, norm 0.011048421263694763 0.40771484375 0.022649627178907394 0.07122629880905151 0.07674279808998108 4.72265625\n",
            "repr, std, cov, clossl, z, norm 0.010856994427740574 0.40478515625 0.024278052151203156 0.0715918317437172 0.23944924771785736 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.01101185753941536 0.403564453125 0.026121078059077263 0.06968899816274643 0.06294628232717514 4.60546875\n",
            "95\n",
            "repr, std, cov, clossl, z, norm 0.00982569344341755 0.408447265625 0.021213572472333908 0.05487106740474701 0.15503011643886566 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.01155523769557476 0.400634765625 0.03153005614876747 0.0809544250369072 0.16194291412830353 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.010947015136480331 0.4091796875 0.020209932699799538 0.06736566871404648 0.18432208895683289 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.01085225585848093 0.41162109375 0.018687700852751732 0.08286501467227936 0.12980294227600098 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.010057530365884304 0.41162109375 0.019274860620498657 0.05924227088689804 0.07356001436710358 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.01147492416203022 0.41162109375 0.019849136471748352 0.0793776661157608 0.28769969940185547 4.25\n",
            "repr, std, cov, clossl, z, norm 0.010845002718269825 0.41064453125 0.019660096615552902 0.0701318308711052 0.12368275225162506 4.5859375\n",
            "repr, std, cov, clossl, z, norm 0.011461731977760792 0.4033203125 0.02675621584057808 0.07585093379020691 0.11524418741464615 4.60546875\n",
            "96\n",
            "repr, std, cov, clossl, z, norm 0.011693000793457031 0.406982421875 0.02244821935892105 0.08405019342899323 0.11310145258903503 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.010399636812508106 0.410888671875 0.01905006170272827 0.061752427369356155 0.08606390655040741 4.29296875\n",
            "repr, std, cov, clossl, z, norm 0.011276816949248314 0.40576171875 0.024383297190070152 0.06927687674760818 0.09693440794944763 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.010393710806965828 0.410888671875 0.019586078822612762 0.06106429547071457 0.08271227777004242 5.56640625\n",
            "repr, std, cov, clossl, z, norm 0.009504062123596668 0.41259765625 0.01839427277445793 0.04801936820149422 0.1569215953350067 5.26171875\n",
            "repr, std, cov, clossl, z, norm 0.011785459704697132 0.409423828125 0.020555585622787476 0.08718720823526382 0.08200139552354813 5.02734375\n",
            "repr, std, cov, clossl, z, norm 0.010794437490403652 0.406982421875 0.02306453511118889 0.07501290738582611 0.12250255048274994 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.010155636817216873 0.410888671875 0.01898673176765442 0.06199471279978752 0.1531417816877365 4.4140625\n",
            "97\n",
            "repr, std, cov, clossl, z, norm 0.010632278397679329 0.4091796875 0.020387571305036545 0.07221775501966476 0.04849150404334068 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.011458121240139008 0.40234375 0.02784004993736744 0.07549896836280823 0.057876959443092346 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.011166573502123356 0.399658203125 0.030308548361063004 0.08923023194074631 0.02266867458820343 4.484375\n",
            "repr, std, cov, clossl, z, norm 0.011286957189440727 0.403076171875 0.026119984686374664 0.060138292610645294 0.05302252992987633 4.64453125\n",
            "repr, std, cov, clossl, z, norm 0.011457107029855251 0.404296875 0.025191962718963623 0.05896119773387909 0.08443398028612137 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.011251542717218399 0.40625 0.022816365584731102 0.07402276992797852 0.22566913068294525 2.458984375\n",
            "repr, std, cov, clossl, z, norm 0.011974858120083809 0.40380859375 0.026649395003914833 0.07149402797222137 0.16214211285114288 4.86328125\n",
            "repr, std, cov, clossl, z, norm 0.011199185624718666 0.4072265625 0.021834570914506912 0.05076130852103233 0.17047211527824402 4.51953125\n",
            "98\n",
            "repr, std, cov, clossl, z, norm 0.01185765489935875 0.40771484375 0.021583201363682747 0.06214017793536186 0.08053429424762726 3.984375\n",
            "repr, std, cov, clossl, z, norm 0.01193585991859436 0.40478515625 0.026203393936157227 0.07483719289302826 0.19389447569847107 4.8828125\n",
            "repr, std, cov, clossl, z, norm 0.009156151674687862 0.40966796875 0.019603317603468895 0.04516135901212692 0.06048304960131645 4.13671875\n",
            "repr, std, cov, clossl, z, norm 0.01085937861353159 0.408203125 0.02121216431260109 0.0743187665939331 0.17769020795822144 3.767578125\n",
            "repr, std, cov, clossl, z, norm 0.010499673895537853 0.409912109375 0.01929866522550583 0.07177229970693588 0.10574526339769363 4.4765625\n",
            "repr, std, cov, clossl, z, norm 0.01137255597859621 0.406494140625 0.02267884649336338 0.08014041930437088 0.12058388441801071 4.625\n",
            "repr, std, cov, clossl, z, norm 0.011361543089151382 0.400390625 0.030015289783477783 0.06816501170396805 0.22757326066493988 3.986328125\n",
            "repr, std, cov, clossl, z, norm 0.010092305019497871 0.406494140625 0.02393435314297676 0.060982901602983475 0.029070664197206497 4.6953125\n",
            "99\n",
            "repr, std, cov, clossl, z, norm 0.010537288151681423 0.404052734375 0.025345787405967712 0.06482580304145813 0.0797576978802681 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.010861802846193314 0.40478515625 0.025462638586759567 0.0721522867679596 0.0 4.93359375\n",
            "repr, std, cov, clossl, z, norm 0.010403217747807503 0.408203125 0.021270636469125748 0.06670314818620682 0.08248420804738998 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.010248457081615925 0.408203125 0.02111199125647545 0.07062368094921112 0.1286870688199997 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.010074730962514877 0.40673828125 0.022722918540239334 0.05948283150792122 0.16118425130844116 4.765625\n",
            "repr, std, cov, clossl, z, norm 0.011035995557904243 0.40478515625 0.024439074099063873 0.08094974607229233 0.09012532979249954 2.623046875\n",
            "repr, std, cov, clossl, z, norm 0.010494114831089973 0.408203125 0.02151930332183838 0.058684661984443665 0.10890710353851318 4.4296875\n",
            "repr, std, cov, clossl, z, norm 0.011506442911922932 0.408935546875 0.021257951855659485 0.07283364236354828 0.05144985765218735 4.5234375\n",
            "100\n",
            "repr, std, cov, clossl, z, norm 0.010585823096334934 0.411376953125 0.01912415586411953 0.06294267624616623 0.0360633060336113 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.011276318691670895 0.4130859375 0.017621535807847977 0.05703223869204521 0.07941675931215286 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.010666477493941784 0.41259765625 0.017688486725091934 0.05643095821142197 0.08693806082010269 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.01108564157038927 0.408203125 0.021364614367485046 0.06012887880206108 0.07197247445583344 4.359375\n",
            "repr, std, cov, clossl, z, norm 0.010943524539470673 0.408447265625 0.021941062062978745 0.06008291244506836 0.05968165025115013 4.48828125\n",
            "repr, std, cov, clossl, z, norm 0.011238080449402332 0.406494140625 0.02238330990076065 0.059781331568956375 0.07356522977352142 4.46875\n",
            "repr, std, cov, clossl, z, norm 0.010527165606617928 0.40185546875 0.02778906375169754 0.057488877326250076 0.10386843234300613 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.010752434842288494 0.405029296875 0.024181414395570755 0.05959896743297577 0.15910474956035614 5.16796875\n",
            "101\n",
            "repr, std, cov, clossl, z, norm 0.011593866162002087 0.404052734375 0.024098969995975494 0.08926113694906235 0.04227651655673981 4.5234375\n",
            "repr, std, cov, clossl, z, norm 0.010695836506783962 0.406982421875 0.02161501906812191 0.0585373193025589 0.015747200697660446 3.970703125\n",
            "repr, std, cov, clossl, z, norm 0.010828039608895779 0.403076171875 0.02687220647931099 0.05543481186032295 0.14727672934532166 5.1171875\n",
            "repr, std, cov, clossl, z, norm 0.011762833222746849 0.39990234375 0.02979763224720955 0.06590810418128967 0.08189623057842255 4.66796875\n",
            "repr, std, cov, clossl, z, norm 0.011326316744089127 0.4052734375 0.02377152442932129 0.07257749140262604 0.1622326672077179 3.326171875\n",
            "repr, std, cov, clossl, z, norm 0.010469792410731316 0.410400390625 0.018439561128616333 0.06648316234350204 0.1789710372686386 2.41796875\n",
            "repr, std, cov, clossl, z, norm 0.010618443600833416 0.41162109375 0.018650108948349953 0.0690404623746872 0.030697448179125786 4.6640625\n",
            "repr, std, cov, clossl, z, norm 0.009646548889577389 0.41015625 0.019688913598656654 0.058263108134269714 0.014362829737365246 4.3984375\n",
            "102\n",
            "repr, std, cov, clossl, z, norm 0.010120552964508533 0.409912109375 0.01958773098886013 0.0456179603934288 0.12930426001548767 4.27734375\n",
            "repr, std, cov, clossl, z, norm 0.010206594131886959 0.40869140625 0.021324291825294495 0.04439124837517738 0.21060718595981598 3.890625\n",
            "repr, std, cov, clossl, z, norm 0.0103445528075099 0.40673828125 0.022190382704138756 0.060514383018016815 0.0769168809056282 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.010293868370354176 0.40771484375 0.022264940664172173 0.07755044847726822 0.08577366918325424 4.09375\n",
            "repr, std, cov, clossl, z, norm 0.010677814483642578 0.402099609375 0.025738362222909927 0.06438750773668289 0.0307616014033556 4.60546875\n",
            "repr, std, cov, clossl, z, norm 0.01111672818660736 0.39794921875 0.032404929399490356 0.07750736176967621 0.09462729096412659 3.892578125\n",
            "repr, std, cov, clossl, z, norm 0.011409854516386986 0.400390625 0.027691131457686424 0.0673137903213501 0.1755230873823166 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.009391890838742256 0.40234375 0.027666587382555008 0.07875826954841614 0.14813114702701569 4.0390625\n",
            "103\n",
            "repr, std, cov, clossl, z, norm 0.01049437839537859 0.399169921875 0.032663896679878235 0.06529545783996582 0.05108093470335007 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.011303084902465343 0.39990234375 0.02962016500532627 0.08032077550888062 0.052659276872873306 4.1015625\n",
            "repr, std, cov, clossl, z, norm 0.011702797375619411 0.400146484375 0.030330650508403778 0.09171924740076065 0.0853111669421196 3.52734375\n",
            "repr, std, cov, clossl, z, norm 0.011650645174086094 0.405517578125 0.024669989943504333 0.08428476005792618 0.049528248608112335 4.26953125\n",
            "repr, std, cov, clossl, z, norm 0.011228646151721478 0.408203125 0.020924536511301994 0.08588218688964844 0.20234577357769012 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.012214277870953083 0.406494140625 0.023562777787446976 0.11259491741657257 0.03558016195893288 3.537109375\n",
            "repr, std, cov, clossl, z, norm 0.012865601107478142 0.40673828125 0.022386379539966583 0.1027730405330658 0.019185496494174004 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.012490911409258842 0.411865234375 0.01737648993730545 0.08940742164850235 0.13303504884243011 4.3046875\n",
            "104\n",
            "repr, std, cov, clossl, z, norm 0.013210298493504524 0.410400390625 0.01865546777844429 0.0974404513835907 0.05484137311577797 3.763671875\n",
            "repr, std, cov, clossl, z, norm 0.013875345699489117 0.4111328125 0.018329694867134094 0.10760923475027084 0.14557971060276031 3.87890625\n",
            "repr, std, cov, clossl, z, norm 0.013253044337034225 0.412109375 0.01767139509320259 0.07554630190134048 0.019914526492357254 4.26171875\n",
            "repr, std, cov, clossl, z, norm 0.012794509530067444 0.413330078125 0.016934625804424286 0.06902575492858887 0.038628801703453064 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.012942940928041935 0.411376953125 0.018735164776444435 0.07312055677175522 0.07893548905849457 3.71484375\n",
            "repr, std, cov, clossl, z, norm 0.012395071797072887 0.41357421875 0.016924919560551643 0.06340251117944717 0.17123183608055115 3.55078125\n",
            "repr, std, cov, clossl, z, norm 0.01177557185292244 0.411376953125 0.019912274554371834 0.0752682313323021 0.10092167556285858 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.012008649297058582 0.41015625 0.020305484533309937 0.09150426834821701 0.11571899056434631 4.4140625\n",
            "105\n",
            "repr, std, cov, clossl, z, norm 0.01068245992064476 0.406982421875 0.023816458880901337 0.06709539145231247 0.029063915833830833 4.86328125\n",
            "repr, std, cov, clossl, z, norm 0.012778772972524166 0.400634765625 0.028014980256557465 0.08970712125301361 0.16111160814762115 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.010977289639413357 0.408447265625 0.021919628605246544 0.07190960645675659 0.21643400192260742 3.865234375\n",
            "repr, std, cov, clossl, z, norm 0.011854005046188831 0.402587890625 0.02604912593960762 0.09805263578891754 0.10273685306310654 3.798828125\n",
            "repr, std, cov, clossl, z, norm 0.011225931346416473 0.40625 0.02263263612985611 0.07522150874137878 0.06119910627603531 4.10546875\n",
            "repr, std, cov, clossl, z, norm 0.010370862670242786 0.406005859375 0.023449724540114403 0.0648205429315567 0.12802772223949432 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.010647669434547424 0.40625 0.022868899628520012 0.06374908983707428 0.07315518707036972 3.876953125\n",
            "repr, std, cov, clossl, z, norm 0.012346911244094372 0.403564453125 0.026107054203748703 0.08852341771125793 0.05074606090784073 4.703125\n",
            "106\n",
            "repr, std, cov, clossl, z, norm 0.011773386038839817 0.403076171875 0.025255048647522926 0.07332614064216614 0.22165286540985107 3.822265625\n",
            "repr, std, cov, clossl, z, norm 0.011339881457388401 0.40478515625 0.023442192003130913 0.07275708764791489 0.03912641108036041 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.011054844595491886 0.408203125 0.021849405020475388 0.07184881716966629 0.10787693411111832 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.012072439305484295 0.4052734375 0.02358931675553322 0.08076879382133484 0.10543571412563324 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.011267825961112976 0.40576171875 0.022540682926774025 0.0737229585647583 0.036486998200416565 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.012262267991900444 0.402587890625 0.026102980598807335 0.08691707998514175 0.05367458239197731 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.010702550411224365 0.404052734375 0.02399662509560585 0.07066266983747482 0.0014526012819260359 4.11328125\n",
            "repr, std, cov, clossl, z, norm 0.010806799866259098 0.404541015625 0.023539075627923012 0.0677584558725357 0.0411594957113266 3.97265625\n",
            "107\n",
            "repr, std, cov, clossl, z, norm 0.01124494057148695 0.404296875 0.026495762169361115 0.06646686047315598 0.07443659752607346 4.5546875\n",
            "repr, std, cov, clossl, z, norm 0.010020474903285503 0.408447265625 0.019868643954396248 0.058018025010824203 0.07535652816295624 5.5859375\n",
            "repr, std, cov, clossl, z, norm 0.011256656609475613 0.4033203125 0.02472434751689434 0.08833146840333939 0.17625641822814941 4.41796875\n",
            "repr, std, cov, clossl, z, norm 0.010013939812779427 0.40771484375 0.02204040065407753 0.07140786200761795 0.17160534858703613 4.171875\n",
            "repr, std, cov, clossl, z, norm 0.010660500265657902 0.40869140625 0.020552698522806168 0.08541282266378403 0.01658753678202629 3.833984375\n",
            "repr, std, cov, clossl, z, norm 0.010652589611709118 0.40966796875 0.02000502496957779 0.07305073738098145 0.09762220084667206 2.72265625\n",
            "repr, std, cov, clossl, z, norm 0.010687722824513912 0.407958984375 0.0220537930727005 0.07454435527324677 0.20598435401916504 4.56640625\n",
            "repr, std, cov, clossl, z, norm 0.010851957835257053 0.407958984375 0.020680921152234077 0.07728542387485504 0.09783361852169037 4.91796875\n",
            "108\n",
            "repr, std, cov, clossl, z, norm 0.011201572604477406 0.404541015625 0.023925933986902237 0.06582759320735931 0.2534578740596771 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.010944666340947151 0.406005859375 0.021604806184768677 0.0655193105340004 0.02769406884908676 4.14453125\n",
            "repr, std, cov, clossl, z, norm 0.010968498885631561 0.407470703125 0.020802654325962067 0.07035176455974579 0.03348034247756004 3.7109375\n",
            "repr, std, cov, clossl, z, norm 0.011390256695449352 0.405029296875 0.023271970450878143 0.07093481719493866 0.11540862917900085 4.26953125\n",
            "repr, std, cov, clossl, z, norm 0.011218558065593243 0.402587890625 0.027128061279654503 0.06907393038272858 0.023999404162168503 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.012232186272740364 0.400634765625 0.027369562536478043 0.07399541884660721 0.06328173726797104 4.734375\n",
            "repr, std, cov, clossl, z, norm 0.012202160432934761 0.40234375 0.025555584579706192 0.060395240783691406 0.03506210818886757 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.01013282872736454 0.40576171875 0.022987496107816696 0.04100295156240463 0.09875347465276718 5.6328125\n",
            "109\n",
            "repr, std, cov, clossl, z, norm 0.011277697049081326 0.407958984375 0.02069343999028206 0.058182112872600555 0.17473424971103668 3.474609375\n",
            "repr, std, cov, clossl, z, norm 0.012016281485557556 0.410888671875 0.018235735595226288 0.06372304260730743 0.007657564710825682 4.67578125\n",
            "repr, std, cov, clossl, z, norm 0.012015609070658684 0.4072265625 0.02049712836742401 0.05842946097254753 0.11891975998878479 4.23828125\n",
            "repr, std, cov, clossl, z, norm 0.011723412200808525 0.409912109375 0.018091393634676933 0.07339639961719513 0.12693297863006592 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.012722402811050415 0.403076171875 0.024163635447621346 0.08835195004940033 0.11406221240758896 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.011122861877083778 0.406982421875 0.021004417911171913 0.05886854976415634 0.12877853214740753 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.011878260411322117 0.4013671875 0.026462700217962265 0.08658154308795929 0.0048056975938379765 4.34375\n",
            "repr, std, cov, clossl, z, norm 0.011654443107545376 0.395751953125 0.032854676246643066 0.06963280588388443 0.03842829540371895 4.4296875\n",
            "110\n",
            "repr, std, cov, clossl, z, norm 0.011090053245425224 0.3984375 0.02896658517420292 0.06765508651733398 0.2908657193183899 4.5\n",
            "repr, std, cov, clossl, z, norm 0.011025279760360718 0.40625 0.021885640919208527 0.0771574005484581 0.07852832227945328 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.011267454363405704 0.40625 0.02172258496284485 0.0732668861746788 0.1818971484899521 5.1484375\n",
            "repr, std, cov, clossl, z, norm 0.009948821738362312 0.40869140625 0.019270438700914383 0.06161739304661751 0.19910240173339844 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.010509240441024303 0.40673828125 0.02058275043964386 0.06787808984518051 0.14877738058567047 4.3984375\n",
            "repr, std, cov, clossl, z, norm 0.00964452512562275 0.40771484375 0.02117820456624031 0.07067897915840149 0.08604593575000763 4.7265625\n",
            "repr, std, cov, clossl, z, norm 0.010101786814630032 0.40673828125 0.02189910039305687 0.08276108652353287 0.14699722826480865 3.759765625\n",
            "repr, std, cov, clossl, z, norm 0.010391146875917912 0.405029296875 0.022806117311120033 0.06791152060031891 0.050480879843235016 4.0\n",
            "111\n",
            "repr, std, cov, clossl, z, norm 0.010251430794596672 0.4033203125 0.02421462908387184 0.0651639997959137 0.20575857162475586 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.011153964325785637 0.403076171875 0.024763882160186768 0.07084789872169495 0.08793342858552933 3.7890625\n",
            "repr, std, cov, clossl, z, norm 0.010366231203079224 0.4013671875 0.025391902774572372 0.06170140951871872 0.24767598509788513 4.671875\n",
            "repr, std, cov, clossl, z, norm 0.010548793710768223 0.400146484375 0.028074704110622406 0.07352924346923828 0.0902625322341919 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.0103268688544631 0.403076171875 0.02491557039320469 0.065614253282547 0.07883971929550171 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.010846778750419617 0.40087890625 0.026037894189357758 0.07484021782875061 0.1445448398590088 4.17578125\n",
            "repr, std, cov, clossl, z, norm 0.010225296951830387 0.405029296875 0.02228027768433094 0.0748785212635994 0.1016274243593216 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.009705492295324802 0.406982421875 0.021142281591892242 0.06396570056676865 0.09041629731655121 4.1484375\n",
            "112\n",
            "repr, std, cov, clossl, z, norm 0.010120506398379803 0.40771484375 0.020939309149980545 0.06998385488986969 0.17901650071144104 3.486328125\n",
            "repr, std, cov, clossl, z, norm 0.010455984622240067 0.408935546875 0.01915566623210907 0.07930415123701096 0.22069720923900604 4.8203125\n",
            "repr, std, cov, clossl, z, norm 0.011049134656786919 0.40576171875 0.02185029909014702 0.07004909962415695 0.033981870859861374 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.010539806447923183 0.406982421875 0.021538369357585907 0.07345573604106903 0.16025416553020477 4.62890625\n",
            "repr, std, cov, clossl, z, norm 0.010108216665685177 0.409423828125 0.019598614424467087 0.04691401496529579 0.03886261209845543 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.011165110394358635 0.40625 0.02111932635307312 0.06716734915971756 0.06859264522790909 4.46875\n",
            "repr, std, cov, clossl, z, norm 0.012020743452012539 0.399169921875 0.028203284367918968 0.06977725028991699 0.13536988198757172 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.01158228237181902 0.404052734375 0.023320086300373077 0.05568753927946091 0.10192527621984482 5.421875\n",
            "113\n",
            "repr, std, cov, clossl, z, norm 0.012031636200845242 0.397705078125 0.03039783239364624 0.05934823676943779 0.15224142372608185 5.0078125\n",
            "repr, std, cov, clossl, z, norm 0.011548025533556938 0.39794921875 0.03028719872236252 0.06318064033985138 0.17651933431625366 6.9140625\n",
            "repr, std, cov, clossl, z, norm 0.01233983039855957 0.4013671875 0.027513157576322556 0.061621006578207016 0.05399985611438751 4.31640625\n",
            "repr, std, cov, clossl, z, norm 0.01120863389223814 0.405029296875 0.023392563685774803 0.051662128418684006 0.0835055336356163 4.36328125\n",
            "repr, std, cov, clossl, z, norm 0.011042701080441475 0.409912109375 0.01814398542046547 0.04866376146674156 0.11023345589637756 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.011388080194592476 0.40673828125 0.0212544072419405 0.04973781108856201 0.02634402923285961 5.0546875\n",
            "repr, std, cov, clossl, z, norm 0.011515098623931408 0.408203125 0.020568251609802246 0.05864930897951126 0.135698139667511 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.010381922125816345 0.41162109375 0.01750689372420311 0.06185699254274368 0.07729993015527725 3.927734375\n",
            "114\n",
            "repr, std, cov, clossl, z, norm 0.011173363775014877 0.406494140625 0.022099986672401428 0.05996844172477722 0.07055886834859848 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.011113172397017479 0.40673828125 0.021097607910633087 0.05510152876377106 0.15603621304035187 4.38671875\n",
            "repr, std, cov, clossl, z, norm 0.01135108433663845 0.404541015625 0.022119641304016113 0.06451160460710526 0.23382148146629333 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.011995121836662292 0.3994140625 0.028364147990942 0.06753432005643845 0.1341627836227417 4.953125\n",
            "repr, std, cov, clossl, z, norm 0.01261154469102621 0.3955078125 0.03398796170949936 0.07845895737409592 0.17892934381961823 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.011401697061955929 0.401611328125 0.02563999779522419 0.08486320078372955 0.12022770941257477 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.013078412972390652 0.397705078125 0.030236776918172836 0.0765608549118042 0.16722294688224792 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.011267352849245071 0.40087890625 0.02798640914261341 0.060850840061903 0.1847945600748062 3.689453125\n",
            "115\n",
            "repr, std, cov, clossl, z, norm 0.011225630529224873 0.40283203125 0.023817092180252075 0.05861504375934601 0.07563795149326324 3.935546875\n",
            "repr, std, cov, clossl, z, norm 0.01227194257080555 0.4033203125 0.024004343897104263 0.07486452907323837 0.17557193338871002 4.36328125\n",
            "repr, std, cov, clossl, z, norm 0.01096896268427372 0.408447265625 0.018806476145982742 0.05770224332809448 0.070082888007164 4.41796875\n",
            "repr, std, cov, clossl, z, norm 0.011885209009051323 0.41015625 0.018425865098834038 0.06452800333499908 0.06638286262750626 3.98828125\n",
            "repr, std, cov, clossl, z, norm 0.010783706791698933 0.411376953125 0.016976993530988693 0.05433220788836479 0.038291409611701965 4.56640625\n",
            "repr, std, cov, clossl, z, norm 0.011668495833873749 0.407958984375 0.01998564787209034 0.06892120093107224 0.1338045746088028 3.736328125\n",
            "repr, std, cov, clossl, z, norm 0.011413400992751122 0.41015625 0.018074946478009224 0.05596122518181801 0.08512834459543228 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.010830549523234367 0.408203125 0.019259700551629066 0.04797518625855446 0.02441154420375824 4.53125\n",
            "116\n",
            "repr, std, cov, clossl, z, norm 0.011499656364321709 0.4052734375 0.022535350173711777 0.05157966539263725 0.09625479578971863 4.76953125\n",
            "repr, std, cov, clossl, z, norm 0.012310788035392761 0.40234375 0.024971093982458115 0.07862617075443268 0.18930211663246155 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.011387974955141544 0.402099609375 0.025405259802937508 0.06335871666669846 0.06419945508241653 3.763671875\n",
            "repr, std, cov, clossl, z, norm 0.012358921580016613 0.395751953125 0.03243468701839447 0.07578113675117493 0.028999244794249535 2.654296875\n",
            "repr, std, cov, clossl, z, norm 0.01168421283364296 0.400390625 0.027106553316116333 0.06414687633514404 0.08131713420152664 4.71875\n",
            "repr, std, cov, clossl, z, norm 0.011440355330705643 0.4072265625 0.021009279415011406 0.05232136696577072 0.051835883408784866 4.3671875\n",
            "repr, std, cov, clossl, z, norm 0.011123527772724628 0.406982421875 0.02049444429576397 0.057794030755758286 0.016049524769186974 4.953125\n",
            "repr, std, cov, clossl, z, norm 0.011156158521771431 0.407958984375 0.02015729621052742 0.06501340121030807 0.2558909058570862 4.7421875\n",
            "117\n",
            "repr, std, cov, clossl, z, norm 0.00966665055602789 0.41015625 0.01815820299088955 0.04515920579433441 0.12483268976211548 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.011434205807745457 0.4033203125 0.024215100333094597 0.08660100400447845 0.13737449049949646 3.841796875\n",
            "repr, std, cov, clossl, z, norm 0.010963303968310356 0.40087890625 0.02658599056303501 0.05817519500851631 0.10133302956819534 3.3125\n",
            "repr, std, cov, clossl, z, norm 0.009787573479115963 0.4033203125 0.02378915064036846 0.04446791857481003 0.13887929916381836 4.1796875\n",
            "repr, std, cov, clossl, z, norm 0.0105958366766572 0.40185546875 0.02506221830844879 0.0615151971578598 0.17480072379112244 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.011554884724318981 0.40283203125 0.024082407355308533 0.07144810259342194 0.07880260795354843 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.010840067639946938 0.394775390625 0.037908438593149185 0.04530314356088638 0.08329224586486816 4.16015625\n",
            "repr, std, cov, clossl, z, norm 0.011086363345384598 0.40283203125 0.02446248196065426 0.05289721488952637 0.04894617199897766 4.48046875\n",
            "118\n",
            "repr, std, cov, clossl, z, norm 0.011221789754927158 0.401611328125 0.025669414550065994 0.05378137528896332 0.10636746138334274 3.82421875\n",
            "repr, std, cov, clossl, z, norm 0.010251815430819988 0.406982421875 0.021462492644786835 0.060420140624046326 0.13283008337020874 3.833984375\n",
            "repr, std, cov, clossl, z, norm 0.010963991284370422 0.406982421875 0.02093243971467018 0.05434694141149521 0.06937716156244278 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.010213413275778294 0.409912109375 0.018359791487455368 0.04709046334028244 0.05474546551704407 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.01103384979069233 0.407470703125 0.020299693569540977 0.05592456832528114 0.08208707720041275 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.011516815982758999 0.40771484375 0.01999368518590927 0.05938106030225754 0.0558057501912117 4.70703125\n",
            "repr, std, cov, clossl, z, norm 0.012457873672246933 0.40478515625 0.02387302927672863 0.05900013819336891 0.13007615506649017 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.010685515590012074 0.408447265625 0.019479066133499146 0.04994542896747589 0.16264396905899048 4.6640625\n",
            "119\n",
            "repr, std, cov, clossl, z, norm 0.011855113320052624 0.402099609375 0.02486252412199974 0.05192863941192627 0.04337877780199051 4.234375\n",
            "repr, std, cov, clossl, z, norm 0.011456788517534733 0.398193359375 0.028416316956281662 0.05466212332248688 0.1503080427646637 5.01953125\n",
            "repr, std, cov, clossl, z, norm 0.012328254990279675 0.396728515625 0.03010987490415573 0.05842568352818489 0.05562235787510872 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.010762338526546955 0.409423828125 0.0176127627491951 0.06014290079474449 0.07195358723402023 4.125\n",
            "repr, std, cov, clossl, z, norm 0.011550883762538433 0.4013671875 0.025815464556217194 0.04853225499391556 0.1517549753189087 3.66796875\n",
            "repr, std, cov, clossl, z, norm 0.01196661964058876 0.404052734375 0.02208811230957508 0.06433200091123581 0.31378573179244995 4.65234375\n",
            "repr, std, cov, clossl, z, norm 0.010952104814350605 0.406005859375 0.020409615710377693 0.041935328394174576 0.12382765859365463 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.011377101764082909 0.404052734375 0.022449735552072525 0.07599209249019623 0.06382191181182861 5.65625\n",
            "120\n",
            "repr, std, cov, clossl, z, norm 0.011417201720178127 0.404541015625 0.02207046188414097 0.0565309040248394 0.12529169023036957 4.75\n",
            "repr, std, cov, clossl, z, norm 0.012278403155505657 0.400146484375 0.02585701458156109 0.06444709748029709 0.0692741870880127 6.16015625\n",
            "repr, std, cov, clossl, z, norm 0.012247307226061821 0.401123046875 0.0251753069460392 0.05598961561918259 0.12739448249340057 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.011765915900468826 0.40380859375 0.02392907813191414 0.055150069296360016 0.09871610254049301 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.012973038479685783 0.39794921875 0.027587980031967163 0.06284720450639725 0.13326284289360046 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.012483665719628334 0.402099609375 0.023811865597963333 0.06264469027519226 0.1820550560951233 4.8203125\n",
            "repr, std, cov, clossl, z, norm 0.011836730875074863 0.405029296875 0.021802369505167007 0.06802535802125931 0.11159765720367432 3.216796875\n",
            "repr, std, cov, clossl, z, norm 0.012029366567730904 0.40087890625 0.02483915165066719 0.06773972511291504 0.1066715270280838 4.45703125\n",
            "121\n",
            "repr, std, cov, clossl, z, norm 0.012323904782533646 0.398193359375 0.02699173428118229 0.0847218856215477 0.08780515193939209 4.90625\n",
            "repr, std, cov, clossl, z, norm 0.0103398896753788 0.4052734375 0.021057704463601112 0.054375529289245605 0.08273075520992279 3.89453125\n",
            "repr, std, cov, clossl, z, norm 0.0122373653575778 0.3974609375 0.028189148753881454 0.08264325559139252 0.045211173593997955 4.3515625\n",
            "repr, std, cov, clossl, z, norm 0.011105305515229702 0.402099609375 0.02313123643398285 0.05453137308359146 0.07711758464574814 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.011335121467709541 0.402099609375 0.023638814687728882 0.06672894954681396 0.3048216700553894 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.011593651957809925 0.400634765625 0.02499404363334179 0.07866471260786057 0.10143651068210602 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.010292625054717064 0.40380859375 0.021807372570037842 0.049363020807504654 0.08584049344062805 4.87109375\n",
            "repr, std, cov, clossl, z, norm 0.011399822309613228 0.40087890625 0.02457023784518242 0.09345956891775131 0.1325657069683075 4.66015625\n",
            "122\n",
            "repr, std, cov, clossl, z, norm 0.011317646130919456 0.402587890625 0.02277873083949089 0.07004596292972565 0.07755804806947708 4.91796875\n",
            "repr, std, cov, clossl, z, norm 0.01125161163508892 0.401123046875 0.02418586239218712 0.06193270534276962 0.043105632066726685 4.4921875\n",
            "repr, std, cov, clossl, z, norm 0.010959221981465816 0.404296875 0.02138109877705574 0.05194392055273056 0.05500667169690132 3.912109375\n",
            "repr, std, cov, clossl, z, norm 0.010895831510424614 0.401123046875 0.024353627115488052 0.06337244063615799 0.1279900223016739 4.31640625\n",
            "repr, std, cov, clossl, z, norm 0.010729429312050343 0.403564453125 0.023159436881542206 0.06309624761343002 0.08925433456897736 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.01142328791320324 0.40380859375 0.022129731252789497 0.07360728830099106 0.05320969223976135 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.012252089567482471 0.402587890625 0.023182755336165428 0.10587593168020248 0.12698106467723846 4.8359375\n",
            "repr, std, cov, clossl, z, norm 0.010333220474421978 0.4033203125 0.02293545752763748 0.05577978119254112 0.10332302004098892 4.55859375\n",
            "123\n",
            "repr, std, cov, clossl, z, norm 0.011477582156658173 0.400634765625 0.025397956371307373 0.06225312873721123 0.12834790349006653 3.751953125\n",
            "repr, std, cov, clossl, z, norm 0.013741033151745796 0.393310546875 0.03485008329153061 0.0903162956237793 0.08835985511541367 4.6953125\n",
            "repr, std, cov, clossl, z, norm 0.0114948321133852 0.403564453125 0.022225182503461838 0.07553675025701523 0.14408797025680542 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.011551198549568653 0.39990234375 0.025255806744098663 0.07875313609838486 0.20167112350463867 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.011378718540072441 0.4052734375 0.021360818296670914 0.06871809810400009 0.033180296421051025 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.012808208353817463 0.3994140625 0.026658281683921814 0.08565276116132736 0.09316547960042953 4.43359375\n",
            "repr, std, cov, clossl, z, norm 0.012982489541172981 0.39697265625 0.030905794352293015 0.11035636812448502 0.07221724838018417 4.7109375\n",
            "repr, std, cov, clossl, z, norm 0.010399594902992249 0.408447265625 0.018459200859069824 0.06785979866981506 0.19388675689697266 4.1953125\n",
            "124\n",
            "repr, std, cov, clossl, z, norm 0.010619521141052246 0.41064453125 0.017916115000844002 0.06248063966631889 0.1561129093170166 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.011081550270318985 0.40771484375 0.019890807569026947 0.07743994891643524 0.029013831168413162 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.010692602954804897 0.407470703125 0.01966795325279236 0.07281770557165146 0.23754554986953735 4.54296875\n",
            "repr, std, cov, clossl, z, norm 0.01120788510888815 0.399658203125 0.02817414328455925 0.08055379986763 0.15257130563259125 3.08984375\n",
            "repr, std, cov, clossl, z, norm 0.011942212469875813 0.400390625 0.02544049732387066 0.07535508275032043 0.11270551383495331 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.0118867764249444 0.399658203125 0.025763139128684998 0.07819538563489914 0.012274743989109993 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.012127839960157871 0.400390625 0.025593871250748634 0.07285743951797485 0.1369716227054596 3.837890625\n",
            "repr, std, cov, clossl, z, norm 0.011455018073320389 0.402587890625 0.02342003956437111 0.07307067513465881 0.04049083963036537 4.078125\n",
            "125\n",
            "repr, std, cov, clossl, z, norm 0.011801963672041893 0.402587890625 0.022958826273679733 0.06381607800722122 0.1340651661157608 3.921875\n",
            "repr, std, cov, clossl, z, norm 0.012493510730564594 0.3984375 0.03189527988433838 0.0889158770442009 0.06978889554738998 2.833984375\n",
            "repr, std, cov, clossl, z, norm 0.012778708711266518 0.40185546875 0.022698868066072464 0.07145427167415619 0.016501668840646744 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.011165544390678406 0.40576171875 0.019580380991101265 0.06734324991703033 0.0750281810760498 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.010600307025015354 0.406005859375 0.01987491175532341 0.05566396564245224 0.0902755856513977 4.9609375\n",
            "repr, std, cov, clossl, z, norm 0.011157884262502193 0.401123046875 0.02462705224752426 0.06804293394088745 0.040353406220674515 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.011607708409428596 0.405517578125 0.020345721393823624 0.06629198789596558 0.05617717280983925 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.011423065327107906 0.403076171875 0.022576231509447098 0.06136075779795647 0.1409245878458023 3.896484375\n",
            "126\n",
            "repr, std, cov, clossl, z, norm 0.01033448800444603 0.4091796875 0.01707356795668602 0.05824822187423706 0.105576291680336 4.0859375\n",
            "repr, std, cov, clossl, z, norm 0.01233027782291174 0.401123046875 0.023649193346500397 0.07981868833303452 0.032755013555288315 3.9765625\n",
            "repr, std, cov, clossl, z, norm 0.011758515611290932 0.39599609375 0.03194651007652283 0.07119834423065186 0.10939563065767288 4.26171875\n",
            "repr, std, cov, clossl, z, norm 0.011736688204109669 0.39501953125 0.0310832466930151 0.051811203360557556 0.12323109805583954 4.09765625\n",
            "repr, std, cov, clossl, z, norm 0.011725463904440403 0.40234375 0.023459602147340775 0.06798180192708969 0.05085199698805809 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.010997402481734753 0.404052734375 0.022380894050002098 0.07099251449108124 0.05918338522315025 5.03125\n",
            "repr, std, cov, clossl, z, norm 0.010789292864501476 0.406982421875 0.01958020031452179 0.06065932288765907 0.12729351222515106 3.67578125\n",
            "repr, std, cov, clossl, z, norm 0.011074342764914036 0.40576171875 0.020460128784179688 0.05509566515684128 0.04801620915532112 3.66015625\n",
            "127\n",
            "repr, std, cov, clossl, z, norm 0.010409216396510601 0.408935546875 0.01821208745241165 0.05251942574977875 0.030872715637087822 4.60546875\n",
            "repr, std, cov, clossl, z, norm 0.011463092640042305 0.401611328125 0.024574249982833862 0.06299717724323273 0.1600281447172165 3.89453125\n",
            "repr, std, cov, clossl, z, norm 0.010831860825419426 0.40380859375 0.021337369456887245 0.053748659789562225 0.04285968095064163 4.58203125\n",
            "repr, std, cov, clossl, z, norm 0.011890890076756477 0.396240234375 0.029311584308743477 0.07435116171836853 0.09000040590763092 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.011706564575433731 0.396484375 0.029923750087618828 0.08087766170501709 0.09646677225828171 4.06640625\n",
            "repr, std, cov, clossl, z, norm 0.012124096974730492 0.39892578125 0.02592448703944683 0.05492003634572029 0.0934479758143425 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.01165152620524168 0.3994140625 0.027012569829821587 0.06831849366426468 0.05133531242609024 3.859375\n",
            "repr, std, cov, clossl, z, norm 0.01103039737790823 0.40234375 0.023658882826566696 0.06107686087489128 0.019020136445760727 4.30078125\n",
            "128\n",
            "repr, std, cov, clossl, z, norm 0.010609538294374943 0.401123046875 0.023659085854887962 0.048912081867456436 0.1157427653670311 4.9765625\n",
            "repr, std, cov, clossl, z, norm 0.010253391228616238 0.39990234375 0.027215417474508286 0.0497344471514225 0.04303843528032303 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.011683725751936436 0.39501953125 0.03135018050670624 0.06900741159915924 0.121096670627594 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.011623715981841087 0.40087890625 0.025801271200180054 0.07025088369846344 0.02619294449687004 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.011391031555831432 0.3994140625 0.026944128796458244 0.07110201567411423 0.03335445001721382 4.65625\n",
            "repr, std, cov, clossl, z, norm 0.011485042050480843 0.400146484375 0.025943120941519737 0.08259807527065277 0.07603582739830017 4.19140625\n",
            "repr, std, cov, clossl, z, norm 0.011124186217784882 0.404541015625 0.020793238654732704 0.060539696365594864 0.09085147827863693 4.765625\n",
            "repr, std, cov, clossl, z, norm 0.010041958652436733 0.403564453125 0.022719595581293106 0.06252241134643555 0.14868535101413727 3.697265625\n",
            "129\n",
            "repr, std, cov, clossl, z, norm 0.010082952678203583 0.4033203125 0.02350565418601036 0.05126331374049187 0.17484258115291595 3.8828125\n",
            "repr, std, cov, clossl, z, norm 0.011387161910533905 0.40087890625 0.02423720434308052 0.06606480479240417 0.1223641037940979 4.39453125\n",
            "repr, std, cov, clossl, z, norm 0.011213192716240883 0.402587890625 0.02303444966673851 0.06158347800374031 0.06768207997083664 4.29296875\n",
            "repr, std, cov, clossl, z, norm 0.01095284428447485 0.40576171875 0.020152229815721512 0.07506806403398514 0.06081603839993477 2.7890625\n",
            "repr, std, cov, clossl, z, norm 0.011587321758270264 0.40087890625 0.024790234863758087 0.06890478730201721 0.07326360046863556 4.3515625\n",
            "repr, std, cov, clossl, z, norm 0.011578230187296867 0.396728515625 0.028852732852101326 0.0637224093079567 0.1493820995092392 2.94921875\n",
            "repr, std, cov, clossl, z, norm 0.011595998890697956 0.39697265625 0.028146103024482727 0.0624224990606308 0.10380503535270691 3.76953125\n",
            "repr, std, cov, clossl, z, norm 0.01239993330091238 0.390625 0.0366397500038147 0.07681064307689667 0.12100937962532043 3.982421875\n",
            "130\n",
            "repr, std, cov, clossl, z, norm 0.011198730207979679 0.404052734375 0.020597612485289574 0.06119026988744736 0.07668757438659668 4.50390625\n",
            "repr, std, cov, clossl, z, norm 0.01183438953012228 0.402587890625 0.022218547761440277 0.059539373964071274 0.1165565624833107 4.75390625\n",
            "repr, std, cov, clossl, z, norm 0.012690989300608635 0.4013671875 0.024773409590125084 0.06684190779924393 0.0527864471077919 3.46484375\n",
            "repr, std, cov, clossl, z, norm 0.012396905571222305 0.40087890625 0.024264678359031677 0.06430121511220932 0.11002223193645477 4.9296875\n",
            "repr, std, cov, clossl, z, norm 0.011566710658371449 0.406494140625 0.01944601908326149 0.05324656516313553 0.0446549691259861 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.012164721265435219 0.401123046875 0.024952609091997147 0.06922274827957153 0.11925609409809113 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.012726139277219772 0.400146484375 0.024947311729192734 0.09611355513334274 0.17976240813732147 4.15625\n",
            "repr, std, cov, clossl, z, norm 0.01104468572884798 0.405029296875 0.02015918679535389 0.06317044794559479 0.12939895689487457 2.296875\n",
            "131\n",
            "repr, std, cov, clossl, z, norm 0.01193232275545597 0.406005859375 0.01996723562479019 0.06592503935098648 0.0873250812292099 4.54296875\n",
            "repr, std, cov, clossl, z, norm 0.012128668837249279 0.403076171875 0.022582128643989563 0.06039461866021156 0.10096559673547745 4.70703125\n",
            "repr, std, cov, clossl, z, norm 0.011691498570144176 0.407470703125 0.019284669309854507 0.0639863982796669 0.17065392434597015 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.012889363802969456 0.40234375 0.023272095248103142 0.07576107978820801 0.16388119757175446 4.0859375\n",
            "repr, std, cov, clossl, z, norm 0.012410135008394718 0.404052734375 0.021825354546308517 0.0844639390707016 0.04065144062042236 4.5859375\n",
            "repr, std, cov, clossl, z, norm 0.01090716477483511 0.399169921875 0.02799450047314167 0.04910225793719292 0.06457168608903885 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.011132913641631603 0.4052734375 0.022329028695821762 0.07737205177545547 0.11643381416797638 4.91796875\n",
            "repr, std, cov, clossl, z, norm 0.012007446028292179 0.400390625 0.025682013481855392 0.06657075881958008 0.07869014143943787 4.80859375\n",
            "132\n",
            "repr, std, cov, clossl, z, norm 0.011848852038383484 0.402099609375 0.02279471792280674 0.06018125265836716 0.04438457638025284 3.904296875\n",
            "repr, std, cov, clossl, z, norm 0.014082169160246849 0.39794921875 0.027504701167345047 0.0875108540058136 0.07387151569128036 3.99609375\n",
            "repr, std, cov, clossl, z, norm 0.013372668996453285 0.39794921875 0.02704373188316822 0.07502999901771545 0.221437007188797 4.53515625\n",
            "repr, std, cov, clossl, z, norm 0.013142119161784649 0.39794921875 0.027445558458566666 0.08125581592321396 0.013173761777579784 3.56640625\n",
            "repr, std, cov, clossl, z, norm 0.014445926994085312 0.396240234375 0.03126584738492966 0.09864714741706848 0.026481589302420616 3.9296875\n",
            "repr, std, cov, clossl, z, norm 0.011643202044069767 0.4052734375 0.020676318556070328 0.05668048560619354 0.10073758661746979 3.66796875\n",
            "repr, std, cov, clossl, z, norm 0.014117131009697914 0.392333984375 0.0338967964053154 0.094670869410038 0.09658188372850418 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.013318538665771484 0.394287109375 0.03223076090216637 0.10192384570837021 0.13334651291370392 3.689453125\n",
            "133\n",
            "repr, std, cov, clossl, z, norm 0.013349723070859909 0.401611328125 0.02297426015138626 0.08900602161884308 0.023335525766015053 3.86328125\n",
            "repr, std, cov, clossl, z, norm 0.01213383860886097 0.406494140625 0.021049242466688156 0.07422176003456116 0.006782887037843466 3.517578125\n",
            "repr, std, cov, clossl, z, norm 0.011822368018329144 0.40673828125 0.022519491612911224 0.08913157880306244 0.0901620090007782 3.751953125\n",
            "repr, std, cov, clossl, z, norm 0.010932118631899357 0.410400390625 0.016224753111600876 0.08447658270597458 0.19782444834709167 5.171875\n",
            "repr, std, cov, clossl, z, norm 0.010051391087472439 0.410888671875 0.016694167628884315 0.052499838173389435 0.1395382285118103 3.958984375\n",
            "repr, std, cov, clossl, z, norm 0.011564554646611214 0.4072265625 0.020756274461746216 0.07726332545280457 0.1786591112613678 4.02734375\n",
            "repr, std, cov, clossl, z, norm 0.011264458298683167 0.406982421875 0.019933724775910378 0.06872517615556717 0.06378774344921112 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.011750089935958385 0.40966796875 0.0179385207593441 0.0744236633181572 0.003380995010957122 3.33984375\n",
            "134\n",
            "repr, std, cov, clossl, z, norm 0.012031209655106068 0.4072265625 0.018903445452451706 0.08768909424543381 0.18111862242221832 3.880859375\n",
            "repr, std, cov, clossl, z, norm 0.0119843240827322 0.40185546875 0.02371581643819809 0.0879988744854927 0.12112100422382355 3.9765625\n",
            "repr, std, cov, clossl, z, norm 0.011949436739087105 0.402099609375 0.022733088582754135 0.0728088915348053 0.08951423317193985 4.31640625\n",
            "repr, std, cov, clossl, z, norm 0.01154946070164442 0.401123046875 0.02509690821170807 0.0797625258564949 0.09280309081077576 3.95703125\n",
            "repr, std, cov, clossl, z, norm 0.011597353965044022 0.39892578125 0.026052560657262802 0.07968796789646149 0.2528397738933563 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.011104363016784191 0.394775390625 0.030526326969265938 0.0616738386452198 0.19437429308891296 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.012458836659789085 0.3876953125 0.03804202750325203 0.07357621937990189 0.3429235816001892 3.9375\n",
            "repr, std, cov, clossl, z, norm 0.01157449372112751 0.394775390625 0.030081845819950104 0.09241490066051483 0.09288149327039719 5.1171875\n",
            "135\n",
            "repr, std, cov, clossl, z, norm 0.011388805694878101 0.40283203125 0.02192641794681549 0.06953894346952438 0.0 2.751953125\n",
            "repr, std, cov, clossl, z, norm 0.011633600108325481 0.407958984375 0.018693340942263603 0.06403935700654984 0.09730130434036255 4.43359375\n",
            "repr, std, cov, clossl, z, norm 0.011465966701507568 0.4033203125 0.02246919646859169 0.05214172229170799 0.021800866350531578 2.033203125\n",
            "repr, std, cov, clossl, z, norm 0.012230240739881992 0.40380859375 0.02221548557281494 0.07022050023078918 0.2142910361289978 4.4296875\n",
            "repr, std, cov, clossl, z, norm 0.011352285742759705 0.402099609375 0.023332418873906136 0.10927335172891617 0.08748260885477066 4.890625\n",
            "repr, std, cov, clossl, z, norm 0.011833690106868744 0.395263671875 0.03189482539892197 0.07195575535297394 0.08273735642433167 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.012925863265991211 0.390625 0.03705465793609619 0.0731630027294159 0.059787921607494354 3.998046875\n",
            "repr, std, cov, clossl, z, norm 0.01390749216079712 0.39208984375 0.03308585286140442 0.0872427448630333 0.11575200408697128 3.7734375\n",
            "136\n",
            "repr, std, cov, clossl, z, norm 0.01320144534111023 0.3984375 0.02657783403992653 0.07556577771902084 0.1377592533826828 4.02734375\n",
            "repr, std, cov, clossl, z, norm 0.012465526349842548 0.4052734375 0.021916531026363373 0.08052220940589905 0.08272744715213776 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.012823790311813354 0.4013671875 0.026473945006728172 0.06733185052871704 0.09831546247005463 4.27734375\n",
            "repr, std, cov, clossl, z, norm 0.011651551350951195 0.40625 0.020591959357261658 0.08476251363754272 0.0872369185090065 4.6171875\n",
            "repr, std, cov, clossl, z, norm 0.011674609035253525 0.4052734375 0.02120833285152912 0.09432683885097504 0.0 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.012196359224617481 0.39892578125 0.029587402939796448 0.0936766192317009 0.07963148504495621 3.7421875\n",
            "repr, std, cov, clossl, z, norm 0.011483545415103436 0.40234375 0.027339817956089973 0.06847323477268219 0.003983883652836084 3.630859375\n",
            "repr, std, cov, clossl, z, norm 0.011494704522192478 0.4052734375 0.02174460142850876 0.07335960865020752 0.0798405334353447 3.599609375\n",
            "137\n",
            "repr, std, cov, clossl, z, norm 0.01099850982427597 0.405517578125 0.020375311374664307 0.047223299741744995 0.14894692599773407 3.505859375\n",
            "repr, std, cov, clossl, z, norm 0.013486971147358418 0.404296875 0.02267572656273842 0.08738753944635391 0.14423538744449615 2.802734375\n",
            "repr, std, cov, clossl, z, norm 0.011904871091246605 0.4111328125 0.016821622848510742 0.06206897646188736 0.0332258976995945 4.27734375\n",
            "repr, std, cov, clossl, z, norm 0.012849723920226097 0.4091796875 0.020661354064941406 0.07443170249462128 0.20876792073249817 4.12890625\n",
            "repr, std, cov, clossl, z, norm 0.012066006660461426 0.40771484375 0.021497048437595367 0.12596040964126587 0.06869551539421082 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.012766644358634949 0.401123046875 0.02580130845308304 0.0896584764122963 0.09563363343477249 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.013236450962722301 0.397705078125 0.02768762968480587 0.08747421205043793 0.11642326414585114 2.83203125\n",
            "repr, std, cov, clossl, z, norm 0.0136576471850276 0.39453125 0.03348573297262192 0.08936269581317902 0.0631721243262291 3.4453125\n",
            "138\n",
            "repr, std, cov, clossl, z, norm 0.014415091834962368 0.397216796875 0.029113369062542915 0.10723350942134857 0.17654210329055786 3.857421875\n",
            "repr, std, cov, clossl, z, norm 0.012471535243093967 0.40283203125 0.023187607526779175 0.083243727684021 0.09688825160264969 4.8125\n",
            "repr, std, cov, clossl, z, norm 0.013414165936410427 0.406005859375 0.020480459555983543 0.1015467718243599 0.03566175326704979 3.3671875\n",
            "repr, std, cov, clossl, z, norm 0.012234515510499477 0.408935546875 0.01856571063399315 0.07762816548347473 0.03833436965942383 3.736328125\n",
            "repr, std, cov, clossl, z, norm 0.013139014132320881 0.40625 0.021601181477308273 0.07860881835222244 0.07557974755764008 4.13671875\n",
            "repr, std, cov, clossl, z, norm 0.013167709112167358 0.406005859375 0.02137879654765129 0.06845182925462723 0.1251516193151474 3.908203125\n",
            "repr, std, cov, clossl, z, norm 0.014779020100831985 0.405517578125 0.023483527824282646 0.21534164249897003 0.17222531139850616 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.012898110784590244 0.400390625 0.027353188022971153 0.07317471504211426 0.11303508281707764 3.763671875\n",
            "139\n",
            "repr, std, cov, clossl, z, norm 0.013174599036574364 0.395751953125 0.03249362111091614 0.11031659692525864 0.09001113474369049 2.5\n",
            "repr, std, cov, clossl, z, norm 0.014142400585114956 0.39404296875 0.03390125185251236 0.13627147674560547 0.24114950001239777 3.33984375\n",
            "repr, std, cov, clossl, z, norm 0.015096861869096756 0.395751953125 0.031302280724048615 0.17236590385437012 0.06194453686475754 5.046875\n",
            "repr, std, cov, clossl, z, norm 0.013492959551513195 0.405517578125 0.021857429295778275 0.14139017462730408 0.11275269091129303 2.478515625\n",
            "repr, std, cov, clossl, z, norm 0.012794108130037785 0.405029296875 0.02298934943974018 0.10361501574516296 0.10099266469478607 5.3515625\n",
            "repr, std, cov, clossl, z, norm 0.013207529671490192 0.407958984375 0.021196609362959862 0.18753352761268616 0.11038558930158615 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.014068269170820713 0.401123046875 0.03204623609781265 0.13471439480781555 0.08190158754587173 3.537109375\n",
            "repr, std, cov, clossl, z, norm 0.013342050835490227 0.4072265625 0.022343739867210388 0.105631023645401 0.18041813373565674 3.734375\n",
            "140\n",
            "repr, std, cov, clossl, z, norm 0.015423493459820747 0.403564453125 0.026484627276659012 0.12413951754570007 0.0966622605919838 3.681640625\n",
            "repr, std, cov, clossl, z, norm 0.014788324944674969 0.406494140625 0.02251734957098961 0.10606173425912857 0.08019211143255234 3.310546875\n",
            "repr, std, cov, clossl, z, norm 0.014523538760840893 0.41015625 0.01858261413872242 0.13342079520225525 0.10577955842018127 3.783203125\n",
            "repr, std, cov, clossl, z, norm 0.014282476156949997 0.412109375 0.016784757375717163 0.11505372822284698 0.1050625890493393 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.013878850266337395 0.412353515625 0.017031230032444 0.07879095524549484 0.19932757318019867 3.732421875\n",
            "repr, std, cov, clossl, z, norm 0.01531405933201313 0.4072265625 0.021234747022390366 0.12002257257699966 0.07790471613407135 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.015888044610619545 0.40673828125 0.020591922104358673 0.16254840791225433 0.15466338396072388 3.791015625\n",
            "repr, std, cov, clossl, z, norm 0.014283961616456509 0.408203125 0.019795969128608704 0.14555451273918152 0.16012710332870483 3.927734375\n",
            "141\n",
            "repr, std, cov, clossl, z, norm 0.014267279766499996 0.4013671875 0.02633262798190117 0.10357078164815903 0.24768735468387604 4.546875\n",
            "repr, std, cov, clossl, z, norm 0.014261918142437935 0.403076171875 0.025897085666656494 0.12417164444923401 0.17202787101268768 2.849609375\n",
            "repr, std, cov, clossl, z, norm 0.014828464947640896 0.4033203125 0.023301327601075172 0.160577654838562 0.16047152876853943 3.09375\n",
            "repr, std, cov, clossl, z, norm 0.015570532530546188 0.400390625 0.02563382312655449 0.126347154378891 0.09274528920650482 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.015289642848074436 0.40087890625 0.026418045163154602 0.15192271769046783 0.10738695412874222 3.423828125\n",
            "repr, std, cov, clossl, z, norm 0.014890125952661037 0.400146484375 0.027168026193976402 0.14255103468894958 0.19351419806480408 3.1015625\n",
            "repr, std, cov, clossl, z, norm 0.015043864026665688 0.399658203125 0.026308098807930946 0.23821818828582764 0.10976949334144592 2.89453125\n",
            "repr, std, cov, clossl, z, norm 0.013889973051846027 0.39990234375 0.02721114456653595 0.1297326534986496 0.1613430380821228 2.916015625\n",
            "142\n",
            "repr, std, cov, clossl, z, norm 0.014871942810714245 0.398681640625 0.02710740827023983 0.1453429013490677 0.16283079981803894 3.9140625\n",
            "repr, std, cov, clossl, z, norm 0.015054001472890377 0.3984375 0.027303621172904968 0.14211460947990417 0.03201484680175781 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.015198031440377235 0.405029296875 0.022103216499090195 0.14670692384243011 0.0768311619758606 4.13671875\n",
            "repr, std, cov, clossl, z, norm 0.015979554504156113 0.407470703125 0.02073492482304573 0.2024918645620346 0.07921739667654037 2.65625\n",
            "repr, std, cov, clossl, z, norm 0.01550805103033781 0.40087890625 0.023585636168718338 0.19660942256450653 0.20763380825519562 3.224609375\n",
            "repr, std, cov, clossl, z, norm 0.015161421149969101 0.396728515625 0.029178231954574585 0.19430579245090485 0.1721421629190445 4.09375\n",
            "repr, std, cov, clossl, z, norm 0.016444651409983635 0.390869140625 0.03938937932252884 0.1885739117860794 0.3106235861778259 1.9462890625\n",
            "repr, std, cov, clossl, z, norm 0.01565699093043804 0.39892578125 0.027336612343788147 0.13169118762016296 0.11263611167669296 2.955078125\n",
            "143\n",
            "repr, std, cov, clossl, z, norm 0.01759021170437336 0.405517578125 0.0198550745844841 0.1618199497461319 0.08909901231527328 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.01669742539525032 0.409423828125 0.0169488787651062 0.17082694172859192 0.019943755120038986 3.708984375\n",
            "repr, std, cov, clossl, z, norm 0.016105178743600845 0.406494140625 0.021379990503191948 0.22045396268367767 0.08781791478395462 3.375\n",
            "repr, std, cov, clossl, z, norm 0.014279626309871674 0.39794921875 0.0357968807220459 0.15230761468410492 0.10667668282985687 1.494140625\n",
            "repr, std, cov, clossl, z, norm 0.01595575362443924 0.39453125 0.03607456386089325 0.21373124420642853 0.17390179634094238 2.896484375\n",
            "repr, std, cov, clossl, z, norm 0.015788614749908447 0.402587890625 0.023970957845449448 0.1970725953578949 0.18060654401779175 3.0625\n",
            "repr, std, cov, clossl, z, norm 0.015423956327140331 0.404052734375 0.0252375490963459 0.18350689113140106 0.148481085896492 3.142578125\n",
            "repr, std, cov, clossl, z, norm 0.014685899950563908 0.406982421875 0.026464063674211502 0.20257370173931122 0.0910763218998909 4.33203125\n",
            "144\n",
            "repr, std, cov, clossl, z, norm 0.01429425086826086 0.40966796875 0.019201168790459633 0.13996583223342896 0.047170091420412064 2.783203125\n",
            "repr, std, cov, clossl, z, norm 0.014906663447618484 0.4091796875 0.019504647701978683 0.13723832368850708 0.07386711984872818 3.009765625\n",
            "repr, std, cov, clossl, z, norm 0.015240330249071121 0.405517578125 0.023641090840101242 0.18064433336257935 0.2210022509098053 3.810546875\n",
            "repr, std, cov, clossl, z, norm 0.015598144382238388 0.402587890625 0.025471732020378113 0.1936444640159607 0.14888516068458557 3.5234375\n",
            "repr, std, cov, clossl, z, norm 0.01662103645503521 0.3974609375 0.03204795718193054 0.19825901091098785 0.05454929545521736 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.017611049115657806 0.401123046875 0.027680352330207825 0.18714489042758942 0.3298179507255554 2.986328125\n",
            "repr, std, cov, clossl, z, norm 0.015513119287788868 0.409912109375 0.018031230196356773 0.19050918519496918 0.12698473036289215 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.01601094752550125 0.40966796875 0.017677243798971176 0.17857742309570312 0.13480426371097565 3.19140625\n",
            "145\n",
            "repr, std, cov, clossl, z, norm 0.014765484258532524 0.408447265625 0.01963263377547264 0.12073353677988052 0.045098867267370224 3.876953125\n",
            "repr, std, cov, clossl, z, norm 0.014893132261931896 0.4091796875 0.019098229706287384 0.20524978637695312 0.06922650337219238 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.014505255967378616 0.408203125 0.02047508955001831 0.14180341362953186 0.18112178146839142 4.0234375\n",
            "repr, std, cov, clossl, z, norm 0.014591455459594727 0.406982421875 0.021026674658060074 0.16007573902606964 0.2397107034921646 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.014954722486436367 0.398681640625 0.033724166452884674 0.15787699818611145 0.18559527397155762 2.43359375\n",
            "repr, std, cov, clossl, z, norm 0.015932612121105194 0.397705078125 0.033625297248363495 0.18362189829349518 0.09026522189378738 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.016693655401468277 0.3994140625 0.028068548068404198 0.17617614567279816 0.08553490787744522 1.890625\n",
            "repr, std, cov, clossl, z, norm 0.016922736540436745 0.400390625 0.026301931589841843 0.21201258897781372 0.04919800907373428 2.365234375\n",
            "146\n",
            "repr, std, cov, clossl, z, norm 0.015332894399762154 0.40380859375 0.022083086892962456 0.1471811681985855 0.08710402995347977 3.25\n",
            "repr, std, cov, clossl, z, norm 0.015068971551954746 0.402099609375 0.02391069196164608 0.12237890809774399 0.21800363063812256 2.26171875\n",
            "repr, std, cov, clossl, z, norm 0.01614335924386978 0.40283203125 0.026335880160331726 0.15556883811950684 0.1429561972618103 0.0\n",
            "repr, std, cov, clossl, z, norm 0.016443511471152306 0.407470703125 0.021295443177223206 0.17590327560901642 0.17111876606941223 2.22265625\n",
            "repr, std, cov, clossl, z, norm 0.015814010053873062 0.409912109375 0.018179427832365036 0.15597645938396454 0.15633641183376312 5.08984375\n",
            "repr, std, cov, clossl, z, norm 0.017099158838391304 0.402587890625 0.023696184158325195 0.1839710921049118 0.10266172140836716 3.791015625\n",
            "repr, std, cov, clossl, z, norm 0.017294831573963165 0.40283203125 0.023110732436180115 0.1434539556503296 0.15658608078956604 3.205078125\n",
            "repr, std, cov, clossl, z, norm 0.018052246421575546 0.39892578125 0.02700435370206833 0.1821681708097458 0.050793878734111786 2.90625\n",
            "147\n",
            "repr, std, cov, clossl, z, norm 0.017428481951355934 0.403076171875 0.021980971097946167 0.15080226957798004 0.07244674116373062 3.86328125\n",
            "repr, std, cov, clossl, z, norm 0.01790350303053856 0.40625 0.020276430994272232 0.13697871565818787 0.04651697352528572 3.74609375\n",
            "repr, std, cov, clossl, z, norm 0.017711974680423737 0.40966796875 0.017065292224287987 0.12461533397436142 0.07132227718830109 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.01601279526948929 0.41064453125 0.016832012683153152 0.16724860668182373 0.025797871872782707 4.4140625\n",
            "repr, std, cov, clossl, z, norm 0.01516727451235056 0.40869140625 0.02047431841492653 0.08993156999349594 0.0846967026591301 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.013608632609248161 0.406494140625 0.022290542721748352 0.09831433743238449 0.11476162075996399 3.62890625\n",
            "repr, std, cov, clossl, z, norm 0.014776069670915604 0.40283203125 0.023746419697999954 0.17314283549785614 0.0484066903591156 2.892578125\n",
            "repr, std, cov, clossl, z, norm 0.013435597531497478 0.404296875 0.023918025195598602 0.12325530499219894 0.014837612397968769 3.34765625\n",
            "148\n",
            "repr, std, cov, clossl, z, norm 0.01445208303630352 0.404296875 0.024571357294917107 0.11131945252418518 0.04383424296975136 3.505859375\n",
            "repr, std, cov, clossl, z, norm 0.014232181012630463 0.40283203125 0.02376820333302021 0.1886550486087799 0.1578647792339325 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.01433560810983181 0.40283203125 0.02357059344649315 0.16123363375663757 0.10149410367012024 3.4765625\n",
            "repr, std, cov, clossl, z, norm 0.01445890124887228 0.40087890625 0.029467634856700897 0.12118668109178543 0.06032250076532364 4.703125\n",
            "repr, std, cov, clossl, z, norm 0.014071458019316196 0.40283203125 0.02160525694489479 0.11333652585744858 0.1276399940252304 3.0859375\n",
            "repr, std, cov, clossl, z, norm 0.014666520059108734 0.40380859375 0.021499330177903175 0.22315427660942078 0.13867589831352234 2.810546875\n",
            "repr, std, cov, clossl, z, norm 0.014130665920674801 0.403564453125 0.02330075018107891 0.10510536283254623 0.2549324035644531 1.921875\n",
            "repr, std, cov, clossl, z, norm 0.014780829660594463 0.39892578125 0.028713414445519447 0.16229891777038574 0.13758988678455353 2.736328125\n",
            "149\n",
            "repr, std, cov, clossl, z, norm 0.015211417339742184 0.395751953125 0.030889127403497696 0.13911062479019165 0.06221553683280945 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.014131827279925346 0.399169921875 0.025587573647499084 0.11911826580762863 0.1402379423379898 3.5859375\n",
            "repr, std, cov, clossl, z, norm 0.014049939811229706 0.401123046875 0.02352156490087509 0.11218305677175522 0.13074983656406403 3.98046875\n",
            "repr, std, cov, clossl, z, norm 0.014415951445698738 0.4033203125 0.02133132889866829 0.14584258198738098 0.021329812705516815 3.083984375\n",
            "repr, std, cov, clossl, z, norm 0.014074926264584064 0.40283203125 0.02159028872847557 0.13331441581249237 0.11157433688640594 3.76171875\n",
            "repr, std, cov, clossl, z, norm 0.014056846499443054 0.406494140625 0.01895013451576233 0.22329844534397125 0.043365590274333954 5.171875\n",
            "repr, std, cov, clossl, z, norm 0.015027165412902832 0.39794921875 0.025914084166288376 0.14638032019138336 0.13523390889167786 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.014642495661973953 0.394287109375 0.02895011194050312 0.12353360652923584 0.08244959264993668 3.658203125\n",
            "150\n",
            "repr, std, cov, clossl, z, norm 0.01409818697720766 0.396240234375 0.027011187747120857 0.1460496336221695 0.08146464079618454 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.013914618641138077 0.398681640625 0.024861011654138565 0.10379895567893982 0.10326577723026276 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.015464860945940018 0.400390625 0.025910494849085808 0.1225285679101944 0.15699239075183868 1.623046875\n",
            "repr, std, cov, clossl, z, norm 0.014064832590520382 0.404052734375 0.02074694260954857 0.1152578815817833 0.015701012685894966 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.013555577956140041 0.409423828125 0.016316117718815804 0.15424391627311707 0.09294255822896957 3.486328125\n",
            "repr, std, cov, clossl, z, norm 0.014808154664933681 0.3984375 0.02591358870267868 0.18558360636234283 0.03190162032842636 3.384765625\n",
            "repr, std, cov, clossl, z, norm 0.014629075303673744 0.39453125 0.03050987422466278 0.14169521629810333 0.16015131771564484 1.62109375\n",
            "repr, std, cov, clossl, z, norm 0.015634870156645775 0.393310546875 0.032102156430482864 0.15408214926719666 0.08907003700733185 2.8671875\n",
            "151\n",
            "repr, std, cov, clossl, z, norm 0.014943897724151611 0.395751953125 0.027760468423366547 0.10922577977180481 0.09822138398885727 3.5\n",
            "repr, std, cov, clossl, z, norm 0.015357806347310543 0.399658203125 0.02267536148428917 0.11606596410274506 0.03388098254799843 2.013671875\n",
            "repr, std, cov, clossl, z, norm 0.01573297008872032 0.39990234375 0.02391737326979637 0.09939919412136078 0.06474490463733673 3.083984375\n",
            "repr, std, cov, clossl, z, norm 0.0162876658141613 0.404296875 0.02182881161570549 0.15960939228534698 0.11015793681144714 1.7041015625\n",
            "repr, std, cov, clossl, z, norm 0.014693157747387886 0.404541015625 0.0221561249345541 0.06908217817544937 0.055497583001852036 3.720703125\n",
            "repr, std, cov, clossl, z, norm 0.015637291595339775 0.403564453125 0.020519785583019257 0.17699725925922394 0.034969910979270935 3.408203125\n",
            "repr, std, cov, clossl, z, norm 0.01585857942700386 0.397216796875 0.027082882821559906 0.1127011850476265 0.07650652527809143 3.283203125\n",
            "repr, std, cov, clossl, z, norm 0.015834858641028404 0.395751953125 0.02671860344707966 0.1171112060546875 0.05768163129687309 2.919921875\n",
            "152\n",
            "repr, std, cov, clossl, z, norm 0.016331883147358894 0.395263671875 0.027377964928746223 0.12698933482170105 0.2587253749370575 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.01672728732228279 0.39501953125 0.02779485285282135 0.11978061497211456 0.046477172523736954 1.6455078125\n",
            "repr, std, cov, clossl, z, norm 0.015431198291480541 0.39892578125 0.025370707735419273 0.13741497695446014 0.03354263678193092 4.76953125\n",
            "repr, std, cov, clossl, z, norm 0.014875924214720726 0.406005859375 0.018204279243946075 0.08999429643154144 0.16452789306640625 3.30078125\n",
            "repr, std, cov, clossl, z, norm 0.014069123193621635 0.40869140625 0.01739616133272648 0.1453644335269928 0.20655035972595215 3.548828125\n",
            "repr, std, cov, clossl, z, norm 0.014884511940181255 0.402587890625 0.021133949980139732 0.11526408791542053 0.1781633496284485 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.014128143899142742 0.3984375 0.024169888347387314 0.09655071049928665 0.04961289465427399 3.322265625\n",
            "repr, std, cov, clossl, z, norm 0.014833354391157627 0.39404296875 0.028941601514816284 0.13421986997127533 0.17727431654930115 2.998046875\n",
            "153\n",
            "repr, std, cov, clossl, z, norm 0.015011806041002274 0.39111328125 0.032078519463539124 0.10166383534669876 0.03358152508735657 2.369140625\n",
            "repr, std, cov, clossl, z, norm 0.01589200273156166 0.391357421875 0.03237023204565048 0.14243565499782562 0.1206018403172493 3.166015625\n",
            "repr, std, cov, clossl, z, norm 0.01449642889201641 0.39990234375 0.02222016453742981 0.11330066621303558 0.0607706718146801 3.29296875\n",
            "repr, std, cov, clossl, z, norm 0.01402883417904377 0.3994140625 0.021679453551769257 0.09611507505178452 0.09413094073534012 3.833984375\n",
            "repr, std, cov, clossl, z, norm 0.013944490812718868 0.400634765625 0.021532412618398666 0.10887628048658371 0.0275314562022686 3.244140625\n",
            "repr, std, cov, clossl, z, norm 0.013539612293243408 0.4013671875 0.021771114319562912 0.0754295140504837 0.12766893208026886 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.01421483512967825 0.403564453125 0.021534062922000885 0.1377798467874527 0.024526342749595642 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.012829696759581566 0.403564453125 0.020599516108632088 0.07860057055950165 0.04568066447973251 4.2578125\n",
            "154\n",
            "repr, std, cov, clossl, z, norm 0.015067323110997677 0.393798828125 0.028377138078212738 0.09962043911218643 0.10291415452957153 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.013980193063616753 0.39453125 0.029385153204202652 0.08529365062713623 0.05216720327734947 3.13671875\n",
            "repr, std, cov, clossl, z, norm 0.015934603288769722 0.389404296875 0.03478146344423294 0.11269184947013855 0.03519434109330177 3.912109375\n",
            "repr, std, cov, clossl, z, norm 0.016326632350683212 0.391357421875 0.030463537201285362 0.10550388693809509 0.08965671807527542 3.634765625\n",
            "repr, std, cov, clossl, z, norm 0.016081349924206734 0.40087890625 0.023302599787712097 0.12502452731132507 0.12660260498523712 4.86328125\n",
            "repr, std, cov, clossl, z, norm 0.014823969453573227 0.40380859375 0.021030772477388382 0.13538797199726105 0.10994582623243332 3.3046875\n",
            "repr, std, cov, clossl, z, norm 0.01575179025530815 0.404541015625 0.018961282446980476 0.10696984827518463 0.1123952567577362 3.49609375\n",
            "repr, std, cov, clossl, z, norm 0.01446235179901123 0.401123046875 0.023287901654839516 0.07329030334949493 0.1021038219332695 2.751953125\n",
            "155\n",
            "repr, std, cov, clossl, z, norm 0.013636444695293903 0.403564453125 0.02028287760913372 0.07663498818874359 0.1152695044875145 3.13671875\n",
            "repr, std, cov, clossl, z, norm 0.01326796505600214 0.402587890625 0.021266506984829903 0.08182662725448608 0.12084745615720749 4.703125\n",
            "repr, std, cov, clossl, z, norm 0.013629617169499397 0.39892578125 0.024225877597928047 0.11848539113998413 0.17801223695278168 3.302734375\n",
            "repr, std, cov, clossl, z, norm 0.013107390142977238 0.39453125 0.0293889157474041 0.08144665509462357 0.06380753219127655 2.587890625\n",
            "repr, std, cov, clossl, z, norm 0.012350927107036114 0.396240234375 0.026935528963804245 0.09271936118602753 0.11116097122430801 3.69921875\n",
            "repr, std, cov, clossl, z, norm 0.012806136161088943 0.400634765625 0.022693946957588196 0.09948507696390152 0.03826354816555977 3.65625\n",
            "repr, std, cov, clossl, z, norm 0.012639999389648438 0.40283203125 0.019540786743164062 0.13967680931091309 0.10595058649778366 3.220703125\n",
            "repr, std, cov, clossl, z, norm 0.012220116332173347 0.403564453125 0.01964661478996277 0.07045500725507736 0.008543982170522213 3.423828125\n",
            "156\n",
            "repr, std, cov, clossl, z, norm 0.011757416650652885 0.40185546875 0.02247649058699608 0.05696044862270355 0.0658823698759079 3.607421875\n",
            "repr, std, cov, clossl, z, norm 0.01412697322666645 0.396728515625 0.028458744287490845 0.09511393308639526 0.1779477298259735 3.8046875\n",
            "repr, std, cov, clossl, z, norm 0.013094071298837662 0.40234375 0.020763833075761795 0.0724242553114891 0.03627622127532959 3.240234375\n",
            "repr, std, cov, clossl, z, norm 0.01336501445621252 0.40576171875 0.018049150705337524 0.10244590789079666 0.01596798747777939 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.014525997452437878 0.400146484375 0.023445207625627518 0.11905105412006378 0.05747341364622116 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.01329890452325344 0.3994140625 0.023874465376138687 0.08751058578491211 0.042767398059368134 3.716796875\n",
            "repr, std, cov, clossl, z, norm 0.013787150382995605 0.39599609375 0.026755081489682198 0.05992959067225456 0.18181096017360687 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.01320617739111185 0.396240234375 0.025740955024957657 0.07692594826221466 0.12161541730165482 3.1953125\n",
            "157\n",
            "repr, std, cov, clossl, z, norm 0.013514194637537003 0.393798828125 0.0301675908267498 0.0737636610865593 0.12215358018875122 4.3984375\n",
            "repr, std, cov, clossl, z, norm 0.01340486854314804 0.398193359375 0.0238117016851902 0.0797436386346817 0.04947618022561073 4.5234375\n",
            "repr, std, cov, clossl, z, norm 0.013600785285234451 0.4013671875 0.02155047282576561 0.06527117639780045 0.1359156221151352 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.01402849517762661 0.39990234375 0.022260837256908417 0.07707204669713974 0.04239053651690483 3.201171875\n",
            "repr, std, cov, clossl, z, norm 0.013341948390007019 0.399658203125 0.024162396788597107 0.0640096440911293 0.1259964555501938 2.873046875\n",
            "repr, std, cov, clossl, z, norm 0.01371812541037798 0.402099609375 0.020928485319018364 0.08702640980482101 0.10343819856643677 3.720703125\n",
            "repr, std, cov, clossl, z, norm 0.013210820965468884 0.40283203125 0.01964186504483223 0.07632297277450562 0.1503010094165802 4.0\n",
            "repr, std, cov, clossl, z, norm 0.01370942685753107 0.397216796875 0.02434798702597618 0.0791844055056572 0.16928142309188843 3.935546875\n",
            "158\n",
            "repr, std, cov, clossl, z, norm 0.013633127324283123 0.398193359375 0.023097768425941467 0.07686730474233627 0.018937265500426292 3.70703125\n",
            "repr, std, cov, clossl, z, norm 0.01269406545907259 0.3955078125 0.026171723380684853 0.06173451617360115 0.06302568316459656 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.012908561155200005 0.3896484375 0.03344782441854477 0.06779352575540543 0.04008873179554939 3.744140625\n",
            "repr, std, cov, clossl, z, norm 0.012568810023367405 0.39208984375 0.03010876290500164 0.07153493911027908 0.06168024614453316 1.447265625\n",
            "repr, std, cov, clossl, z, norm 0.01441987045109272 0.390380859375 0.031448863446712494 0.07044201344251633 0.07317332178354263 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.012106039561331272 0.398193359375 0.022744406014680862 0.057267311960458755 0.08185284584760666 2.59765625\n",
            "repr, std, cov, clossl, z, norm 0.012400996871292591 0.39599609375 0.025235552340745926 0.054789185523986816 0.0836312472820282 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.013360219076275826 0.39892578125 0.02354922890663147 0.10016952455043793 0.15218961238861084 3.412109375\n",
            "159\n",
            "repr, std, cov, clossl, z, norm 0.012257339432835579 0.401123046875 0.020850487053394318 0.0506717674434185 0.13757343590259552 3.998046875\n",
            "repr, std, cov, clossl, z, norm 0.012670214287936687 0.39990234375 0.02172575518488884 0.07429911196231842 0.09293977171182632 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.012884399853646755 0.399658203125 0.02160012349486351 0.07107417285442352 0.007665522862225771 5.04296875\n",
            "repr, std, cov, clossl, z, norm 0.012156670913100243 0.400146484375 0.021505922079086304 0.049171384423971176 0.0021628469694405794 4.45703125\n",
            "repr, std, cov, clossl, z, norm 0.01326640136539936 0.395751953125 0.025131570175290108 0.08011243492364883 0.06728485226631165 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.013027269393205643 0.395751953125 0.02414023131132126 0.08189776539802551 0.0960974469780922 3.958984375\n",
            "repr, std, cov, clossl, z, norm 0.013775022700428963 0.389892578125 0.03046083077788353 0.07546016573905945 0.11632147431373596 3.17578125\n",
            "repr, std, cov, clossl, z, norm 0.01258815173059702 0.392578125 0.02726115845143795 0.06994201242923737 0.11410348862409592 2.87109375\n",
            "160\n",
            "repr, std, cov, clossl, z, norm 0.012690284289419651 0.3935546875 0.0265523549169302 0.055997028946876526 0.08766337484121323 3.759765625\n",
            "repr, std, cov, clossl, z, norm 0.01297926064580679 0.39306640625 0.02791224606335163 0.06690776348114014 0.08724190294742584 3.234375\n",
            "repr, std, cov, clossl, z, norm 0.013047863729298115 0.395751953125 0.02425963431596756 0.07726313918828964 0.14116939902305603 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.012393700890243053 0.402587890625 0.01977430284023285 0.05923068895936012 0.07733999937772751 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.012287173420190811 0.401123046875 0.020280256867408752 0.04521559551358223 0.12078006565570831 3.59765625\n",
            "repr, std, cov, clossl, z, norm 0.012730399146676064 0.396728515625 0.024368014186620712 0.05312695354223251 0.0863719955086708 3.8203125\n",
            "repr, std, cov, clossl, z, norm 0.01292284857481718 0.399169921875 0.02240877039730549 0.05353212356567383 0.08764872699975967 4.71875\n",
            "repr, std, cov, clossl, z, norm 0.013031565584242344 0.396240234375 0.02666066400706768 0.07898178696632385 0.08714661747217178 3.974609375\n",
            "161\n",
            "repr, std, cov, clossl, z, norm 0.012905854731798172 0.397216796875 0.024066966027021408 0.06083520129323006 0.1522800475358963 3.865234375\n",
            "repr, std, cov, clossl, z, norm 0.012101476080715656 0.39501953125 0.025146394968032837 0.04602129012346268 0.06328888237476349 4.0703125\n",
            "repr, std, cov, clossl, z, norm 0.013710619881749153 0.390380859375 0.029791180044412613 0.06358921527862549 0.04770372435450554 3.990234375\n",
            "repr, std, cov, clossl, z, norm 0.01231091096997261 0.39453125 0.025868119671940804 0.05831770598888397 0.29953935742378235 4.0234375\n",
            "repr, std, cov, clossl, z, norm 0.01379302702844143 0.388671875 0.03188396990299225 0.06737368553876877 0.19405080378055573 4.6484375\n",
            "repr, std, cov, clossl, z, norm 0.011475483886897564 0.392578125 0.02741917222738266 0.04677191749215126 0.06135915219783783 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.012250784784555435 0.393798828125 0.026337265968322754 0.0740007609128952 0.14926216006278992 3.736328125\n",
            "repr, std, cov, clossl, z, norm 0.011645598337054253 0.396240234375 0.02327774465084076 0.05957715958356857 0.039295949041843414 3.49609375\n",
            "162\n",
            "repr, std, cov, clossl, z, norm 0.011858060956001282 0.39501953125 0.024268455803394318 0.04390571638941765 0.027159729972481728 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.011494221165776253 0.39599609375 0.02430732361972332 0.05436858907341957 0.10904104262590408 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.012098543345928192 0.395263671875 0.025248290970921516 0.059738654643297195 0.1338951140642166 3.423828125\n",
            "repr, std, cov, clossl, z, norm 0.010688419453799725 0.396728515625 0.024341803044080734 0.04764869436621666 0.1117430105805397 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.012009664438664913 0.39404296875 0.026048079133033752 0.05849089473485947 0.052341751754283905 5.12109375\n",
            "repr, std, cov, clossl, z, norm 0.011950889602303505 0.396240234375 0.025466497987508774 0.055254243314266205 0.07939214259386063 2.763671875\n",
            "repr, std, cov, clossl, z, norm 0.011837453581392765 0.39697265625 0.023506972938776016 0.05588677152991295 0.052868571132421494 3.98828125\n",
            "repr, std, cov, clossl, z, norm 0.011852486059069633 0.401123046875 0.019692640751600266 0.04869856685400009 0.12923425436019897 3.123046875\n",
            "163\n",
            "repr, std, cov, clossl, z, norm 0.0116188395768404 0.399169921875 0.02143634855747223 0.041583094745874405 0.11392349749803543 3.845703125\n",
            "repr, std, cov, clossl, z, norm 0.011948012746870518 0.39599609375 0.02346890978515148 0.04741368070244789 0.09038183838129044 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.011740034446120262 0.400146484375 0.020485330373048782 0.04625165835022926 0.13006162643432617 3.71484375\n",
            "repr, std, cov, clossl, z, norm 0.012137901969254017 0.397216796875 0.02245805412530899 0.046188514679670334 0.03798894211649895 3.482421875\n",
            "repr, std, cov, clossl, z, norm 0.012071763165295124 0.3955078125 0.0246351957321167 0.0412774495780468 0.02910740114748478 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.0117836594581604 0.395751953125 0.025719251483678818 0.05867968872189522 0.03318529948592186 3.828125\n",
            "repr, std, cov, clossl, z, norm 0.012611731886863708 0.386474609375 0.03463398665189743 0.040258850902318954 0.11822136491537094 3.923828125\n",
            "repr, std, cov, clossl, z, norm 0.013239726424217224 0.3876953125 0.03248614817857742 0.05912274122238159 0.14124764502048492 3.974609375\n",
            "164\n",
            "repr, std, cov, clossl, z, norm 0.012494441121816635 0.39306640625 0.028651049360632896 0.03005358576774597 0.07675638049840927 4.8671875\n",
            "repr, std, cov, clossl, z, norm 0.011988933198153973 0.39404296875 0.025889862328767776 0.04309745877981186 0.010366353206336498 2.548828125\n",
            "repr, std, cov, clossl, z, norm 0.014005124568939209 0.389404296875 0.03141922503709793 0.06004054471850395 0.19267617166042328 4.08984375\n",
            "repr, std, cov, clossl, z, norm 0.011732129380106926 0.39599609375 0.023267073556780815 0.04602654650807381 0.16087770462036133 3.783203125\n",
            "repr, std, cov, clossl, z, norm 0.012388009577989578 0.39404296875 0.025525197386741638 0.04654212296009064 0.18384774029254913 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.012667116709053516 0.398193359375 0.022519126534461975 0.057943351566791534 0.10520995408296585 3.7578125\n",
            "repr, std, cov, clossl, z, norm 0.01192453596740961 0.40234375 0.01969144493341446 0.03190042823553085 0.1951601356267929 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.01337255910038948 0.39990234375 0.021881522610783577 0.06878774613142014 0.0539851151406765 3.609375\n",
            "165\n",
            "repr, std, cov, clossl, z, norm 0.01170374359935522 0.401123046875 0.019665781408548355 0.047032661736011505 0.1631993055343628 4.25390625\n",
            "repr, std, cov, clossl, z, norm 0.012846509926021099 0.390869140625 0.029580332338809967 0.05342049524188042 0.15489816665649414 4.6015625\n",
            "repr, std, cov, clossl, z, norm 0.01330454833805561 0.395751953125 0.02412615716457367 0.04961007833480835 0.05460970103740692 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.011280369944870472 0.398193359375 0.021974800154566765 0.05243673548102379 0.04426395520567894 3.384765625\n",
            "repr, std, cov, clossl, z, norm 0.012560918927192688 0.3935546875 0.025506556034088135 0.051712825894355774 0.09667699784040451 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.012153216637670994 0.396484375 0.023251065984368324 0.046137768775224686 0.026652874425053596 2.326171875\n",
            "repr, std, cov, clossl, z, norm 0.01189604215323925 0.391845703125 0.027761075645685196 0.04575969651341438 0.05959491431713104 3.56640625\n",
            "repr, std, cov, clossl, z, norm 0.012441219761967659 0.39404296875 0.02497253566980362 0.05600563809275627 0.1942611187696457 3.0625\n",
            "166\n",
            "repr, std, cov, clossl, z, norm 0.012071354314684868 0.392578125 0.026211224496364594 0.05048401653766632 0.11494384706020355 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.012621240690350533 0.388427734375 0.031041573733091354 0.04844658449292183 0.047140177339315414 3.828125\n",
            "repr, std, cov, clossl, z, norm 0.0119027616456151 0.393310546875 0.02662476897239685 0.04315720871090889 0.07289208471775055 3.560546875\n",
            "repr, std, cov, clossl, z, norm 0.010938775725662708 0.393310546875 0.02665945142507553 0.038546644151210785 0.06553877890110016 2.6328125\n",
            "repr, std, cov, clossl, z, norm 0.010976401157677174 0.393798828125 0.025618508458137512 0.05145614594221115 0.021815147250890732 1.818359375\n",
            "repr, std, cov, clossl, z, norm 0.011528692208230495 0.391357421875 0.02743803709745407 0.04385081306099892 0.11024682223796844 3.900390625\n",
            "repr, std, cov, clossl, z, norm 0.011580929160118103 0.39404296875 0.025454355403780937 0.055489763617515564 0.10310433059930801 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.011317728087306023 0.393798828125 0.02714977227151394 0.04364028200507164 0.19388575851917267 3.880859375\n",
            "167\n",
            "repr, std, cov, clossl, z, norm 0.011836739256978035 0.39208984375 0.027225973084568977 0.048727039247751236 0.18303154408931732 1.9462890625\n",
            "repr, std, cov, clossl, z, norm 0.011409983038902283 0.391845703125 0.02721956931054592 0.053079985082149506 0.12308898568153381 3.1640625\n",
            "repr, std, cov, clossl, z, norm 0.012301936745643616 0.390869140625 0.02753574401140213 0.04472995176911354 0.04944033920764923 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.011148099787533283 0.39404296875 0.024943020194768906 0.0425867885351181 0.2032717764377594 3.697265625\n",
            "repr, std, cov, clossl, z, norm 0.011789561249315739 0.3916015625 0.027866866439580917 0.05009712278842926 0.0720132365822792 3.72265625\n",
            "repr, std, cov, clossl, z, norm 0.011237656697630882 0.39453125 0.024612948298454285 0.044889211654663086 0.26283103227615356 3.611328125\n",
            "repr, std, cov, clossl, z, norm 0.011430809274315834 0.393310546875 0.026247164234519005 0.04354988783597946 0.23357458412647247 3.96875\n",
            "repr, std, cov, clossl, z, norm 0.01139846257865429 0.39501953125 0.024396922439336777 0.05590816214680672 0.05009521543979645 3.53125\n",
            "168\n",
            "repr, std, cov, clossl, z, norm 0.010885149240493774 0.3984375 0.0213101115077734 0.04745301976799965 0.14395421743392944 3.87109375\n",
            "repr, std, cov, clossl, z, norm 0.012186539359390736 0.39501953125 0.024118389934301376 0.04564594104886055 0.11737821251153946 3.87109375\n",
            "repr, std, cov, clossl, z, norm 0.011942805722355843 0.391357421875 0.028621867299079895 0.04115023836493492 0.04076668992638588 3.826171875\n",
            "repr, std, cov, clossl, z, norm 0.012022927403450012 0.3935546875 0.025542259216308594 0.04790802672505379 0.1111687496304512 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.0120012778788805 0.397705078125 0.022060507908463478 0.0497591570019722 0.04425793141126633 3.10546875\n",
            "repr, std, cov, clossl, z, norm 0.011136707849800587 0.392333984375 0.027137426659464836 0.03415552154183388 0.09681921452283859 3.435546875\n",
            "repr, std, cov, clossl, z, norm 0.012174845673143864 0.392822265625 0.026627792045474052 0.0528593435883522 0.13575658202171326 4.46484375\n",
            "repr, std, cov, clossl, z, norm 0.010719051584601402 0.395751953125 0.024170368909835815 0.03704346716403961 0.17213451862335205 3.83203125\n",
            "169\n",
            "repr, std, cov, clossl, z, norm 0.01169116236269474 0.396484375 0.023589443415403366 0.04860159754753113 0.24212604761123657 3.341796875\n",
            "repr, std, cov, clossl, z, norm 0.010993840172886848 0.39892578125 0.02156768925487995 0.033386170864105225 0.22180381417274475 3.6171875\n",
            "repr, std, cov, clossl, z, norm 0.011944149620831013 0.392333984375 0.025965631008148193 0.027779635041952133 0.08364308625459671 3.48046875\n",
            "repr, std, cov, clossl, z, norm 0.012679005973041058 0.392578125 0.027032174170017242 0.04369790107011795 0.19938607513904572 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.011816400103271008 0.392333984375 0.02693760022521019 0.048898160457611084 0.1669144183397293 2.869140625\n",
            "repr, std, cov, clossl, z, norm 0.011210999451577663 0.392578125 0.027202025055885315 0.03961563482880592 0.06594306230545044 2.919921875\n",
            "repr, std, cov, clossl, z, norm 0.01190239004790783 0.390625 0.02919991873204708 0.04370037838816643 0.11731559783220291 3.35546875\n",
            "repr, std, cov, clossl, z, norm 0.011236028745770454 0.39404296875 0.025198958814144135 0.03707801178097725 0.03297816216945648 3.787109375\n",
            "170\n",
            "repr, std, cov, clossl, z, norm 0.011915845796465874 0.389892578125 0.02875163033604622 0.03342972323298454 0.14416339993476868 3.2421875\n",
            "repr, std, cov, clossl, z, norm 0.011706165969371796 0.392578125 0.026579219847917557 0.03844264894723892 0.20365720987319946 3.59375\n",
            "repr, std, cov, clossl, z, norm 0.011078822426497936 0.395751953125 0.023131482303142548 0.030611447989940643 0.06277938187122345 3.896484375\n",
            "repr, std, cov, clossl, z, norm 0.011852010153234005 0.392578125 0.026107672601938248 0.05145659297704697 0.21788127720355988 4.03515625\n",
            "repr, std, cov, clossl, z, norm 0.01152881607413292 0.398681640625 0.022008977830410004 0.048940468579530716 0.15690754354000092 3.515625\n",
            "repr, std, cov, clossl, z, norm 0.011747943237423897 0.3955078125 0.02378728985786438 0.05035354942083359 0.07316514849662781 3.068359375\n",
            "repr, std, cov, clossl, z, norm 0.012218608520925045 0.39208984375 0.02703975699841976 0.04622875154018402 0.09005740284919739 3.71484375\n",
            "repr, std, cov, clossl, z, norm 0.011917761527001858 0.390380859375 0.029093988239765167 0.044785384088754654 0.11681133508682251 3.365234375\n",
            "171\n",
            "repr, std, cov, clossl, z, norm 0.012317677028477192 0.38818359375 0.03136882558465004 0.043029192835092545 0.17134664952754974 3.71875\n",
            "repr, std, cov, clossl, z, norm 0.0124124800786376 0.387939453125 0.03122248686850071 0.04837346822023392 0.20584982633590698 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.012324824929237366 0.386962890625 0.03374088928103447 0.032120928168296814 0.16085664927959442 3.443359375\n",
            "repr, std, cov, clossl, z, norm 0.013161910697817802 0.38671875 0.033080972731113434 0.047995831817388535 0.08570150285959244 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.01215463038533926 0.392333984375 0.027340799570083618 0.06958004087209702 0.051549751311540604 4.125\n",
            "repr, std, cov, clossl, z, norm 0.013236411847174168 0.392578125 0.026953797787427902 0.06394718587398529 0.14088691771030426 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.012571660801768303 0.39599609375 0.02435380220413208 0.0709306001663208 0.07555858790874481 3.7578125\n",
            "repr, std, cov, clossl, z, norm 0.012172549962997437 0.39892578125 0.021267516538500786 0.06646545976400375 0.1029830053448677 3.13671875\n",
            "172\n",
            "repr, std, cov, clossl, z, norm 0.012301156297326088 0.396484375 0.02224666438996792 0.06276346743106842 0.03222694620490074 3.810546875\n",
            "repr, std, cov, clossl, z, norm 0.011436283588409424 0.398193359375 0.021748583763837814 0.04705153405666351 0.0885215550661087 3.51171875\n",
            "repr, std, cov, clossl, z, norm 0.012948650866746902 0.39013671875 0.028996288776397705 0.07303217053413391 0.06402969360351562 3.78515625\n",
            "repr, std, cov, clossl, z, norm 0.012750367633998394 0.39404296875 0.025142554193735123 0.06680314242839813 0.17345938086509705 4.5\n",
            "repr, std, cov, clossl, z, norm 0.012673706747591496 0.397705078125 0.02284940332174301 0.059485774487257004 0.043543290346860886 3.806640625\n",
            "repr, std, cov, clossl, z, norm 0.012264223769307137 0.39990234375 0.02094573900103569 0.05800727754831314 0.19084317982196808 3.408203125\n",
            "repr, std, cov, clossl, z, norm 0.012082165107131004 0.400390625 0.019631003960967064 0.05149632692337036 0.1476818174123764 3.41796875\n",
            "repr, std, cov, clossl, z, norm 0.012917010113596916 0.3984375 0.02156524918973446 0.06855383515357971 0.0438547283411026 3.837890625\n",
            "173\n",
            "repr, std, cov, clossl, z, norm 0.013671403750777245 0.39599609375 0.023050863295793533 0.0759505033493042 0.09460899233818054 3.830078125\n",
            "repr, std, cov, clossl, z, norm 0.012719505466520786 0.395263671875 0.024114077910780907 0.03563946485519409 0.003659644862636924 3.939453125\n",
            "repr, std, cov, clossl, z, norm 0.012996644712984562 0.3916015625 0.027415070682764053 0.06382942199707031 0.05164948105812073 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.012960987165570259 0.3916015625 0.0269845649600029 0.04275261610746384 0.11456011980772018 3.80859375\n",
            "repr, std, cov, clossl, z, norm 0.013358418829739094 0.390869140625 0.02732672728598118 0.05490415543317795 0.08644244074821472 5.30078125\n",
            "repr, std, cov, clossl, z, norm 0.013946551829576492 0.39111328125 0.02786847949028015 0.06126323714852333 0.1363280713558197 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.013253912329673767 0.389404296875 0.028643911704421043 0.050294943153858185 0.12336315214633942 4.23828125\n",
            "repr, std, cov, clossl, z, norm 0.013314180076122284 0.38916015625 0.030939051881432533 0.0532330721616745 0.15395310521125793 2.560546875\n",
            "174\n",
            "repr, std, cov, clossl, z, norm 0.013126357458531857 0.38720703125 0.030662648379802704 0.041074126958847046 0.04506978392601013 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.013392698019742966 0.39306640625 0.024928998202085495 0.04365381598472595 0.14657534658908844 3.865234375\n",
            "repr, std, cov, clossl, z, norm 0.01206001453101635 0.394775390625 0.023714952170848846 0.0475943461060524 0.13470172882080078 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.013026759028434753 0.388916015625 0.029380477964878082 0.05297921597957611 0.14237545430660248 4.15234375\n",
            "repr, std, cov, clossl, z, norm 0.011711412109434605 0.39404296875 0.024773642420768738 0.04524476081132889 0.15485632419586182 3.177734375\n",
            "repr, std, cov, clossl, z, norm 0.012000171467661858 0.398193359375 0.021099455654621124 0.045862358063459396 0.10173467546701431 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.011586853303015232 0.393798828125 0.024496199563145638 0.05156276002526283 0.06630325317382812 3.587890625\n",
            "repr, std, cov, clossl, z, norm 0.011661441996693611 0.394287109375 0.024379288777709007 0.05297805368900299 0.0404895655810833 4.078125\n",
            "175\n",
            "repr, std, cov, clossl, z, norm 0.011926252394914627 0.391357421875 0.0267421156167984 0.04780300334095955 0.06566989421844482 3.716796875\n",
            "repr, std, cov, clossl, z, norm 0.011673066765069962 0.3916015625 0.026598194614052773 0.062330007553100586 0.2208137959241867 3.59375\n",
            "repr, std, cov, clossl, z, norm 0.010805714875459671 0.393310546875 0.024577351287007332 0.04185640066862106 0.01879153959453106 3.734375\n",
            "repr, std, cov, clossl, z, norm 0.012059877626597881 0.389892578125 0.028459975495934486 0.05603452026844025 0.13116984069347382 3.236328125\n",
            "repr, std, cov, clossl, z, norm 0.011604390107095242 0.393310546875 0.025143273174762726 0.048337116837501526 0.07795129716396332 3.4296875\n",
            "repr, std, cov, clossl, z, norm 0.011194740422070026 0.39453125 0.02450328692793846 0.03727559745311737 0.02742944285273552 3.263671875\n",
            "repr, std, cov, clossl, z, norm 0.00999676063656807 0.397216796875 0.02235645055770874 0.03863387554883957 0.11336825042963028 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.010645127855241299 0.39306640625 0.026364751160144806 0.05055810883641243 0.07494673132896423 3.736328125\n",
            "176\n",
            "repr, std, cov, clossl, z, norm 0.01173009630292654 0.392333984375 0.025779854506254196 0.05602700263261795 0.012998825870454311 3.939453125\n",
            "repr, std, cov, clossl, z, norm 0.010724463500082493 0.396728515625 0.022010423243045807 0.03034573420882225 0.15437662601470947 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.010932602919638157 0.3916015625 0.02725600078701973 0.04091046378016472 0.1036156490445137 3.50390625\n",
            "repr, std, cov, clossl, z, norm 0.010729565285146236 0.390380859375 0.02850593812763691 0.035103507339954376 0.06997290253639221 3.099609375\n",
            "repr, std, cov, clossl, z, norm 0.011513279750943184 0.392333984375 0.025547582656145096 0.03392905741930008 0.10801144689321518 4.15234375\n",
            "repr, std, cov, clossl, z, norm 0.010740445926785469 0.39404296875 0.025617273524403572 0.027369551360607147 0.07357122004032135 3.947265625\n",
            "repr, std, cov, clossl, z, norm 0.011498946696519852 0.39306640625 0.02575603686273098 0.04905719682574272 0.11258681863546371 4.359375\n",
            "repr, std, cov, clossl, z, norm 0.012446204200387001 0.388671875 0.02941928431391716 0.04848584160208702 0.023279577493667603 3.9921875\n",
            "177\n",
            "repr, std, cov, clossl, z, norm 0.011521818116307259 0.3916015625 0.027207499369978905 0.04204919561743736 0.07119154930114746 3.654296875\n",
            "repr, std, cov, clossl, z, norm 0.01336517184972763 0.387939453125 0.030258934944868088 0.03680502623319626 0.11085870862007141 3.693359375\n",
            "repr, std, cov, clossl, z, norm 0.01231177058070898 0.388916015625 0.030285079032182693 0.03279038518667221 0.21282276511192322 2.78125\n",
            "repr, std, cov, clossl, z, norm 0.011691847816109657 0.3935546875 0.024164024740457535 0.033984530717134476 0.08613193035125732 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.012165416963398457 0.39208984375 0.025401681661605835 0.05128355696797371 0.0750967338681221 3.935546875\n",
            "repr, std, cov, clossl, z, norm 0.012237527407705784 0.388916015625 0.028950724750757217 0.047763608396053314 0.2767324149608612 3.921875\n",
            "repr, std, cov, clossl, z, norm 0.011864271014928818 0.397216796875 0.021343793720006943 0.03537522256374359 0.09589112550020218 3.859375\n",
            "repr, std, cov, clossl, z, norm 0.01146574504673481 0.395751953125 0.023474184796214104 0.04537089169025421 0.04655146226286888 4.3671875\n",
            "178\n",
            "repr, std, cov, clossl, z, norm 0.011334611102938652 0.39453125 0.024410158395767212 0.03834730014204979 0.09518690407276154 3.66796875\n",
            "repr, std, cov, clossl, z, norm 0.012156862765550613 0.391845703125 0.02630699798464775 0.05827164649963379 0.09683593362569809 4.8984375\n",
            "repr, std, cov, clossl, z, norm 0.011939866468310356 0.393798828125 0.02369098924100399 0.05500221252441406 0.042305976152420044 4.77734375\n",
            "repr, std, cov, clossl, z, norm 0.010751396417617798 0.392578125 0.02509746327996254 0.028275327757000923 0.02748921699821949 3.9609375\n",
            "repr, std, cov, clossl, z, norm 0.01189382839947939 0.38720703125 0.029978791251778603 0.03571220487356186 0.06066373363137245 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.011332709342241287 0.38916015625 0.028627298772335052 0.04035918042063713 0.12455950677394867 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.011843332089483738 0.38720703125 0.03035065159201622 0.04165377467870712 0.12195538729429245 3.5234375\n",
            "repr, std, cov, clossl, z, norm 0.010867595672607422 0.390380859375 0.027654416859149933 0.04358944669365883 0.15048371255397797 3.6640625\n",
            "179\n",
            "repr, std, cov, clossl, z, norm 0.011645905673503876 0.389892578125 0.02813110500574112 0.038780391216278076 0.21815237402915955 3.5703125\n",
            "repr, std, cov, clossl, z, norm 0.011033493094146252 0.389404296875 0.02847783826291561 0.023882634937763214 0.08985106647014618 4.90625\n",
            "repr, std, cov, clossl, z, norm 0.01189186330884695 0.3837890625 0.036269646137952805 0.04298729822039604 0.028747037053108215 4.3515625\n",
            "repr, std, cov, clossl, z, norm 0.011362439021468163 0.39111328125 0.026407796889543533 0.055088918656110764 0.12872345745563507 3.55859375\n",
            "repr, std, cov, clossl, z, norm 0.011573412455618382 0.392333984375 0.02496728114783764 0.05670808628201485 0.025262530893087387 3.7734375\n",
            "repr, std, cov, clossl, z, norm 0.01040817704051733 0.397216796875 0.020988866686820984 0.038836520165205 0.11567766219377518 3.697265625\n",
            "repr, std, cov, clossl, z, norm 0.010876662097871304 0.39599609375 0.022752966731786728 0.03578515350818634 0.18860958516597748 3.794921875\n",
            "repr, std, cov, clossl, z, norm 0.01243368349969387 0.391357421875 0.02603677287697792 0.05324210971593857 0.05269944295287132 4.171875\n",
            "180\n",
            "repr, std, cov, clossl, z, norm 0.01204905565828085 0.392822265625 0.025917667895555496 0.05132914334535599 0.06106274574995041 3.6171875\n",
            "repr, std, cov, clossl, z, norm 0.010453524067997932 0.39404296875 0.024112213402986526 0.024737859144806862 0.0828440934419632 3.740234375\n",
            "repr, std, cov, clossl, z, norm 0.010034814476966858 0.395751953125 0.022141296416521072 0.034087181091308594 0.06765624135732651 3.87890625\n",
            "repr, std, cov, clossl, z, norm 0.01093010138720274 0.395263671875 0.02282649651169777 0.03615567460656166 0.1714642494916916 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.011639165692031384 0.39013671875 0.028328493237495422 0.03652719780802727 0.04077897220849991 3.31640625\n",
            "repr, std, cov, clossl, z, norm 0.011237578466534615 0.39111328125 0.02770273946225643 0.0263688825070858 0.05577618256211281 3.869140625\n",
            "repr, std, cov, clossl, z, norm 0.010195238515734673 0.39453125 0.0239606574177742 0.03510626032948494 0.13728651404380798 4.375\n",
            "repr, std, cov, clossl, z, norm 0.01199139840900898 0.388671875 0.029275305569171906 0.031487345695495605 0.1952463537454605 4.12109375\n",
            "181\n",
            "repr, std, cov, clossl, z, norm 0.010142592713236809 0.393798828125 0.024373214691877365 0.031216954812407494 0.12692652642726898 4.0703125\n",
            "repr, std, cov, clossl, z, norm 0.011442567221820354 0.38671875 0.0340891107916832 0.03262436389923096 0.1821136623620987 3.912109375\n",
            "repr, std, cov, clossl, z, norm 0.011186531744897366 0.38671875 0.0321304127573967 0.04552895575761795 0.10084874927997589 3.80859375\n",
            "repr, std, cov, clossl, z, norm 0.01080286968499422 0.39208984375 0.025613274425268173 0.03262981027364731 0.22673581540584564 4.5390625\n",
            "repr, std, cov, clossl, z, norm 0.010998654179275036 0.392333984375 0.026098769158124924 0.02940383180975914 0.05220409855246544 4.01171875\n",
            "repr, std, cov, clossl, z, norm 0.011586488224565983 0.393798828125 0.025177130475640297 0.05286496877670288 0.06903218477964401 2.73046875\n",
            "repr, std, cov, clossl, z, norm 0.011106079444289207 0.39306640625 0.026371894404292107 0.03046613559126854 0.18653209507465363 4.09765625\n",
            "repr, std, cov, clossl, z, norm 0.011070831678807735 0.388427734375 0.03202420473098755 0.03155549615621567 0.09897790104150772 3.814453125\n",
            "182\n",
            "repr, std, cov, clossl, z, norm 0.010814819484949112 0.39599609375 0.021412212401628494 0.03493303805589676 0.011257645674049854 3.361328125\n",
            "repr, std, cov, clossl, z, norm 0.01043091993778944 0.39501953125 0.024637119844555855 0.027855724096298218 0.11151344329118729 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.011100937612354755 0.390380859375 0.030003055930137634 0.03897453099489212 0.047445446252822876 3.689453125\n",
            "repr, std, cov, clossl, z, norm 0.011376769281923771 0.39453125 0.02331305481493473 0.030218375846743584 0.23071998357772827 4.32421875\n",
            "repr, std, cov, clossl, z, norm 0.011216185055673122 0.396728515625 0.02158351242542267 0.035853173583745956 0.11934252828359604 3.486328125\n",
            "repr, std, cov, clossl, z, norm 0.010769150219857693 0.398193359375 0.021284986287355423 0.04021656513214111 0.1524326205253601 3.82421875\n",
            "repr, std, cov, clossl, z, norm 0.011009667068719864 0.399169921875 0.02101970836520195 0.04258926957845688 0.12279030680656433 3.7890625\n",
            "repr, std, cov, clossl, z, norm 0.011292979121208191 0.392333984375 0.026171009987592697 0.045052964240312576 0.016612941399216652 4.3125\n",
            "183\n",
            "repr, std, cov, clossl, z, norm 0.01128244400024414 0.394287109375 0.022593941539525986 0.08548307418823242 0.07684272527694702 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.011650639586150646 0.389404296875 0.027986980974674225 0.07953747361898422 0.07556760311126709 4.35546875\n",
            "repr, std, cov, clossl, z, norm 0.013918345794081688 0.38671875 0.030489642173051834 0.04473569616675377 0.03798666596412659 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.014779436402022839 0.391845703125 0.02537856623530388 0.10265567898750305 0.08580455183982849 4.0234375\n",
            "repr, std, cov, clossl, z, norm 0.017915882170200348 0.382568359375 0.03696371987462044 0.07951812446117401 0.07477748394012451 2.693359375\n",
            "repr, std, cov, clossl, z, norm 0.01699681393802166 0.387939453125 0.031568482518196106 0.05653528869152069 0.1573924720287323 3.5703125\n",
            "repr, std, cov, clossl, z, norm 0.018118593841791153 0.381591796875 0.03864208608865738 0.09397762268781662 0.15438514947891235 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.016787933185696602 0.385009765625 0.033926308155059814 0.08392129093408585 0.031260114163160324 3.8359375\n",
            "184\n",
            "repr, std, cov, clossl, z, norm 0.01816815324127674 0.384521484375 0.03375300019979477 0.09586925059556961 0.08050920814275742 3.263671875\n",
            "repr, std, cov, clossl, z, norm 0.017241565510630608 0.393310546875 0.024683456867933273 0.08505141735076904 0.14851389825344086 3.33984375\n",
            "repr, std, cov, clossl, z, norm 0.018447306007146835 0.3935546875 0.024407805874943733 0.10232920199632645 0.08214694261550903 4.71484375\n",
            "repr, std, cov, clossl, z, norm 0.01736675202846527 0.398193359375 0.022259606048464775 0.10111909359693527 0.23198948800563812 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.016429375857114792 0.39501953125 0.027676889672875404 0.1057571992278099 0.05583822727203369 3.689453125\n",
            "repr, std, cov, clossl, z, norm 0.01534080971032381 0.400146484375 0.02095651999115944 0.08248364925384521 0.20260614156723022 3.76171875\n",
            "repr, std, cov, clossl, z, norm 0.014967373572289944 0.3994140625 0.02149946242570877 0.08785116672515869 0.11762569099664688 3.5\n",
            "repr, std, cov, clossl, z, norm 0.015944253653287888 0.39306640625 0.026889096945524216 0.1072351261973381 0.11983823776245117 3.21875\n",
            "185\n",
            "repr, std, cov, clossl, z, norm 0.014878752641379833 0.40283203125 0.018156373873353004 0.08248494565486908 0.22622740268707275 3.220703125\n",
            "repr, std, cov, clossl, z, norm 0.015057224780321121 0.39501953125 0.02351207286119461 0.10315071046352386 0.13638390600681305 4.7265625\n",
            "repr, std, cov, clossl, z, norm 0.014958289451897144 0.391357421875 0.02726622298359871 0.09473354369401932 0.15239916741847992 4.53515625\n",
            "repr, std, cov, clossl, z, norm 0.013532392680644989 0.3974609375 0.02254462242126465 0.07205844670534134 0.0500083789229393 3.38671875\n",
            "repr, std, cov, clossl, z, norm 0.01421150378882885 0.39208984375 0.027294516563415527 0.09378810971975327 0.07835753262042999 3.494140625\n",
            "repr, std, cov, clossl, z, norm 0.013084668666124344 0.39306640625 0.0262097530066967 0.07571596652269363 0.08853604644536972 3.5703125\n",
            "repr, std, cov, clossl, z, norm 0.013379959389567375 0.393310546875 0.026202252134680748 0.08897016197443008 0.10564000904560089 3.203125\n",
            "repr, std, cov, clossl, z, norm 0.014165994711220264 0.391845703125 0.026882518082857132 0.09462819993495941 0.05085599049925804 3.216796875\n",
            "186\n",
            "repr, std, cov, clossl, z, norm 0.014765767380595207 0.390625 0.02790428325533867 0.09254289418458939 0.18040184676647186 2.59765625\n",
            "repr, std, cov, clossl, z, norm 0.013178671710193157 0.396484375 0.022692736238241196 0.06850682944059372 0.17849871516227722 1.69140625\n",
            "repr, std, cov, clossl, z, norm 0.014394438825547695 0.395263671875 0.023200271651148796 0.08566935360431671 0.06661836057901382 3.51171875\n",
            "repr, std, cov, clossl, z, norm 0.013512825593352318 0.3955078125 0.022917721420526505 0.07922865450382233 0.13559772074222565 3.9375\n",
            "repr, std, cov, clossl, z, norm 0.012892055325210094 0.39453125 0.023716602474451065 0.05976845696568489 0.07259639352560043 3.634765625\n",
            "repr, std, cov, clossl, z, norm 0.01346561498939991 0.392333984375 0.025527222082018852 0.07474088668823242 0.13337716460227966 1.5791015625\n",
            "repr, std, cov, clossl, z, norm 0.013404752127826214 0.38818359375 0.029219411313533783 0.0669536218047142 0.024412961676716805 3.408203125\n",
            "repr, std, cov, clossl, z, norm 0.013226387090981007 0.39111328125 0.026986481621861458 0.07011193037033081 0.10333374887704849 4.20703125\n",
            "187\n",
            "repr, std, cov, clossl, z, norm 0.013926771469414234 0.39111328125 0.025571482256054878 0.08654864877462387 0.07417462766170502 2.44921875\n",
            "repr, std, cov, clossl, z, norm 0.013403885066509247 0.393310546875 0.024744156748056412 0.04836960509419441 0.04249805957078934 3.513671875\n",
            "repr, std, cov, clossl, z, norm 0.014248248189687729 0.390625 0.02657793089747429 0.067903533577919 0.09567820280790329 2.84765625\n",
            "repr, std, cov, clossl, z, norm 0.014414883218705654 0.389892578125 0.027251143008470535 0.07435604184865952 0.0911983773112297 3.896484375\n",
            "repr, std, cov, clossl, z, norm 0.013706816360354424 0.391357421875 0.02652142010629177 0.05496566742658615 0.006094544194638729 3.484375\n",
            "repr, std, cov, clossl, z, norm 0.013876230455935001 0.392333984375 0.024777943268418312 0.059380464255809784 0.06621009111404419 3.873046875\n",
            "repr, std, cov, clossl, z, norm 0.013495059683918953 0.39306640625 0.02401273138821125 0.05911491811275482 0.046826690435409546 3.859375\n",
            "repr, std, cov, clossl, z, norm 0.014581025578081608 0.391357421875 0.026207255199551582 0.08269154280424118 0.12507429718971252 4.22265625\n",
            "188\n",
            "repr, std, cov, clossl, z, norm 0.013163730502128601 0.395751953125 0.022524353116750717 0.06233731284737587 0.10260537266731262 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.013476025313138962 0.3935546875 0.023478597402572632 0.06895232200622559 0.12873490154743195 3.708984375\n",
            "repr, std, cov, clossl, z, norm 0.012579228729009628 0.393798828125 0.023353220894932747 0.04476083815097809 0.09909657388925552 2.958984375\n",
            "repr, std, cov, clossl, z, norm 0.01267738826572895 0.392822265625 0.024328108876943588 0.05910748243331909 0.07061274349689484 3.732421875\n",
            "repr, std, cov, clossl, z, norm 0.012458337470889091 0.39013671875 0.026608774438500404 0.048831548541784286 0.09483741223812103 3.53125\n",
            "repr, std, cov, clossl, z, norm 0.013495070859789848 0.383544921875 0.033724281936883926 0.057882051914930344 0.08616509288549423 3.30859375\n",
            "repr, std, cov, clossl, z, norm 0.012617189437150955 0.3876953125 0.028781291097402573 0.06723921000957489 0.10113513469696045 3.7265625\n",
            "repr, std, cov, clossl, z, norm 0.012569312006235123 0.385986328125 0.031874045729637146 0.05400751158595085 0.03261525556445122 3.681640625\n",
            "189\n",
            "repr, std, cov, clossl, z, norm 0.012259059585630894 0.386474609375 0.030870338901877403 0.04782453551888466 0.09645944833755493 3.177734375\n",
            "repr, std, cov, clossl, z, norm 0.012880378402769566 0.386962890625 0.030192648991942406 0.05888492614030838 0.24863311648368835 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.013236603699624538 0.391845703125 0.025684654712677002 0.04281162470579147 0.11988994479179382 3.65625\n",
            "repr, std, cov, clossl, z, norm 0.012862073257565498 0.393798828125 0.0233343206346035 0.03855612874031067 0.10213486850261688 3.611328125\n",
            "repr, std, cov, clossl, z, norm 0.013092675246298313 0.39208984375 0.025203082710504532 0.05030743405222893 0.12242726981639862 3.984375\n",
            "repr, std, cov, clossl, z, norm 0.01255299523472786 0.39208984375 0.024986974895000458 0.04923111945390701 0.10462989658117294 4.1015625\n",
            "repr, std, cov, clossl, z, norm 0.0134457191452384 0.392578125 0.023936865851283073 0.06357328593730927 0.007293335627764463 4.50390625\n",
            "repr, std, cov, clossl, z, norm 0.013466214761137962 0.39111328125 0.025913886725902557 0.0683838278055191 0.06419802457094193 2.705078125\n",
            "190\n",
            "repr, std, cov, clossl, z, norm 0.01276801060885191 0.389404296875 0.027972470968961716 0.04660714045166969 0.09196281433105469 3.12890625\n",
            "repr, std, cov, clossl, z, norm 0.012374228797852993 0.390625 0.02546992525458336 0.04437820240855217 0.06877918541431427 3.474609375\n",
            "repr, std, cov, clossl, z, norm 0.012716111727058887 0.386474609375 0.030002202838659286 0.05104070529341698 0.0897265151143074 3.482421875\n",
            "repr, std, cov, clossl, z, norm 0.0118052763864398 0.392578125 0.02446170710027218 0.04624224081635475 0.04658025503158569 3.609375\n",
            "repr, std, cov, clossl, z, norm 0.012052634730935097 0.3876953125 0.029789693653583527 0.05736543610692024 0.08567377924919128 2.966796875\n",
            "repr, std, cov, clossl, z, norm 0.012860970571637154 0.388671875 0.027753401547670364 0.05258486792445183 0.11938555538654327 3.50390625\n",
            "repr, std, cov, clossl, z, norm 0.012175258249044418 0.392578125 0.024980800226330757 0.04159471020102501 0.11822490394115448 3.748046875\n",
            "repr, std, cov, clossl, z, norm 0.012103213928639889 0.392822265625 0.02525302581489086 0.037769611924886703 0.035577353090047836 4.171875\n",
            "191\n",
            "repr, std, cov, clossl, z, norm 0.012327093631029129 0.393310546875 0.022454120218753815 0.043652553111314774 0.09692037850618362 3.470703125\n",
            "repr, std, cov, clossl, z, norm 0.011570620350539684 0.392333984375 0.024586278945207596 0.04227285459637642 0.1658107191324234 3.1171875\n",
            "repr, std, cov, clossl, z, norm 0.012707049958407879 0.38916015625 0.02775280922651291 0.048382800072431564 0.03568302467465401 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.012468218803405762 0.391845703125 0.02448340505361557 0.03478937968611717 0.26044604182243347 4.33203125\n",
            "repr, std, cov, clossl, z, norm 0.013033203780651093 0.3896484375 0.026847152039408684 0.04672347754240036 0.1668202131986618 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.012285719625651836 0.3916015625 0.02557509019970894 0.03257066756486893 0.11431249231100082 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.012937976978719234 0.385498046875 0.03133741393685341 0.04268074408173561 0.18933379650115967 3.744140625\n",
            "repr, std, cov, clossl, z, norm 0.0135832279920578 0.385986328125 0.03132696449756622 0.04509417340159416 0.09724932909011841 4.51953125\n",
            "192\n",
            "repr, std, cov, clossl, z, norm 0.013020418584346771 0.38818359375 0.02810993604362011 0.04385191947221756 0.07412375509738922 4.2109375\n",
            "repr, std, cov, clossl, z, norm 0.012680333107709885 0.386962890625 0.028248939663171768 0.04557738080620766 0.1038452684879303 3.5625\n",
            "repr, std, cov, clossl, z, norm 0.012043297290802002 0.388671875 0.028980618342757225 0.04060256853699684 0.03367970883846283 3.794921875\n",
            "repr, std, cov, clossl, z, norm 0.012212495319545269 0.390380859375 0.025174858048558235 0.035033583641052246 0.04129266366362572 4.01171875\n",
            "repr, std, cov, clossl, z, norm 0.01225635502487421 0.3857421875 0.03192479908466339 0.0362408310174942 0.07794911414384842 2.69140625\n",
            "repr, std, cov, clossl, z, norm 0.012411498464643955 0.391357421875 0.02550998516380787 0.035384368151426315 0.14147034287452698 3.64453125\n",
            "repr, std, cov, clossl, z, norm 0.013499200344085693 0.388916015625 0.026980523020029068 0.056905943900346756 0.20256683230400085 3.26171875\n",
            "repr, std, cov, clossl, z, norm 0.01189503911882639 0.393798828125 0.02446570433676243 0.033560752868652344 0.10792390257120132 3.869140625\n",
            "193\n",
            "repr, std, cov, clossl, z, norm 0.011798660270869732 0.3916015625 0.024509292095899582 0.037238314747810364 0.030716124922037125 3.41015625\n",
            "repr, std, cov, clossl, z, norm 0.01200790423899889 0.39111328125 0.02558908984065056 0.04882262274622917 0.054140448570251465 3.40625\n",
            "repr, std, cov, clossl, z, norm 0.01115078292787075 0.3896484375 0.028402293100953102 0.052504412829875946 0.0651896744966507 3.3671875\n",
            "repr, std, cov, clossl, z, norm 0.011683446355164051 0.387939453125 0.028001919388771057 0.04282689094543457 0.1429184228181839 2.201171875\n",
            "repr, std, cov, clossl, z, norm 0.011420042254030704 0.39208984375 0.02433779463171959 0.04562867060303688 0.17049279808998108 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.011978955008089542 0.386962890625 0.02985076606273651 0.05861062556505203 0.0937524363398552 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.011388936080038548 0.390625 0.02618689090013504 0.02386554889380932 0.012706915847957134 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.011441542766988277 0.39111328125 0.02520189806818962 0.04602454602718353 0.16506442427635193 3.9765625\n",
            "194\n",
            "repr, std, cov, clossl, z, norm 0.011599068529903889 0.38916015625 0.0265614353120327 0.03527173027396202 0.05262992903590202 3.8359375\n",
            "repr, std, cov, clossl, z, norm 0.012342916801571846 0.388427734375 0.028041638433933258 0.04843806475400925 0.08024798333644867 3.72265625\n",
            "repr, std, cov, clossl, z, norm 0.012307679280638695 0.3896484375 0.026307402178645134 0.041925352066755295 0.20369572937488556 3.880859375\n",
            "repr, std, cov, clossl, z, norm 0.01254159677773714 0.38671875 0.02879304625093937 0.037918124347925186 0.05407067388296127 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.013063598424196243 0.3837890625 0.03186386823654175 0.0294150710105896 0.09794142097234726 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.012468538247048855 0.388427734375 0.02728883922100067 0.04312187433242798 0.06563983112573624 3.748046875\n",
            "repr, std, cov, clossl, z, norm 0.012388146482408047 0.3896484375 0.026985526084899902 0.02641824446618557 0.04697411134839058 3.251953125\n",
            "repr, std, cov, clossl, z, norm 0.012691180221736431 0.388916015625 0.02675500698387623 0.04638442024588585 0.0036096766125410795 3.384765625\n",
            "195\n",
            "repr, std, cov, clossl, z, norm 0.011807160452008247 0.388916015625 0.02785550057888031 0.03873331844806671 0.13037827610969543 3.783203125\n",
            "repr, std, cov, clossl, z, norm 0.011625177226960659 0.3896484375 0.025987926870584488 0.03507990390062332 0.12681074440479279 3.662109375\n",
            "repr, std, cov, clossl, z, norm 0.011436204425990582 0.38623046875 0.029853880405426025 0.03488354757428169 0.13379906117916107 3.837890625\n",
            "repr, std, cov, clossl, z, norm 0.011525281704962254 0.39111328125 0.02410048246383667 0.0523008331656456 0.09873761981725693 3.869140625\n",
            "repr, std, cov, clossl, z, norm 0.01188820693641901 0.390625 0.025180529803037643 0.03861711919307709 0.11589735746383667 3.439453125\n",
            "repr, std, cov, clossl, z, norm 0.011566409841179848 0.388671875 0.026925472542643547 0.036232948303222656 0.05657447502017021 4.234375\n",
            "repr, std, cov, clossl, z, norm 0.011098858900368214 0.385986328125 0.02978341653943062 0.04489278793334961 0.11727950721979141 3.341796875\n",
            "repr, std, cov, clossl, z, norm 0.011497830972075462 0.38623046875 0.029689177870750427 0.05045468360185623 0.05251316726207733 3.21484375\n",
            "196\n",
            "repr, std, cov, clossl, z, norm 0.011779287829995155 0.387939453125 0.026674017310142517 0.05256478488445282 0.1551325023174286 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.01178488228470087 0.387939453125 0.027724824845790863 0.04899519681930542 0.08766114711761475 3.181640625\n",
            "repr, std, cov, clossl, z, norm 0.010764753445982933 0.39111328125 0.023776249960064888 0.0376720055937767 0.07049569487571716 4.0703125\n",
            "repr, std, cov, clossl, z, norm 0.011523139663040638 0.38916015625 0.025606561452150345 0.035965800285339355 0.17512907087802887 3.47265625\n",
            "repr, std, cov, clossl, z, norm 0.011261073872447014 0.391357421875 0.024717412889003754 0.0392053984105587 0.08682525902986526 4.36328125\n",
            "repr, std, cov, clossl, z, norm 0.011525877751410007 0.388671875 0.026896266266703606 0.041958510875701904 0.024288645014166832 3.826171875\n",
            "repr, std, cov, clossl, z, norm 0.010745109058916569 0.391357421875 0.024574536830186844 0.03773733973503113 0.0515875369310379 5.3046875\n",
            "repr, std, cov, clossl, z, norm 0.010946786031126976 0.386962890625 0.028951773419976234 0.04101186618208885 0.022594578564167023 4.03515625\n",
            "197\n",
            "repr, std, cov, clossl, z, norm 0.011246025562286377 0.385498046875 0.029895715415477753 0.02789895422756672 0.14947351813316345 2.37109375\n",
            "repr, std, cov, clossl, z, norm 0.012256831862032413 0.3837890625 0.03049495443701744 0.046124983578920364 0.09748386591672897 3.654296875\n",
            "repr, std, cov, clossl, z, norm 0.011725025251507759 0.38623046875 0.029231781139969826 0.04231741651892662 0.12230170518159866 3.89453125\n",
            "repr, std, cov, clossl, z, norm 0.011341992765665054 0.38623046875 0.030773360282182693 0.04040569067001343 0.005285654682666063 4.05078125\n",
            "repr, std, cov, clossl, z, norm 0.01112053170800209 0.3896484375 0.026941750198602676 0.03248615562915802 0.24053610861301422 4.015625\n",
            "repr, std, cov, clossl, z, norm 0.011922518722712994 0.386474609375 0.02870003506541252 0.05506892874836922 0.09960608929395676 3.611328125\n",
            "repr, std, cov, clossl, z, norm 0.012141444720327854 0.3896484375 0.025950249284505844 0.045139167457818985 0.1225527822971344 3.435546875\n",
            "repr, std, cov, clossl, z, norm 0.010676337406039238 0.392822265625 0.023344311863183975 0.02210175059735775 0.020801521837711334 3.62890625\n",
            "198\n",
            "repr, std, cov, clossl, z, norm 0.011865808628499508 0.3896484375 0.02510082721710205 0.035116612911224365 0.001956990920007229 3.724609375\n",
            "repr, std, cov, clossl, z, norm 0.011543805710971355 0.389892578125 0.02499663457274437 0.03320709243416786 0.044778306037187576 3.62890625\n",
            "repr, std, cov, clossl, z, norm 0.01129541452974081 0.389892578125 0.025523612275719643 0.043521419167518616 0.07194770127534866 3.775390625\n",
            "repr, std, cov, clossl, z, norm 0.011753500439226627 0.389892578125 0.024787167087197304 0.045529503375291824 0.16286928951740265 3.421875\n",
            "repr, std, cov, clossl, z, norm 0.012101049534976482 0.39013671875 0.02455436810851097 0.03925492241978645 0.27440178394317627 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.01170781347900629 0.38671875 0.028356190770864487 0.03100251406431198 0.08364121615886688 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.012800048105418682 0.3828125 0.03263864666223526 0.044808968901634216 0.022409945726394653 3.955078125\n",
            "repr, std, cov, clossl, z, norm 0.011646675877273083 0.388427734375 0.026534195989370346 0.033049795776605606 0.17593823373317719 3.4140625\n",
            "199\n",
            "repr, std, cov, clossl, z, norm 0.012304367497563362 0.385986328125 0.029154229909181595 0.0343443788588047 0.0289598535746336 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.011105525307357311 0.386474609375 0.028253154829144478 0.04130278527736664 0.06587453186511993 3.689453125\n",
            "repr, std, cov, clossl, z, norm 0.011668834835290909 0.38671875 0.02811974100768566 0.04393015801906586 0.16246221959590912 3.69140625\n",
            "repr, std, cov, clossl, z, norm 0.011264286004006863 0.38818359375 0.026347391307353973 0.040482960641384125 0.06819544732570648 3.845703125\n",
            "repr, std, cov, clossl, z, norm 0.011544336564838886 0.38671875 0.027821490541100502 0.038464855402708054 0.12799261510372162 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.01086905226111412 0.388427734375 0.02640007808804512 0.030043721199035645 0.027208629995584488 3.76953125\n",
            "repr, std, cov, clossl, z, norm 0.010703654028475285 0.387939453125 0.027745019644498825 0.03457185626029968 0.11834139376878738 3.81640625\n",
            "repr, std, cov, clossl, z, norm 0.010683400556445122 0.39111328125 0.023559698835015297 0.02713300660252571 0.06590168923139572 3.8984375\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.figure(figsize=(4, 4))\n",
        "    plt.figure()\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(200):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # # # torch.save(checkpoint, folder+'agentgru3tcost3.pkl')\n",
        "    # # torch.save(checkpoint, folder+'agentcovgru1tcost3drop.pkl')\n",
        "    # torch.save(checkpoint, folder+'agentgru1tcost3goscratch.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "# batch64 28m58s 84\n",
        "\n",
        "\n",
        "# repr, std, cov, clossl, z, norm 0.005674966610968113 0.3493117690086365 0.1219848245382309 0.11347860097885132 0.16498714685440063 1.429775595664978\n",
        "# repr, std, cov, clossl, z, norm 0.004161856137216091 0.35110002756118774 0.10768019407987595 0.14673584699630737 0.17844633758068085 8.552577018737793\n",
        "# repr, std, cov, clossl, z, norm 0.005356341600418091 0.354144811630249 0.10272272676229477 0.14740866422653198 0.1459342986345291 0.08476211130619049\n",
        "\n",
        "\n",
        "# repr, std, cov, clossl, z, norm 0.03280682861804962 0.3602587580680847 0.0940496101975441 12.544517517089844 0.0145032973960042 0.10470973700284958\n",
        "# repr, std, cov, clossl, z, norm 0.0022784259635955095 0.37327876687049866 0.12404247373342514 5.347583770751953 0.0835014060139656 0.7087528705596924\n",
        "# repr, std, cov, clossl, z, norm 0.005345265381038189 0.34533047676086426 0.4332529306411743 2.4848036766052246 0.12125874310731888 0.9996055960655212\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "fb8f86c1-a32d-4c3b-aadc-08ff4c574c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "facdef34-26c9-4f57-ee90-ede0308c6f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4211955ac4d0>:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-9-4211955ac4d0>:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dided\n",
            "time\n",
            "[11, 7, 5, 10, 2, 4, 7, 2, 2, 4, 7, 5, 4, 3, 2, 5, 3, 11, 11, 1, 5, 10, 6, 9]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "oFZDopEKGCO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cm6KjvBrnNO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title alllll\n",
        "for i in range(200):\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer_=[]\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "        # buffer_ = simulate(agent, buffer_)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    torch.save(checkpoint, folder+'agentgru1tcost3goscratch.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "    print(\"train_data.data\",len(train_data.data))\n",
        "    while len(train_data.data)>10000: # 10000:6.9gb, 20000:5.5gb\n",
        "        buffer.pop(random.randrange(len(buffer)))\n",
        "        train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "923b3a88-3352-4e51-a952-04979f44d378"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792,
          "referenced_widgets": [
            "4a3969e8301440febb6fa153a1f299ec",
            "58a5d62d9d6a44beb5ac8a5b712ef1f2",
            "e52be7bbafb449ceb1b5a881bc27924f",
            "c56cf033bafb42bd9bf59ea2a5385d9f",
            "b37835e7b2fa49efa633f97b14e4c3f7",
            "fd693277fd5d4ab789ee250fa99949ae",
            "42188f4346174342bd49be1a934a85db",
            "3f16260f2413492a959ace04cc2895ff"
          ]
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "79b698d8-ec39-4d7c-992e-4d3d7b815019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.15.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:g7mysf97) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.481 MB of 0.481 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a3969e8301440febb6fa153a1f299ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▇█▇▆▇▆▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁</td></tr><tr><td>cov</td><td>▄▇▁▁▂▂▄▁▂▂▂▃▃▅▄▄▃▄▃▄▄▂▄▅▃▃▅▃▃▃▄▄▄█▅▆▄▆▄▅</td></tr><tr><td>repr</td><td>▁▁▆▆▆█▇█▇▇▅▅▄▅▄▃▃▃▃▃▄▄▄▃▃▃▃▃▃▂▃▂▃▃▃▃▃▃▃▃</td></tr><tr><td>std</td><td>██▆▆▆▆▄▅▅▄▄▅▄▄▄▅▄▄▃▄▄▄▃▃▃▃▃▃▃▃▄▂▂▃▂▂▁▃▁▂</td></tr><tr><td>z_norm</td><td>▁▃▁▂▁█▃▂▅▂▄▅▃▅▄▂▃▃▂▄▁▂▃▅▂▄▃▃▄▄▄▁▄▃▄▄▄▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>0.10189</td></tr><tr><td>cov</td><td>0.02598</td></tr><tr><td>repr</td><td>0.00764</td></tr><tr><td>std</td><td>0.40161</td></tr><tr><td>z_norm</td><td>0.14309</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">honest-thunder-128</strong> at: <a href='https://wandb.ai/bobdole/procgen/runs/g7mysf97' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/g7mysf97</a><br/> View project at: <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241007_090739-g7mysf97/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:g7mysf97). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241007_101623-9fre2ub3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/9fre2ub3' target=\"_blank\">efficient-sun-129</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/9fre2ub3' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/9fre2ub3</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY7BITKjSKC",
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model] # batch argm z for search\n",
        "        batch_, T, _ = x.shape\n",
        "        batch = 16 # 16\n",
        "        # z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        # idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        idx = torch.argmin(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T]\n",
        "        return torch.index_select(z, 0, idx) # [batch_, T,dim_z]\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "\n",
        "                # lh0 = torch.zeros((rwd.shape[1],)+h0.shape, device=device)\n",
        "                # lz = torch.zeros((lsy.shape[0], lsy.shape[1], self.dim_z), device=device)\n",
        "                    # for name, param in agent.tcost.named_parameters():\n",
        "                    #     print(\"param.data\",param.max().item(),param.min().item())\n",
        "                    #     print(\"agent.tcost\",param.data)\n",
        "\n",
        "# # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "# ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n",
        "# for epoch in range(300):\n",
        "#       for input, target in loader:\n",
        "#           optimizer.zero_grad()\n",
        "#           loss_fn(model(input), target).backward()\n",
        "#           optimizer.step()\n",
        "#           ema_model.update_parameters(model)\n",
        "# # Update bn statistics for the ema_model at the end\n",
        "# torch.optim.swa_utils.update_bn(loader, ema_model)\n",
        "# # Use ema_model to make predictions on test data\n",
        "# preds = ema_model(test_input)\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd): # best case z for train\n",
        "        # self.tcost.eval() # disable tcost dropout\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(2): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lsy_, lh0 = self.rnn_it(sy_, la, lz, h0_)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = self.tcost.loss(syh0, rwd.flatten(), reduction='none')\n",
        "            # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb0b4b7-e261-48ff-aac0-65f53ba04747",
        "cellView": "form",
        "id": "B_EPhszEbbwD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-4f4b4824202d>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru batch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 10 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=3. # 50 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 1 # ν cov Covariance\n",
        "        self.closs_coeff=10. # 10\n",
        "        # self.zloss_coeff=0. # 10 1\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model] # batch argm z for search\n",
        "        batch_, T, _ = x.shape\n",
        "        batch = 16 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch*batch_, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        idx = torch.argmin(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T] -> [batch_]\n",
        "        return torch.index_select(z, 0, idx) # [batch_, T,dim_z]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch = 16\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:,:self.lx.shape[0]] = self.lx.repeat(batch,1,1)[:,:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [batch, T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [batch,T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        idx = torch.argmin(loss.sum(-1)) # loss [batch, T]\n",
        "        return lact[idx], lh0[:,:,idx], x.data[idx], z[idx] # [batch,T], [T, num_layers, batch, d_model], [batch,T, dim_a], [batch,T, dim_z]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz, h0, gamma=0.9): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        lsx, lh0 = self.rnn_it(sx, la, lz, h0)\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        syh0 = torch.cat([lsx, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        # if len(c.shape) == 1: print(\"rnn_pred c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        if len(tcost.shape) == 1: print(\"rnn_pred tcost\", [f'{cc.item():g}' for cc in tcost.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        return c, lh0\n",
        "\n",
        "    def rnn_it(self, sx, la, lz, h0): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        batch_ = batch//sx.shape[0]\n",
        "        sx, h0 = sx.repeat(batch_, 1), h0.repeat(1, batch_, 1)\n",
        "        lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1) # [batch, d_model+dim_a/z]\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            # sx = sx + out.squeeze(1) # [batch,seq_len,d_model] # h0 = h0 +\n",
        "            sx = out.squeeze(1) # [batch,1,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        return lsx, lh0\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd): # best case z for train\n",
        "        # self.tcost.eval() # disable tcost dropout\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        batch = 64 # 16\n",
        "        lsy, la, rwd = lsy.repeat(batch,1,1), la.repeat(batch,1,1), rwd.repeat(batch,1) # [batch*batch_size, bptt, d_model], [batch*batch_size, d_model, dim_a], [batch*batch_size, bptt]\n",
        "        lz = nn.Parameter(torch.zeros((batch*batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lsy_, lh0 = self.rnn_it(sy_, la, lz, h0_)\n",
        "            # repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            repr_loss = ((lsy-lsy_)**2).unflatten(0, (batch,batch_size)).flatten(1).mean(-1)\n",
        "            syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch*batch_size,bptt,d_model], [bptt,num_layers,batch*batch_size,d_model] -> [batch*batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "            clossl = self.tcost.loss(syh0, rwd.flatten(), reduction='none').unflatten(0, (batch,batch_size*bptt)).mean(-1) # [batch*batch_size*bptt] -> [batch]\n",
        "            # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        idx = torch.argmin(cost)\n",
        "        return lz.unflatten(0, (batch,batch_size))[idx].squeeze(0).detach()\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            sy_ = self.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    lz = self.argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    # with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0.5)).mul_((torch.rand_like(lz)>0.1).bool()) # dropout without scaling\n",
        "                    with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0)).mul_((torch.rand_like(lz)>0.5).bool()) # dropout without scaling\n",
        "                    lsy_, lh0 = self.rnn_it(sy_.squeeze(1), la, lz, h0)\n",
        "                    repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model] # not lsy_, else unstable\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    # print(\"pred\",pred[0])\n",
        "                    # print(\"rwd\",rwd[0])\n",
        "                    # mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # reprloss = ((lsy-lsy_)**2).mean(-1) # [batch_size, bptt]\n",
        "                    # print(\"reprloss\",reprloss[0])\n",
        "                    # mask = (reprloss>0.05)[0]\n",
        "                    # # imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[0][mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                # torch.norm(lsy-torch.cat([sy_,lsy[:-1]], dim=1), dim=-1) # -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "                # prob = F.softmax(output, dim=-1)\n",
        "                # entropy = -torch.sum(prob * torch.log(prob + 1e-5), dim=-1)\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                norm = torch.norm(lsy[0][0], dim=-1).item()\n",
        "                z_norm = torch.norm(lz[0][-1], dim=-1)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                # print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "                print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item(), \"z_norm\": z_norm.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cg0BI2TwY9-p"
      },
      "outputs": [],
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VMebkQ1mJtD"
      },
      "outputs": [],
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jx0k_ndHOEMe"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "N2TGs69fnrZo",
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ],
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "outputs": [],
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ],
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1yvJkX89C_o"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3nfZRhVc9Ssp"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1_GgDzoDyYB"
      },
      "outputs": [],
      "source": [
        "# @title torchrl.data.PrioritizedReplayBuffer\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage\n",
        "buffer_lazytensor = ReplayBuffer(storage=LazyTensorStorage(size))\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "buffer_lazymemmap = ReplayBuffer(storage=LazyMemmapStorage(size), batch_size=32, sampler=SamplerWithoutReplacement())\n",
        "\n",
        "\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))\n",
        "data = range(10)\n",
        "rb.extend(data)\n",
        "# rb.extend(buffer)\n",
        "\n",
        "\n",
        "sample = rb.sample(3)\n",
        "print(sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfzJ6zoL6vGc"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "\n",
        "# def closure():\n",
        "#     optimizer.zero_grad()  # Zero out the gradients\n",
        "#     output = f(x)          # Forward pass through the model\n",
        "#     loss = loss_fn(output, target)  # Calculate the loss\n",
        "#     loss.backward()         # Backpropagate\n",
        "#     return loss\n",
        "\n",
        "batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "z = nn.Parameter(torch.zeros((batch_size, bptt, 1), device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "print(z.squeeze().data[-1])\n",
        "# print(lz.squeeze().data[-1])\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "sy = agent.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device))#.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "\n",
        "def closure():\n",
        "    lz = torch.cat([z, torch.zeros((batch_size, bptt, agent.dim_z-z.shape[-1]), device=device)], dim=-1)\n",
        "    sy_, h0_ = sy.detach(), h0.detach()\n",
        "    lsy_, lh0 = agent.rnn_it(sy_, la, lz, h0_)\n",
        "    repr_loss = F.mse_loss(lsy, lsy_)\n",
        "    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "    clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "    cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "    return cost\n",
        "\n",
        "import time\n",
        "optimizer = torch.optim.LBFGS([z], lr=1e-2, max_iter=10)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(20):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viimAIpYSJq_",
        "outputId": "2377ee32-2d6a-4c90-a91f-93ff4b885315",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6931, 0.3133, 0.6931])\n",
            "tensor([[0.6931, 0.6931],\n",
            "        [0.3133, 1.3133],\n",
            "        [0.6931, 0.6931]])\n"
          ]
        }
      ],
      "source": [
        "# @title test CrossEntropyLoss\n",
        "\n",
        "labels = torch.tensor([0,0,0])\n",
        "pred = torch.tensor([[50.,50.],[1.,0.],[1.,1.]])\n",
        "\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "# loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)]))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# weight=torch.tensor([1/a, 1/(1-a)])\n",
        "weight=torch.tensor([1, 1])\n",
        "# weight=torch.tensor([1/2, 1/2])\n",
        "\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(nn.Softmax(dim=-1)(pred))\n",
        "# print(-(weight*torch.log(nn.Softmax(dim=-1)(pred))).mean(-1))\n",
        "# print(-(labels.float()@torch.log(nn.Softmax(dim=-1)(pred))))\n",
        "print(-(torch.log(nn.Softmax(dim=-1)(pred))))\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "arange = torch.arange(pred.shape[-1]).repeat(pred.shape[0],1)\n",
        "# torch.where(1,0).bool()\n",
        "mask=(arange==pred)\n",
        "\n",
        "# 2*pred-1\n",
        "# (1-pred)\n",
        "(pred[mask] + (1-pred[mask]))\n",
        "\n",
        "log_preds = torch.log(torch.softmax(pred, dim=1))\n",
        "target_log_probs = log_preds[range(pred.shape[0]), labels]\n",
        "loss = -torch.mean(target_log_probs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tensorboard\n",
        "# !pip install tensorboard\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# writer = SummaryWriter('runs/molecules')\n",
        "\n",
        "# writer.add_scalar(\"Loss/train\", 3.0, 1)\n",
        "# writer.add_scalar(\"Loss/train\", 2.0, 2)\n",
        "# writer.add_scalar(\"Loss/train\", 1.5, 3)\n",
        "# writer.flush()\n",
        "# writer.close()\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AwRYdovgABiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WHjFn2gmzI"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6BYoXsX57o"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJdFpDr2wIMT"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LKUSzmYLLuRh",
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ],
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqBL9hljvW-5"
      },
      "outputs": [],
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o0UHSueqx9W8"
      },
      "outputs": [],
      "source": [
        "# @title test tcost3\n",
        "# def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "#     self.train()\n",
        "#     for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "#         state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             lsy = self.jepa.enc(state.flatten(end_dim=1)) # [batch_size, bptt, d_model]\n",
        "#             # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "#             # lsy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "#             std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy))\n",
        "#             jloss = self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "#             clossl = self.tcost.loss(lsy, reward.flatten(end_dim=1))\n",
        "#             closs = self.closs_coeff * clossl\n",
        "\n",
        "#             pred = self.tcost(lsy).squeeze(-1).unflatten(0, reward.shape) # [batch_size, bptt]\n",
        "#             print(\"pred\",pred[0])\n",
        "#             print(\"rwd\",reward[0])\n",
        "#             mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "#             # # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "#             # try: imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "#             # # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "#             # except ZeroDivisionError: pass\n",
        "\n",
        "#         loss = jloss + closs\n",
        "\n",
        "#         print(\"std, cov, clossl, wrong\", std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "#         # print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "#         scaler.scale(loss).backward()\n",
        "#         # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "#         scaler.step(optim)\n",
        "#         scaler.update()\n",
        "#         optim.zero_grad()\n",
        "#         try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "#         except: pass\n",
        "\n",
        "# # for i in range(5):\n",
        "# #     print(i)\n",
        "# #     train_jepa(agent, train_loader, optim)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Jt_UlGz6Xoq3",
        "m8WHjFn2gmzI"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a3969e8301440febb6fa153a1f299ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58a5d62d9d6a44beb5ac8a5b712ef1f2",
              "IPY_MODEL_e52be7bbafb449ceb1b5a881bc27924f"
            ],
            "layout": "IPY_MODEL_c56cf033bafb42bd9bf59ea2a5385d9f"
          }
        },
        "58a5d62d9d6a44beb5ac8a5b712ef1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b37835e7b2fa49efa633f97b14e4c3f7",
            "placeholder": "​",
            "style": "IPY_MODEL_fd693277fd5d4ab789ee250fa99949ae",
            "value": "0.481 MB of 0.481 MB uploaded\r"
          }
        },
        "e52be7bbafb449ceb1b5a881bc27924f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42188f4346174342bd49be1a934a85db",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f16260f2413492a959ace04cc2895ff",
            "value": 1
          }
        },
        "c56cf033bafb42bd9bf59ea2a5385d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b37835e7b2fa49efa633f97b14e4c3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd693277fd5d4ab789ee250fa99949ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42188f4346174342bd49be1a934a85db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f16260f2413492a959ace04cc2895ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}